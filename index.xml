<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>murmuring in the background</title>
    <link>/</link>
    <description>Recent content on murmuring in the background</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>2020 May 01 (Fri), 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>the mapper construction in base R</title>
      <link>/2020/05/01/mapper-construction-base-r/</link>
      <pubDate>2020 May 01 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2020/05/01/mapper-construction-base-r/</guid>
      <description><p>Several mature, open-source implementations of <a href="http://diglib.eg.org/handle/10.2312/SPBG.SPBG07.091-100">the mapper construction</a> now exist, including the standalone <a href="http://danifold.net/mapper/index.html">Python Mapper</a> module and <a href="https://kepler-mapper.scikit-tda.org/">KeplerMapper</a>, part of the scikit family. I’m currently experimenting with Matt Piekenbrock’s R package <a href="https://github.com/peekxc/Mapper/">Mapper</a>, which is not on CRAN but makes—from my beginner’s vantage point—exemplary use of the R6 system.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>What these implementations have in common is that the underlying construction is mostly hidden from view. Users can specify its constituent parts, but what gets visualized is almost exclusively either</p>
<ul>
<li>the lensed (or filtered) point cloud, usually in two dimensions; or</li>
<li>the simplicial complex, usually annotated by size and color.</li>
</ul>
<p>This is fine of course for routine experimental or practical use, but it limits the pedagogical potential of these tools.</p>
<p>The most rapidly i’ve come to understand the construction has been through implementation. In fact, it’s tedius but not terribly time-consuming to construct a mapper entirely in base R.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> The following “implementation” is taken from a document i’ve been using internally to convey the basic idea of mapper and to illustrate the effects of different choices of cover. Maybe you can find more illustrative uses for it, but i recommend against building an analysis pipeline around it!</p>
<div id="precap" class="section level2">
<h2>precap</h2>
<p>Briefly, here is some notation for the elements of the construction, adapted from <a href="https://link.springer.com/article/10.1007/s11538-019-00614-z">Mémoli and Singhal</a>:</p>
<ul>
<li><span class="math inline">\(X\)</span> – the point cloud (a finite metric space)</li>
<li><span class="math inline">\(f:X\to Z\)</span> – the lens function from <span class="math inline">\(X\)</span> to the lens space <span class="math inline">\(Z\)</span></li>
<li><span class="math inline">\(\mathcal{U}=\{U_i\}\)</span> – the cover of <span class="math inline">\(f(X)\)</span>, usually inferred from (and used interchangeably with) a finite cover of a subset <span class="math inline">\(S\subset Z\)</span> that contains <span class="math inline">\(f(X)\)</span></li>
<li><span class="math inline">\(\delta:X\times X\to\mathbb{R}_{\geq0}\)</span> – the distance metric on <span class="math inline">\(X\)</span></li>
<li><span class="math inline">\(C:\mathcal{M}\to\mathcal{P}\)</span> – the clustering method, based on <span class="math inline">\(\delta\)</span>, that sends metric spaces <span class="math inline">\(Y\subseteq X\)</span> to partitions of their elements</li>
<li><span class="math inline">\(\mathcal{V}=\{V_{ij}\}=\bigcup_i{C(f^{-1}(U_i))}\)</span> – the refinement of the pullback cover <span class="math inline">\(\{f^{-1}\mathcal{U}\}\)</span> of <span class="math inline">\(X\)</span> obtained by partitioning each subset <span class="math inline">\(f^{-1}(U_i)\subset X\)</span></li>
<li><span class="math inline">\(N\)</span> – the nerve of <span class="math inline">\(\mathcal{V}\)</span>, comprising a simplex for every non-empty intersection of sets in <span class="math inline">\(\mathcal{V}\)</span></li>
</ul>
</div>
<div id="construction" class="section level2">
<h2>construction</h2>
<p>I’ll begin with probably the most common toy example, the noisy circle. Since this isn’t part of the construction per se, i’m allowing myself the vanity of invoking <a href="https://github.com/corybrunson/tdaunif/">a manifold sampling package</a> under development with two high school interns:</p>
<pre class="r"><code>set.seed(2)
# point cloud
x &lt;- tdaunif::sample_circle(n = 120, sd = .1)
x[, 2] &lt;- x[, 2] + 1.5
print(head(x))</code></pre>
<pre><code>##                x         y
## [1,]  0.21900250 2.3144202
## [2,] -0.09167245 0.4182471
## [3,] -0.96604574 1.0946227
## [4,]  0.50825992 2.2572006
## [5,]  0.98900895 1.2088241
## [6,]  0.85559229 1.2699230</code></pre>
<p>I’ve shifted the coordinates upward for downstream plotting purposes. Note that this point cloud <span class="math inline">\(X\)</span> exists in <span class="math inline">\(\mathbb{R}^2\)</span> but that i have not yet declared a distance metric <span class="math inline">\(\delta\)</span> on <span class="math inline">\(X\)</span>.</p>
<p>The lens (or filter) <span class="math inline">\(f\)</span> is usually (a) obtained via dimensionality reduction on the <span class="math inline">\(X\)</span> or (b) taken to be a meaningful stratification variable of or function on <span class="math inline">\(X\)</span>. This example takes approach (a), projecting <span class="math inline">\(X\)</span> to its first coordinate:</p>
<pre class="r"><code># lensed point cloud
f &lt;- x[, 1]
print(head(f))</code></pre>
<pre><code>## [1]  0.21900250 -0.09167245 -0.96604574  0.50825992  0.98900895  0.85559229</code></pre>
<p>For ease of visualization as well as personal preference, i’ll use a fixed-width interval cover <span class="math inline">\(\mathcal{U}=\{U_i\}\)</span> with diameter half that of <span class="math inline">\(f(X)\)</span> and 50% overlap, which comprises five sets. I like this sort of cover because it is (uniformly) <em>2-fold</em>—every point of <span class="math inline">\(f(X)\)</span> is contained in exactly two <span class="math inline">\(U_i\)</span>:</p>
<pre class="r"><code># cover interval length
d &lt;- diff(range(f))*1/2
# cover interval centers
c &lt;- seq(min(f), max(f), d/2)
# cover intervals
u &lt;- cbind(x0 = c - d/2, x1 = c + d/2)
print(u)</code></pre>
<pre><code>##                x0           x1
## [1,] -1.748626379 -0.586016272
## [2,] -1.167321326 -0.004711219
## [3,] -0.586016272  0.576593835
## [4,] -0.004711219  1.157898888
## [5,]  0.576593835  1.739203942</code></pre>
<p>The pullback cover <span class="math inline">\(f^{-1}(\mathcal{U})\)</span> of <span class="math inline">\(X\)</span> is a straightforward step that involves no choice on the users part, though note the use of <code>&lt;=</code> and <code>&lt;</code> to ensure (up to mechanical precision) that cover set boundaries are handled correctly:</p>
<pre class="r"><code># pullback cover sets
p &lt;- apply(u, 1, function(u_i) which(u_i[1] &lt;= f &amp; f &lt; u_i[2]))
names(p) &lt;- paste0(seq(p), &quot;:&quot;)
print(lapply(p, head))</code></pre>
<pre><code>## $`1:`
## [1]  3  9 10 11 15 19
## 
## $`2:`
## [1]  2  3  9 10 11 15
## 
## $`3:`
## [1]  1  2  4  7  8 12
## 
## $`4:`
## [1] 1 4 5 6 7 8
## 
## $`5:`
## [1]  5  6 17 20 29 30</code></pre>
<p>To partition the preimages <span class="math inline">\(f^{-1}(U_i)\in f^{-1}(\mathcal{U})\)</span>, i’ve used the <a href="http://www.jmlr.org/papers/v11/carlsson10a.html">theoretically exceptional</a> method of single-linkage hierarchical clustering, but with a dangerously naïve <em>fixed</em> cutoff at height <span class="math inline">\(\frac{1}{3}\)</span>:</p>
<pre class="r"><code># clustered pullback cover sets
cl &lt;- function(p_i) {
  m &lt;- cutree(hclust(dist(x[p_i, , drop = FALSE]), method = &quot;single&quot;), h = 1/3)
  lapply(unique(m), function(v_ij) p_i[which(m == v_ij)])
}
v &lt;- unlist(lapply(p, cl), recursive = FALSE)
print(lapply(v, head))</code></pre>
<pre><code>## $`1:`
## [1]  3  9 10 11 15 19
## 
## $`2:`
## [1]  2  3  9 10 11 15
## 
## $`3:1`
## [1]  1  4  7 12 14 18
## 
## $`3:2`
## [1]  2  8 13 16 21 23
## 
## $`4:`
## [1] 1 4 5 6 7 8
## 
## $`5:`
## [1]  5  6 17 20 29 30</code></pre>
<p>While the clusters <span class="math inline">\(V_{ij}\)</span> will be encoded as vertices, higher-dimensional simplices will represent overlaps <span class="math inline">\(V_{ij}\cap V_{i&#39;j&#39;}\)</span> among them (<span class="math inline">\(i\neq i&#39;\)</span>). Happily, because the cover is 2-fold, any lensed point <span class="math inline">\(f(x)\)</span> lies in the intersection <span class="math inline">\(U_i\cap U_{i&#39;}\)</span> of at most two cover sets, so in this case we only have to worry about edges between <em>pairs</em> of vertices. Moreover, the only intersections to check are those between cover sets of adjacent indices <span class="math inline">\(i&#39;\in\{i-1,i+1\}\)</span>:</p>
<pre class="r"><code># matrix of pairwise overlaps
b &lt;- matrix(NA, nrow = 0, ncol = 2)
for (ij in seq(length(v) - 1)) {
  i &lt;- as.integer(gsub(&quot;:.*$&quot;, &quot;&quot;, names(v)[[ij]]))
  i1js &lt;- grep(paste0(&quot;^&quot;, i + 1, &quot;:&quot;), names(v))
  for (i1j in i1js) {
    if (length(intersect(v[[ij]], v[[i1j]])) &gt; 0) {
      b &lt;- rbind(b, c(ij, i1j))
    }
  }
}
print(b)</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    2
## [2,]    2    3
## [3,]    2    4
## [4,]    3    5
## [5,]    4    5
## [6,]    5    6</code></pre>
<p>(It’s possible, just cumbersome, to handle arbitrary covers and arbitrarily high-dimensional simplices.)</p>
</div>
<div id="visualization" class="section level2">
<h2>visualization</h2>
<p>The whole process is visualized below, also in base R (with the exception of a <a href="https://colorbrewer2.org/">ColorBrewer</a> palette), with comments separating the steps. To construct constituent frames of a gif or slideshow, just execute progressively more commented chunks from start to end.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<pre class="r"><code>plot.new()
plot.window(c(-1.25, 1.25), c(-.25, 2.75), asp = 1)
# point cloud
points(x, pch = 19, cex = .5)
# lens
lines(x = c(-2, 2), y = c(0, 0), lty = 1)
rug(f, pos = 0)
# cover
u_cols &lt;- RColorBrewer::brewer.pal(nrow(u), &quot;Set1&quot;)
segments(
  x0 = u[, 1] + .015, x1 = u[, 2] - .015,
  y0 = c(-.1, -.2), col = u_cols, lwd = 3
)
l_nudge &lt;- rep_len(c(T, F), length.out = nrow(u))
# pullback cover
rect(
  xleft = u[, 1] + .015, xright = u[, 2] - .015,
  ybottom = .15 + .2*l_nudge, ytop = 2.65 + .2*l_nudge,
  col = paste0(u_cols, &quot;77&quot;), border = NA
)
# nerve
n_lay &lt;- t(sapply(seq_along(v), function(i) {
  apply(x[v[[i]], , drop = FALSE], 2, mean)
}))
for (i in seq(nrow(b))) lines(x = n_lay[b[i, ], 1], y = n_lay[b[i, ], 2])
points(x = n_lay[, 1], y = n_lay[, 2],
       pch = 21, cex = 2, lwd = 2, bg = &quot;white&quot;,
       col = u_cols[as.integer(gsub(&quot;^([0-9]+)\\:.*$&quot;, &quot;\\1&quot;, names(v)))])</code></pre>
<p><img src="/post/2020-05-01-mapper-construction-base-r_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>I’m pretty sure that this is the only visualization of mapper i’ve seen that</p>
<ul>
<li>comes from an implementation on a non-trivial point cloud <em>and</em></li>
<li>represents every step in the construction.</li>
</ul>
<p>I general, it would be difficult to visualize the mapper internals like this, for example when <span class="math inline">\(Z\)</span> isn’t low-enough-dimensional or <span class="math inline">\(f\)</span> is not a linear projection of <span class="math inline">\(X\)</span>. But i think it poses an interesting challenge—for example, given any lens <span class="math inline">\(f:X\to\mathbb{R}\)</span>, to find a map <span class="math inline">\(g:X\to\mathbb{R}\)</span> such that <span class="math inline">\(x\mapsto(f(x),g(x))\)</span> roughly preserves distances—and i’d be keen to see the results.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I’m in a book club with a colleague on Wickham’s <a href="https://adv-r.hadley.nz/"><em>Advanced R</em></a>, and we should arrive at <a href="https://adv-r.hadley.nz/r6.html">the R6 chapter</a> in a month or two.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>OK, i’m actually using the usual distribution of R, including <code>stats::hclust()</code> for example.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>I’ve used the resulting images in a slideshow to mixed effect. Be sure to check slides on the system you’ll be using to present them!<a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><p>Several mature, open-source implementations of <a href="http://diglib.eg.org/handle/10.2312/SPBG.SPBG07.091-100">the mapper construction</a> now exist, including the standalone <a href="http://danifold.net/mapper/index.html">Python Mapper</a> module and <a href="https://kepler-mapper.scikit-tda.org/">KeplerMapper</a>, part of the scikit family. I’m currently experimenting with Matt Piekenbrock’s R package <a href="https://github.com/peekxc/Mapper/">Mapper</a>, which is not on CRAN but makes—from my beginner’s vantage point—exemplary use of the R6 system.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>What these implementations have in common is that the underlying construction is mostly hidden from view. Users can specify its constituent parts, but what gets visualized is almost exclusively either</p>
<ul>
<li>the lensed (or filtered) point cloud, usually in two dimensions; or</li>
<li>the simplicial complex, usually annotated by size and color.</li>
</ul>
<p>This is fine of course for routine experimental or practical use, but it limits the pedagogical potential of these tools.</p>
<p>The most rapidly i’ve come to understand the construction has been through implementation. In fact, it’s tedius but not terribly time-consuming to construct a mapper entirely in base R.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> The following “implementation” is taken from a document i’ve been using internally to convey the basic idea of mapper and to illustrate the effects of different choices of cover. Maybe you can find more illustrative uses for it, but i recommend against building an analysis pipeline around it!</p>
<div id="precap" class="section level2">
<h2>precap</h2>
<p>Briefly, here is some notation for the elements of the construction, adapted from <a href="https://link.springer.com/article/10.1007/s11538-019-00614-z">Mémoli and Singhal</a>:</p>
<ul>
<li><span class="math inline">\(X\)</span> – the point cloud (a finite metric space)</li>
<li><span class="math inline">\(f:X\to Z\)</span> – the lens function from <span class="math inline">\(X\)</span> to the lens space <span class="math inline">\(Z\)</span></li>
<li><span class="math inline">\(\mathcal{U}=\{U_i\}\)</span> – the cover of <span class="math inline">\(f(X)\)</span>, usually inferred from (and used interchangeably with) a finite cover of a subset <span class="math inline">\(S\subset Z\)</span> that contains <span class="math inline">\(f(X)\)</span></li>
<li><span class="math inline">\(\delta:X\times X\to\mathbb{R}_{\geq0}\)</span> – the distance metric on <span class="math inline">\(X\)</span></li>
<li><span class="math inline">\(C:\mathcal{M}\to\mathcal{P}\)</span> – the clustering method, based on <span class="math inline">\(\delta\)</span>, that sends metric spaces <span class="math inline">\(Y\subseteq X\)</span> to partitions of their elements</li>
<li><span class="math inline">\(\mathcal{V}=\{V_{ij}\}=\bigcup_i{C(f^{-1}(U_i))}\)</span> – the refinement of the pullback cover <span class="math inline">\(\{f^{-1}\mathcal{U}\}\)</span> of <span class="math inline">\(X\)</span> obtained by partitioning each subset <span class="math inline">\(f^{-1}(U_i)\subset X\)</span></li>
<li><span class="math inline">\(N\)</span> – the nerve of <span class="math inline">\(\mathcal{V}\)</span>, comprising a simplex for every non-empty intersection of sets in <span class="math inline">\(\mathcal{V}\)</span></li>
</ul>
</div>
<div id="construction" class="section level2">
<h2>construction</h2>
<p>I’ll begin with probably the most common toy example, the noisy circle. Since this isn’t part of the construction per se, i’m allowing myself the vanity of invoking <a href="https://github.com/corybrunson/tdaunif/">a manifold sampling package</a> under development with two high school interns:</p>
<pre class="r"><code>set.seed(2)
# point cloud
x &lt;- tdaunif::sample_circle(n = 120, sd = .1)
x[, 2] &lt;- x[, 2] + 1.5
print(head(x))</code></pre>
<pre><code>##                x         y
## [1,]  0.21900250 2.3144202
## [2,] -0.09167245 0.4182471
## [3,] -0.96604574 1.0946227
## [4,]  0.50825992 2.2572006
## [5,]  0.98900895 1.2088241
## [6,]  0.85559229 1.2699230</code></pre>
<p>I’ve shifted the coordinates upward for downstream plotting purposes. Note that this point cloud <span class="math inline">\(X\)</span> exists in <span class="math inline">\(\mathbb{R}^2\)</span> but that i have not yet declared a distance metric <span class="math inline">\(\delta\)</span> on <span class="math inline">\(X\)</span>.</p>
<p>The lens (or filter) <span class="math inline">\(f\)</span> is usually (a) obtained via dimensionality reduction on the <span class="math inline">\(X\)</span> or (b) taken to be a meaningful stratification variable of or function on <span class="math inline">\(X\)</span>. This example takes approach (a), projecting <span class="math inline">\(X\)</span> to its first coordinate:</p>
<pre class="r"><code># lensed point cloud
f &lt;- x[, 1]
print(head(f))</code></pre>
<pre><code>## [1]  0.21900250 -0.09167245 -0.96604574  0.50825992  0.98900895  0.85559229</code></pre>
<p>For ease of visualization as well as personal preference, i’ll use a fixed-width interval cover <span class="math inline">\(\mathcal{U}=\{U_i\}\)</span> with diameter half that of <span class="math inline">\(f(X)\)</span> and 50% overlap, which comprises five sets. I like this sort of cover because it is (uniformly) <em>2-fold</em>—every point of <span class="math inline">\(f(X)\)</span> is contained in exactly two <span class="math inline">\(U_i\)</span>:</p>
<pre class="r"><code># cover interval length
d &lt;- diff(range(f))*1/2
# cover interval centers
c &lt;- seq(min(f), max(f), d/2)
# cover intervals
u &lt;- cbind(x0 = c - d/2, x1 = c + d/2)
print(u)</code></pre>
<pre><code>##                x0           x1
## [1,] -1.748626379 -0.586016272
## [2,] -1.167321326 -0.004711219
## [3,] -0.586016272  0.576593835
## [4,] -0.004711219  1.157898888
## [5,]  0.576593835  1.739203942</code></pre>
<p>The pullback cover <span class="math inline">\(f^{-1}(\mathcal{U})\)</span> of <span class="math inline">\(X\)</span> is a straightforward step that involves no choice on the users part, though note the use of <code>&lt;=</code> and <code>&lt;</code> to ensure (up to mechanical precision) that cover set boundaries are handled correctly:</p>
<pre class="r"><code># pullback cover sets
p &lt;- apply(u, 1, function(u_i) which(u_i[1] &lt;= f &amp; f &lt; u_i[2]))
names(p) &lt;- paste0(seq(p), &quot;:&quot;)
print(lapply(p, head))</code></pre>
<pre><code>## $`1:`
## [1]  3  9 10 11 15 19
## 
## $`2:`
## [1]  2  3  9 10 11 15
## 
## $`3:`
## [1]  1  2  4  7  8 12
## 
## $`4:`
## [1] 1 4 5 6 7 8
## 
## $`5:`
## [1]  5  6 17 20 29 30</code></pre>
<p>To partition the preimages <span class="math inline">\(f^{-1}(U_i)\in f^{-1}(\mathcal{U})\)</span>, i’ve used the <a href="http://www.jmlr.org/papers/v11/carlsson10a.html">theoretically exceptional</a> method of single-linkage hierarchical clustering, but with a dangerously naïve <em>fixed</em> cutoff at height <span class="math inline">\(\frac{1}{3}\)</span>:</p>
<pre class="r"><code># clustered pullback cover sets
cl &lt;- function(p_i) {
  m &lt;- cutree(hclust(dist(x[p_i, , drop = FALSE]), method = &quot;single&quot;), h = 1/3)
  lapply(unique(m), function(v_ij) p_i[which(m == v_ij)])
}
v &lt;- unlist(lapply(p, cl), recursive = FALSE)
print(lapply(v, head))</code></pre>
<pre><code>## $`1:`
## [1]  3  9 10 11 15 19
## 
## $`2:`
## [1]  2  3  9 10 11 15
## 
## $`3:1`
## [1]  1  4  7 12 14 18
## 
## $`3:2`
## [1]  2  8 13 16 21 23
## 
## $`4:`
## [1] 1 4 5 6 7 8
## 
## $`5:`
## [1]  5  6 17 20 29 30</code></pre>
<p>While the clusters <span class="math inline">\(V_{ij}\)</span> will be encoded as vertices, higher-dimensional simplices will represent overlaps <span class="math inline">\(V_{ij}\cap V_{i&#39;j&#39;}\)</span> among them (<span class="math inline">\(i\neq i&#39;\)</span>). Happily, because the cover is 2-fold, any lensed point <span class="math inline">\(f(x)\)</span> lies in the intersection <span class="math inline">\(U_i\cap U_{i&#39;}\)</span> of at most two cover sets, so in this case we only have to worry about edges between <em>pairs</em> of vertices. Moreover, the only intersections to check are those between cover sets of adjacent indices <span class="math inline">\(i&#39;\in\{i-1,i+1\}\)</span>:</p>
<pre class="r"><code># matrix of pairwise overlaps
b &lt;- matrix(NA, nrow = 0, ncol = 2)
for (ij in seq(length(v) - 1)) {
  i &lt;- as.integer(gsub(&quot;:.*$&quot;, &quot;&quot;, names(v)[[ij]]))
  i1js &lt;- grep(paste0(&quot;^&quot;, i + 1, &quot;:&quot;), names(v))
  for (i1j in i1js) {
    if (length(intersect(v[[ij]], v[[i1j]])) &gt; 0) {
      b &lt;- rbind(b, c(ij, i1j))
    }
  }
}
print(b)</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    2
## [2,]    2    3
## [3,]    2    4
## [4,]    3    5
## [5,]    4    5
## [6,]    5    6</code></pre>
<p>(It’s possible, just cumbersome, to handle arbitrary covers and arbitrarily high-dimensional simplices.)</p>
</div>
<div id="visualization" class="section level2">
<h2>visualization</h2>
<p>The whole process is visualized below, also in base R (with the exception of a <a href="https://colorbrewer2.org/">ColorBrewer</a> palette), with comments separating the steps. To construct constituent frames of a gif or slideshow, just execute progressively more commented chunks from start to end.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<pre class="r"><code>plot.new()
plot.window(c(-1.25, 1.25), c(-.25, 2.75), asp = 1)
# point cloud
points(x, pch = 19, cex = .5)
# lens
lines(x = c(-2, 2), y = c(0, 0), lty = 1)
rug(f, pos = 0)
# cover
u_cols &lt;- RColorBrewer::brewer.pal(nrow(u), &quot;Set1&quot;)
segments(
  x0 = u[, 1] + .015, x1 = u[, 2] - .015,
  y0 = c(-.1, -.2), col = u_cols, lwd = 3
)
l_nudge &lt;- rep_len(c(T, F), length.out = nrow(u))
# pullback cover
rect(
  xleft = u[, 1] + .015, xright = u[, 2] - .015,
  ybottom = .15 + .2*l_nudge, ytop = 2.65 + .2*l_nudge,
  col = paste0(u_cols, &quot;77&quot;), border = NA
)
# nerve
n_lay &lt;- t(sapply(seq_along(v), function(i) {
  apply(x[v[[i]], , drop = FALSE], 2, mean)
}))
for (i in seq(nrow(b))) lines(x = n_lay[b[i, ], 1], y = n_lay[b[i, ], 2])
points(x = n_lay[, 1], y = n_lay[, 2],
       pch = 21, cex = 2, lwd = 2, bg = &quot;white&quot;,
       col = u_cols[as.integer(gsub(&quot;^([0-9]+)\\:.*$&quot;, &quot;\\1&quot;, names(v)))])</code></pre>
<p><img src="/post/2020-05-01-mapper-construction-base-r_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>I’m pretty sure that this is the only visualization of mapper i’ve seen that</p>
<ul>
<li>comes from an implementation on a non-trivial point cloud <em>and</em></li>
<li>represents every step in the construction.</li>
</ul>
<p>I general, it would be difficult to visualize the mapper internals like this, for example when <span class="math inline">\(Z\)</span> isn’t low-enough-dimensional or <span class="math inline">\(f\)</span> is not a linear projection of <span class="math inline">\(X\)</span>. But i think it poses an interesting challenge—for example, given any lens <span class="math inline">\(f:X\to\mathbb{R}\)</span>, to find a map <span class="math inline">\(g:X\to\mathbb{R}\)</span> such that <span class="math inline">\(x\mapsto(f(x),g(x))\)</span> roughly preserves distances—and i’d be keen to see the results.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I’m in a book club with a colleague on Wickham’s <a href="https://adv-r.hadley.nz/"><em>Advanced R</em></a>, and we should arrive at <a href="https://adv-r.hadley.nz/r6.html">the R6 chapter</a> in a month or two.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>OK, i’m actually using the usual distribution of R, including <code>stats::hclust()</code> for example.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>I’ve used the resulting images in a slideshow to mixed effect. Be sure to check slides on the system you’ll be using to present them!<a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>how to calculate aesthetics in a ggplot2 extension</title>
      <link>/2020/04/17/calculate-aesthetics/</link>
      <pubDate>2020 Apr 17 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/17/calculate-aesthetics/</guid>
      <description><div id="how-to-use-computed-variables" class="section level2">
<h2>how to use computed variables</h2>
<p>One of the many subtle features of ggplot2 is the ability to pass variables to aesthetics that are not present in the data but rather are computed internally by a statistical transformation (stat). For users, <a href="https://ggplot2.tidyverse.org/reference/stat.html">the documentation for this feature</a> illustrates the use of <code>stat(&lt;variable&gt;)</code> (previously <code>..&lt;variable&gt;..</code>, since upgraded to <code>after_stat(&lt;variable&gt;)</code>) in an aesthetic specification.</p>
<p>Some stats do this by default. For example, the count stat sends its computed <code>count</code> to both coordinate aesthetics by default. Because it requires <em>exactly</em> one of them to be specified, only the other receives the count:</p>
<pre class="r"><code>print(StatCount$default_aes)</code></pre>
<pre><code>## Aesthetic mapping: 
## * `x`      -&gt; `after_stat(count)`
## * `y`      -&gt; `after_stat(count)`
## * `weight` -&gt; 1</code></pre>
<pre class="r"><code>print(StatCount$required_aes)</code></pre>
<pre><code>## [1] &quot;x|y&quot;</code></pre>
<p>This is how the count stat supports its companion graphical element, the bar geom. This geom needs both the categorical variable from the data and the count variable computed by the stat in order to produce a <em>(frequency) bar plot</em>. The code below, which tallies the cars in the <code>mpg</code> data set by classification, makes some of this implicit control explicit:</p>
<pre class="r"><code>table(mpg$class)</code></pre>
<pre><code>## 
##    2seater    compact    midsize    minivan     pickup subcompact        suv 
##          5         47         41         11         33         35         62</code></pre>
<pre class="r"><code>ggplot(mpg) +
  stat_count(aes(x = class, y = after_stat(count)))</code></pre>
<p><img src="/post/2020-04-17-calculate-aesthetics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>More than calling single variables, <code>after_stat()</code> can also perform and return calculations involving these variables. For example, to produce a <em>relative (frequency) bar plot</em> of the classified cars, the <code>y</code> variable needs not the raw counts but what proportion they make up of the total Note the <code>y</code> axis range in this revised plot:</p>
<pre class="r"><code>ggplot(mpg) +
  stat_count(aes(x = class, y = after_stat(count / sum(count)))) +
  scale_y_continuous(labels = scales::percent)</code></pre>
<p><img src="/post/2020-04-17-calculate-aesthetics_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>More illustrations can be found in the aforelinked documentation, which are reasonably intuitive from a user’s perspective. Though it’s not immediately evident where <code>after_stat()</code> locates these computed variables and what the limits are to its ability to perform calculations on them. The documentation <code>help(after_stat)</code> thoroughly tracks the processing of an aesthetic from start to stat to scale, but again with users in mind.</p>
</div>
<div id="how-to-make-computed-variables" class="section level2">
<h2>how to make computed variables</h2>
<p>Another exceptional feature of ggplot2 is its extensibility. Users with specialized plotting needs can, with limited exposure to the package internals, write <a href="https://exts.ggplot2.tidyverse.org/gallery/">stats and geoms that produce new types of plots</a>. Because they are extensions, rather than standalone packages, they benefit from the grammatical rigor of ggplot2 and often combine well with existing stats and geoms.</p>
<p>From a developer’s perspective, especially someoene like myself with limited low-level programming experience, computed variables can appear mysterious.
Yet, they are perhaps the single easiest feature to include in a ggplot2 extension.</p>
<p>To illustrate, consider this simplified custom stat from <a href="https://ggplot2.tidyverse.org/articles/extending-ggplot2.html">the vignette on extending ggplot2</a>:</p>
<pre class="r"><code># a custom ggproto stat to fit a linear model to data
StatLm &lt;- ggproto(&quot;StatLm&quot;, Stat, 
  required_aes = c(&quot;x&quot;, &quot;y&quot;),
  
  compute_group = function(data, scales, params, n = 100, formula = y ~ x) {
    rng &lt;- range(data$x, na.rm = TRUE)
    grid &lt;- data.frame(x = seq(rng[1], rng[2], length = n))
    
    mod &lt;- lm(formula, data = data)
    grid$y &lt;- predict(mod, newdata = grid)
    
    grid
  }
)
# a corresponding stat layer
stat_lm &lt;- function(mapping = NULL, data = NULL, geom = &quot;line&quot;,
                    position = &quot;identity&quot;, na.rm = FALSE, show.legend = NA, 
                    inherit.aes = TRUE, n = 50, formula = y ~ x, 
                    ...) {
  layer(
    stat = StatLm, data = data, mapping = mapping, geom = geom, 
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(n = n, formula = formula, na.rm = na.rm, ...)
  )
}
# an illustration of the stat
ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm()</code></pre>
<p><img src="/post/2020-04-17-calculate-aesthetics_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The computation step of this linear model stat returns a data frame <code>grid</code> with two columns: a regular sequence of <code>x</code> values spanning the range of engine displacement volumes in the data set, whose number are determined by the parameter <code>n</code>; and the predicted highway speed <code>y</code> at each, according to the internally-fitted model <code>mod</code>. Notice that the data returned by the stat is differently sized than the data passed to it:</p>
<pre class="r"><code>dim(mpg)</code></pre>
<pre><code>## [1] 234  11</code></pre>
<pre class="r"><code>head(mpg)</code></pre>
<pre><code>## # A tibble: 6 x 11
##   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class 
##   &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 
## 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…
## 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…
## 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…
## 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…
## 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…
## 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…</code></pre>
<pre class="r"><code>data &lt;- transform(mpg, x = displ, y = hwy)[, c(&quot;x&quot;, &quot;y&quot;)]
dim(StatLm$compute_group(data))</code></pre>
<pre><code>## [1] 100   2</code></pre>
<pre class="r"><code>head(StatLm$compute_group(data))</code></pre>
<pre><code>##          x        y
## 1 1.600000 30.04871
## 2 1.654545 29.85613
## 3 1.709091 29.66355
## 4 1.763636 29.47098
## 5 1.818182 29.27840
## 6 1.872727 29.08582</code></pre>
<p>The predictions computed by the stat are estimates of conditional means (under a set of assumptions outside the scope of this example), and it’s often useful for a plot to encode the uncertainty of those estimates graphically. First, the uncertainty must be computed by the stat, as below by including a standard error calculation at the <code>predict()</code> step:</p>
<pre class="r"><code># the linear model ggproto stat, with a computed variable for standard error
StatLm &lt;- ggproto(&quot;StatLm&quot;, Stat, 
  required_aes = c(&quot;x&quot;, &quot;y&quot;),
  
  compute_group = function(data, scales, params, n = 100, formula = y ~ x) {
    rng &lt;- range(data$x, na.rm = TRUE)
    grid &lt;- data.frame(x = seq(rng[1], rng[2], length = n))
    
    mod &lt;- lm(formula, data = data)
    pred &lt;- predict(mod, newdata = grid, se.fit = TRUE)
    grid$y &lt;- pred$fit
    grid$yse &lt;- pred$se.fit
    
    grid
  }
)</code></pre>
<p>In addition to <code>x</code> and <code>y</code>, the data frame computed by the stat now includes a <code>yse</code> column, containing the standard errors of the predicted means contained in <code>y</code>:</p>
<pre class="r"><code>head(StatLm$compute_group(data))</code></pre>
<pre><code>##          x        y       yse
## 1 1.600000 30.04871 0.4420916
## 2 1.654545 29.85613 0.4333956
## 3 1.709091 29.66355 0.4247865
## 4 1.763636 29.47098 0.4162698
## 5 1.818182 29.27840 0.4078513
## 6 1.872727 29.08582 0.3995371</code></pre>
<p>Paired with the ribbon geom, this stat can now produce a 95% confidence band for the mean highway speeds predicted for the full range of engine displacement volumes<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>:</p>
<pre class="r"><code>ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm(geom = &quot;ribbon&quot;, alpha = .2,
          aes(ymin = after_stat(y - 2 * yse), ymax = after_stat(y + 2 * yse))) +
  stat_lm()</code></pre>
<p><img src="/post/2020-04-17-calculate-aesthetics_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>As a concluding caveat, i haven’t dug far enough into the ggplot2 source code to know exactly how the expressions fed to <code>after_stat()</code> are evaluated. In principle, if a stat returns the data frame <code>ret</code>, then <code>after_stat(&lt;expression&gt;)</code> is evaluated like <code>with(ret, &lt;expression&gt;)</code>. In particular, objects in the global environment are recognized:</p>
<pre class="r"><code>z &lt;- 3
ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm(geom = &quot;ribbon&quot;, alpha = .2,
          aes(ymin = after_stat(y - z * yse), ymax = after_stat(y + z * yse))) +
  stat_lm()</code></pre>
<p><img src="/post/2020-04-17-calculate-aesthetics_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Finally, to document new computed variables, the natural thing to do is mimic the documentation of those in the main package—for example, <a href="https://github.com/tidyverse/ggplot2/blob/master/R/stat-count.r"><code>help(stat_count)</code></a>. Here is some minimal documentation for the standard error variable:</p>
<pre class="r"><code>#&#39; @section Computed variables:
#&#39; \describe{
#&#39;   \item{yse}{standard error of predicted means}
#&#39; }</code></pre>
<p>In a pinch, the computed variables may be gleaned from the source code for the relevant compute method of the stat—if the internals are concise and tidy enough. Since the count stat performs group-wise tallies, the method to inspect is <code>StatCount$compute_group()</code>:</p>
<pre class="r"><code>print(StatCount$compute_group)</code></pre>
<pre><code>## &lt;ggproto method&gt;
##   &lt;Wrapper function&gt;
##     function (...) 
## f(..., self = self)
## 
##   &lt;Inner function (f)&gt;
##     function (self, data, scales, width = NULL, flipped_aes = FALSE) 
## {
##     data &lt;- flip_data(data, flipped_aes)
##     x &lt;- data$x
##     weight &lt;- data$weight %||% rep(1, length(x))
##     width &lt;- width %||% (resolution(x) * 0.9)
##     count &lt;- as.numeric(tapply(weight, x, sum, na.rm = TRUE))
##     count[is.na(count)] &lt;- 0
##     bars &lt;- new_data_frame(list(count = count, prop = count/sum(abs(count)), 
##         x = sort(unique(x)), width = width, flipped_aes = flipped_aes), 
##         n = length(count))
##     flip_data(bars, flipped_aes)
## }</code></pre>
<p>Indeed, <code>count</code> is not the only variable computed by the stat. The code is specialized, but it’s clear that some additional variables—<code>prop</code>, <code>x</code>, and <code>width</code>—are computed as well. The last two are meant for the paired geom; they <em>could</em> be invoked using <code>after_stat()</code>, but this is not their intended role, and they are not documented among the computed stats. The documentation serves dual purposes: enable intended use, and avert unintended use.</p>
</div>
<div id="conclusion" class="section level2">
<h2>conclusion</h2>
<p>To sum up the topic for ggplot2 extension developers:</p>
<ul>
<li>A <em>computed variable</em> is just a column of the data frame returned by <code>Stat*$compute_*()</code>.</li>
<li>Any expression involving such computed variables can be passed as a <em>calculated aesthetic</em> via <code>aes(&lt;aes&gt; = after_stat(&lt;expr&gt;))</code>.</li>
<li>Users should be able to learn about computed variables in a specific <strong>Computed variables</strong> section of the documentation for such a stat.</li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note that the <code>yse</code> are <em>not</em> standard errors for the estimates; hence, the confidence bands do not represent expected prediction errors. This confusion seems to inflame tempers, if top search results are any indication.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><div id="how-to-use-computed-variables" class="section level2">
<h2>how to use computed variables</h2>
<p>One of the many subtle features of ggplot2 is the ability to pass variables to aesthetics that are not present in the data but rather are computed internally by a statistical transformation (stat). For users, <a href="https://ggplot2.tidyverse.org/reference/stat.html">the documentation for this feature</a> illustrates the use of <code>stat(&lt;variable&gt;)</code> (previously <code>..&lt;variable&gt;..</code>, since upgraded to <code>after_stat(&lt;variable&gt;)</code>) in an aesthetic specification.</p>
<p>Some stats do this by default. For example, the count stat sends its computed <code>count</code> to both coordinate aesthetics by default. Because it requires <em>exactly</em> one of them to be specified, only the other receives the count:</p>
<pre class="r"><code>print(StatCount$default_aes)</code></pre>
<pre><code>## Aesthetic mapping: 
## * `x`      -&gt; `after_stat(count)`
## * `y`      -&gt; `after_stat(count)`
## * `weight` -&gt; 1</code></pre>
<pre class="r"><code>print(StatCount$required_aes)</code></pre>
<pre><code>## [1] &quot;x|y&quot;</code></pre>
<p>This is how the count stat supports its companion graphical element, the bar geom. This geom needs both the categorical variable from the data and the count variable computed by the stat in order to produce a <em>(frequency) bar plot</em>. The code below, which tallies the cars in the <code>mpg</code> data set by classification, makes some of this implicit control explicit:</p>
<pre class="r"><code>table(mpg$class)</code></pre>
<pre><code>## 
##    2seater    compact    midsize    minivan     pickup subcompact        suv 
##          5         47         41         11         33         35         62</code></pre>
<pre class="r"><code>ggplot(mpg) +
  stat_count(aes(x = class, y = after_stat(count)))</code></pre>
<p><img src="/post/2020-04-17-calculate-aesthetics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>More than calling single variables, <code>after_stat()</code> can also perform and return calculations involving these variables. For example, to produce a <em>relative (frequency) bar plot</em> of the classified cars, the <code>y</code> variable needs not the raw counts but what proportion they make up of the total Note the <code>y</code> axis range in this revised plot:</p>
<pre class="r"><code>ggplot(mpg) +
  stat_count(aes(x = class, y = after_stat(count / sum(count)))) +
  scale_y_continuous(labels = scales::percent)</code></pre>
<p><img src="/post/2020-04-17-calculate-aesthetics_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>More illustrations can be found in the aforelinked documentation, which are reasonably intuitive from a user’s perspective. Though it’s not immediately evident where <code>after_stat()</code> locates these computed variables and what the limits are to its ability to perform calculations on them. The documentation <code>help(after_stat)</code> thoroughly tracks the processing of an aesthetic from start to stat to scale, but again with users in mind.</p>
</div>
<div id="how-to-make-computed-variables" class="section level2">
<h2>how to make computed variables</h2>
<p>Another exceptional feature of ggplot2 is its extensibility. Users with specialized plotting needs can, with limited exposure to the package internals, write <a href="https://exts.ggplot2.tidyverse.org/gallery/">stats and geoms that produce new types of plots</a>. Because they are extensions, rather than standalone packages, they benefit from the grammatical rigor of ggplot2 and often combine well with existing stats and geoms.</p>
<p>From a developer’s perspective, especially someoene like myself with limited low-level programming experience, computed variables can appear mysterious.
Yet, they are perhaps the single easiest feature to include in a ggplot2 extension.</p>
<p>To illustrate, consider this simplified custom stat from <a href="https://ggplot2.tidyverse.org/articles/extending-ggplot2.html">the vignette on extending ggplot2</a>:</p>
<pre class="r"><code># a custom ggproto stat to fit a linear model to data
StatLm &lt;- ggproto(&quot;StatLm&quot;, Stat, 
  required_aes = c(&quot;x&quot;, &quot;y&quot;),
  
  compute_group = function(data, scales, params, n = 100, formula = y ~ x) {
    rng &lt;- range(data$x, na.rm = TRUE)
    grid &lt;- data.frame(x = seq(rng[1], rng[2], length = n))
    
    mod &lt;- lm(formula, data = data)
    grid$y &lt;- predict(mod, newdata = grid)
    
    grid
  }
)
# a corresponding stat layer
stat_lm &lt;- function(mapping = NULL, data = NULL, geom = &quot;line&quot;,
                    position = &quot;identity&quot;, na.rm = FALSE, show.legend = NA, 
                    inherit.aes = TRUE, n = 50, formula = y ~ x, 
                    ...) {
  layer(
    stat = StatLm, data = data, mapping = mapping, geom = geom, 
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(n = n, formula = formula, na.rm = na.rm, ...)
  )
}
# an illustration of the stat
ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm()</code></pre>
<p><img src="/post/2020-04-17-calculate-aesthetics_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The computation step of this linear model stat returns a data frame <code>grid</code> with two columns: a regular sequence of <code>x</code> values spanning the range of engine displacement volumes in the data set, whose number are determined by the parameter <code>n</code>; and the predicted highway speed <code>y</code> at each, according to the internally-fitted model <code>mod</code>. Notice that the data returned by the stat is differently sized than the data passed to it:</p>
<pre class="r"><code>dim(mpg)</code></pre>
<pre><code>## [1] 234  11</code></pre>
<pre class="r"><code>head(mpg)</code></pre>
<pre><code>## # A tibble: 6 x 11
##   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class 
##   &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 
## 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…
## 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…
## 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…
## 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…
## 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…
## 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…</code></pre>
<pre class="r"><code>data &lt;- transform(mpg, x = displ, y = hwy)[, c(&quot;x&quot;, &quot;y&quot;)]
dim(StatLm$compute_group(data))</code></pre>
<pre><code>## [1] 100   2</code></pre>
<pre class="r"><code>head(StatLm$compute_group(data))</code></pre>
<pre><code>##          x        y
## 1 1.600000 30.04871
## 2 1.654545 29.85613
## 3 1.709091 29.66355
## 4 1.763636 29.47098
## 5 1.818182 29.27840
## 6 1.872727 29.08582</code></pre>
<p>The predictions computed by the stat are estimates of conditional means (under a set of assumptions outside the scope of this example), and it’s often useful for a plot to encode the uncertainty of those estimates graphically. First, the uncertainty must be computed by the stat, as below by including a standard error calculation at the <code>predict()</code> step:</p>
<pre class="r"><code># the linear model ggproto stat, with a computed variable for standard error
StatLm &lt;- ggproto(&quot;StatLm&quot;, Stat, 
  required_aes = c(&quot;x&quot;, &quot;y&quot;),
  
  compute_group = function(data, scales, params, n = 100, formula = y ~ x) {
    rng &lt;- range(data$x, na.rm = TRUE)
    grid &lt;- data.frame(x = seq(rng[1], rng[2], length = n))
    
    mod &lt;- lm(formula, data = data)
    pred &lt;- predict(mod, newdata = grid, se.fit = TRUE)
    grid$y &lt;- pred$fit
    grid$yse &lt;- pred$se.fit
    
    grid
  }
)</code></pre>
<p>In addition to <code>x</code> and <code>y</code>, the data frame computed by the stat now includes a <code>yse</code> column, containing the standard errors of the predicted means contained in <code>y</code>:</p>
<pre class="r"><code>head(StatLm$compute_group(data))</code></pre>
<pre><code>##          x        y       yse
## 1 1.600000 30.04871 0.4420916
## 2 1.654545 29.85613 0.4333956
## 3 1.709091 29.66355 0.4247865
## 4 1.763636 29.47098 0.4162698
## 5 1.818182 29.27840 0.4078513
## 6 1.872727 29.08582 0.3995371</code></pre>
<p>Paired with the ribbon geom, this stat can now produce a 95% confidence band for the mean highway speeds predicted for the full range of engine displacement volumes<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>:</p>
<pre class="r"><code>ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm(geom = &quot;ribbon&quot;, alpha = .2,
          aes(ymin = after_stat(y - 2 * yse), ymax = after_stat(y + 2 * yse))) +
  stat_lm()</code></pre>
<p><img src="/post/2020-04-17-calculate-aesthetics_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>As a concluding caveat, i haven’t dug far enough into the ggplot2 source code to know exactly how the expressions fed to <code>after_stat()</code> are evaluated. In principle, if a stat returns the data frame <code>ret</code>, then <code>after_stat(&lt;expression&gt;)</code> is evaluated like <code>with(ret, &lt;expression&gt;)</code>. In particular, objects in the global environment are recognized:</p>
<pre class="r"><code>z &lt;- 3
ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm(geom = &quot;ribbon&quot;, alpha = .2,
          aes(ymin = after_stat(y - z * yse), ymax = after_stat(y + z * yse))) +
  stat_lm()</code></pre>
<p><img src="/post/2020-04-17-calculate-aesthetics_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Finally, to document new computed variables, the natural thing to do is mimic the documentation of those in the main package—for example, <a href="https://github.com/tidyverse/ggplot2/blob/master/R/stat-count.r"><code>help(stat_count)</code></a>. Here is some minimal documentation for the standard error variable:</p>
<pre class="r"><code>#&#39; @section Computed variables:
#&#39; \describe{
#&#39;   \item{yse}{standard error of predicted means}
#&#39; }</code></pre>
<p>In a pinch, the computed variables may be gleaned from the source code for the relevant compute method of the stat—if the internals are concise and tidy enough. Since the count stat performs group-wise tallies, the method to inspect is <code>StatCount$compute_group()</code>:</p>
<pre class="r"><code>print(StatCount$compute_group)</code></pre>
<pre><code>## &lt;ggproto method&gt;
##   &lt;Wrapper function&gt;
##     function (...) 
## f(..., self = self)
## 
##   &lt;Inner function (f)&gt;
##     function (self, data, scales, width = NULL, flipped_aes = FALSE) 
## {
##     data &lt;- flip_data(data, flipped_aes)
##     x &lt;- data$x
##     weight &lt;- data$weight %||% rep(1, length(x))
##     width &lt;- width %||% (resolution(x) * 0.9)
##     count &lt;- as.numeric(tapply(weight, x, sum, na.rm = TRUE))
##     count[is.na(count)] &lt;- 0
##     bars &lt;- new_data_frame(list(count = count, prop = count/sum(abs(count)), 
##         x = sort(unique(x)), width = width, flipped_aes = flipped_aes), 
##         n = length(count))
##     flip_data(bars, flipped_aes)
## }</code></pre>
<p>Indeed, <code>count</code> is not the only variable computed by the stat. The code is specialized, but it’s clear that some additional variables—<code>prop</code>, <code>x</code>, and <code>width</code>—are computed as well. The last two are meant for the paired geom; they <em>could</em> be invoked using <code>after_stat()</code>, but this is not their intended role, and they are not documented among the computed stats. The documentation serves dual purposes: enable intended use, and avert unintended use.</p>
</div>
<div id="conclusion" class="section level2">
<h2>conclusion</h2>
<p>To sum up the topic for ggplot2 extension developers:</p>
<ul>
<li>A <em>computed variable</em> is just a column of the data frame returned by <code>Stat*$compute_*()</code>.</li>
<li>Any expression involving such computed variables can be passed as a <em>calculated aesthetic</em> via <code>aes(&lt;aes&gt; = after_stat(&lt;expr&gt;))</code>.</li>
<li>Users should be able to learn about computed variables in a specific <strong>Computed variables</strong> section of the documentation for such a stat.</li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note that the <code>yse</code> are <em>not</em> standard errors for the estimates; hence, the confidence bands do not represent expected prediction errors. This confusion seems to inflame tempers, if top search results are any indication.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>the dimensions of (abstracted) polyamory literature</title>
      <link>/2019/10/25/goodreads-polyamory/</link>
      <pubDate>2019 Oct 25 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/25/goodreads-polyamory/</guid>
      <description><p>I’ve gained immensely from reading the handful of non-fiction books on polyamory i’ve made time to, including Dossie Easton and Janet Hardy’s <em>The Ethical Slut</em> and Franklin Veaux and Eve Rickert’s <em>More Than Two</em>. While i’ve identified as poly since i discovered the term in grad school, i exhibit at least my share of emotional immaturity, and in addition to actual experience building healthy relationships i know i’d benefit from funneling a bit more of this literature into my reading queue. So, with a small group of friends (which has reduced for the time being to someone i’m dating plus myself), i recently started a poly/kink book club.</p>
<p>From the start, i intended to slide fiction, fantasy/scifi, memoir, anthropology, history, and any other genres i could discover onto our shelf, to accrue a well-rounded appreciation for what was available. Though this of course put me to wondering how i could even learn the contours of the poly literature, in order to ensure that i sampled widely from it! Fortunes of timing provided me with three excellent resources:</p>
<ul>
<li><a href="https://www.goodreads.com/user/show/57466005">a Goodreads account</a>;</li>
<li><a href="https://maraaverick.rbind.io/2017/08/goodreads-part-i-rgoodreads/">a blog tutorial</a> by Mara Averick on using R packages to scrape and crunch Goodreads data; and</li>
<li><a href="https://www.goodreads.com/list/tag/polyamory">a book</a> by Julia Silge and David Robinson on doing text mining in tidyverse style.</li>
</ul>
<p>The tutorial and <a href="https://maraaverick.rbind.io/2017/10/goodreads-part-2/">its sequel</a> will get you up to speed; i’ll outline my web-scraping script and focus mostly on the analysis.</p>
<div id="scrape" class="section level2">
<h2>scrape</h2>
<p>It would be, let’s say, impractical to manually search out books with explicitly poly content or themes, and even then i’d likely miss a bunch whose descriptions don’t let on too clearly. Fortunately, Goodreads allows users both to curate thematic lists <em>and</em> to tag their lists with keywords! <a href="https://www.goodreads.com/list/tag/polyamory">Here is the collection of lists tagged “polyamory”</a>, numbering in the dozens. Helpfully, the lists span genres, including fiction, young adult, memoirs, specific configurations like triads, and space opera (natch). Less helpfully, they also range more broadly in topic and theme, for example exotica, sex positivity, and love. On the whole, though, the tag seems like a good candidate for a one-off look.</p>
<p>There are other relevant tags, of course, like <a href="https://www.goodreads.com/list/tag/open-relationships">“open-relationships”</a>. But the others i’ve found turn out to be far less sensitive and often less specific. For example, <a href="https://www.goodreads.com/list/tag/nonmonogamy">“nonmonogamy”</a> turns up two relevant lists but <a href="https://www.goodreads.com/list/tag/non-monogamy">“non monogamy”</a> turns up a third, and <a href="https://www.goodreads.com/list/tag/swinging">“swinging”</a> yields four lists, of which one is a conspicuous false positive. For simplicity, i’ll stick with the “polyamory” tag. This is something i can revisit if the results aren’t facially relevant.</p>
<p>My code (in <a href="../supplementary/">the supplementary folder</a>) scrapes first the list URLs from the meta-list (<code>polyamory_listopia</code>), then the book URLs from each list page (<code>polyamory_lists</code>), and finally metadata from each book page (<code>polyamory_books</code>): title, link, author(s), genre(s), and book description. While the authors and title serve as identifiers and the genres may serve as annotations, the descriptions constitute the raw material on which i’ll perform a text analysis. (The full texts of the books themselves are not so freely available,<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> the titles are unlikely to reliably encode recurring features, and the genres are likely too few to discriminate except between broad categories.)</p>
<p>It’s important to note that there are two tiers of redundancy in the book list, only one of which i eliminate. First, the same Goodreads <em>entry</em><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> may appear in multiple lists; second, the same <em>book</em> may have been erroneously entered into Goodreads multiple times (which may appear in different lists or even the same list). It would require a few hours of manual curation to resolve the latter problem,<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> and i haven’t put in that time here; but the first problem is easily handled using <code>group_by()</code> and <code>summarize()</code>.</p>
</div>
<div id="crunch" class="section level2">
<h2>crunch</h2>
<p>Here is the book list obtained from the scraping script, slightly tidied, using the identifying string of the URL as a unique identifier:</p>
<pre class="r"><code>library(tidyverse)
read_rds(here::here(&quot;supplementary/goodreads-polyamory-booklist.rds&quot;)) %&gt;%
  ungroup() %&gt;%
  select(title = title_page, id = link, lists, genres, description) %&gt;%
  mutate(short_title = str_replace(title, &quot;(: .+$)|( \\(.+$)&quot;, &quot;&quot;)) %&gt;%
  mutate(id = str_replace(id, &quot;/book/show/([0-9]+)[^0-9].*$&quot;, &quot;\\1&quot;)) %&gt;%
  mutate(id = as.integer(id)) %&gt;%
  print() -&gt; polyamory_booklist</code></pre>
<pre><code>## # A tibble: 1,179 x 6
##    title           id lists      genres      description     short_title   
##    &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;         
##  1 100 Love …  1.13e4 Books on … Poetry|Cla… Against the ba… 100 Love Sonn…
##  2 199 Ways …  1.61e7 Sex, Love… &quot;&quot;          199 Ways To Im… 199 Ways To I…
##  3 30th Cent…  3.53e7 Ménage Po… Science Fi… CAPTAIN JENNIF… 30th Century  
##  4 A + E 4ev…  1.24e7 The Most … Sequential… Asher Machnik … A + E 4ever   
##  5 A Bear&#39;s …  2.73e7 Ménage Po… Erotica|Me… Most days, Oli… A Bear&#39;s Jour…
##  6 A Bear&#39;s …  4.06e7 Polyfi Tr… Paranormal… Most days, Oli… A Bear&#39;s Jour…
##  7 A Bear&#39;s …  2.71e7 Ménage Po… Erotica|Me… Charlotte “Cha… A Bear&#39;s Mercy
##  8 A Bear&#39;s …  4.06e7 Polyfi Tr… Menage|M M… Charlotte “Cha… A Bear&#39;s Mercy
##  9 A Bear&#39;s …  2.68e7 Ménage Po… Erotica|Me… Quinn Taylor h… A Bear&#39;s Neme…
## 10 A Bear&#39;s …  4.05e7 Polyfi Tr… Fantasy|Pa… Quinn Taylor h… A Bear&#39;s Neme…
## # … with 1,169 more rows</code></pre>
<p>(Some of the duplicate entries are evident.) My goal here is to represent these 1,179 book entries as points (or vectors) in some low-dimensional space, based on the co-occurrence of words in their descriptions. Ideally, the coordinate dimenisons of this space will correspond to identifiable features that will help characterize the dimensions <em>and</em> allow the dimensions to characterize individual books in turn. If the number of dimensions is low enough, then it will also be possible to visualize the point cloud and coordinate vectors.</p>
<div id="word-counts" class="section level3">
<h3>word counts</h3>
<p>Adapting a workflow from <a href="https://www.tidytextmining.com/">the tidytext book</a>, i first “unnest” the book list—in <a href="https://tidyr.tidyverse.org/reference/unnest.html">the tidyr sense</a>, except using a tidytext method specific to strings of written language. “Stop” words are articles, prepositions, and other words that add little to no value to a text analysis; instances of them in the unnested data are excluded via an <a href="https://dplyr.tidyverse.org/reference/join.html">anti-join</a>. Finally, i count the number of times each word is used in each description as a new variable <span class="math inline">\(n\)</span>.</p>
<pre class="r"><code>library(tidytext)
polyamory_booklist %&gt;%
  mutate(clean_descr = str_replace_all(description, &quot;[^[:alpha:]\\s]&quot;, &quot; &quot;)) %&gt;%
  mutate(clean_descr = str_trim(clean_descr)) %&gt;%
  select(-description) %&gt;%
  unnest_tokens(word, clean_descr) %&gt;%
  anti_join(stop_words, by = &quot;word&quot;) %&gt;%
  count(title, short_title, id, lists, genres, word, name = &quot;n&quot;) %&gt;%
  print() -&gt; polybooks_wordcounts</code></pre>
<pre><code>## # A tibble: 76,194 x 7
##    title       short_title       id lists     genres         word         n
##    &lt;chr&gt;       &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;    &lt;int&gt;
##  1 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… backdrop     1
##  2 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… beloved      1
##  3 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… celebra…     1
##  4 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… de           1
##  5 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… delicate     1
##  6 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… flowers      1
##  7 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… hot          1
##  8 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… isla         1
##  9 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… love         2
## 10 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… matilde      1
## # … with 76,184 more rows</code></pre>
<p>A couple more word counts that will be useful downstream are the number <span class="math inline">\(d\)</span> of book descriptions that use each word and the total number <span class="math inline">\(m\)</span> of uses across the corpus.</p>
<pre class="r"><code>polybooks_wordcounts %&gt;%
  left_join(
    polybooks_wordcounts %&gt;%
      group_by(word) %&gt;%
      summarize(d = n(), m = sum(n)),
    by = c(&quot;word&quot;)
  ) %&gt;%
  filter(m &gt;= 12) %&gt;%
  arrange(desc(d), desc(n)) %&gt;%
  print() -&gt; polybooks_wordusages</code></pre>
<pre><code>## # A tibble: 48,020 x 9
##    title      short_title      id lists  genres     word      n     d     m
##    &lt;chr&gt;      &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 What Love… What Love Is 2.95e7 Best … Nonfictio… love     16   495  1226
##  2 The Four … The Four Lo… 3.06e4 Books… Christian… love     14   495  1226
##  3 The Futur… The Future … 7.21e5 Books… Relations… love     14   495  1226
##  4 Why We Lo… Why We Love  1.32e5 Sex, … Psycholog… love     14   495  1226
##  5 Longing f… Longing for… 1.73e7 Sex, … &quot;&quot;         love     13   495  1226
##  6 The Art o… The Art of … 1.41e4 Books… Psycholog… love     13   495  1226
##  7 Rogue Eve… Rogue Ever … 4.52e7 FFF+ … Romance|C… love     12   495  1226
##  8 Ardently:… Ardently     2.56e7 Books… Classics|… love     11   495  1226
##  9 Love Magi… Love Magic   1.36e7 Sex, … &quot;&quot;         love     10   495  1226
## 10 In Praise… In Praise o… 1.36e7 Books… Philosoph… love      9   495  1226
## # … with 48,010 more rows</code></pre>
<p>2,197 distinct words appear in these descriptions (omitting the stop words).
Happily, the most prevalent (and, it turns out, most frequent) word in these descriptions is “love”. &lt;83</p>
</div>
<div id="relative-frequencies" class="section level3">
<h3>relative frequencies</h3>
<p>Its ubiquity makes “love” unlikely to be an effective token for the purpose of mapping this book collection in coordinate space: If a feature describes everything, then it describes nothing. The traditional usefulness weighting on words is instead the <em>term frequency–inverse document frequency</em>, or <em>tf-idf</em>. This is the product of two quotients: the term frequency <span class="math inline">\(\frac{n}{N}\)</span> for a given book, where <span class="math inline">\(N\)</span> is the number of words in its description; and (the logarithm of) the inverse of the document frequency <span class="math inline">\(\frac{d}{D}\)</span>, where <span class="math inline">\(D\)</span> is the number of books (descriptions) in the corpus. The tidytext package provides a helper function for this step, <code>bind_tf_idf()</code>, which obviates the previous chunk. Additionally i’ve filtered out the less discriminating words (having maximum td-idf at most <span class="math inline">\(0.1\)</span>):</p>
<pre class="r"><code>polybooks_wordcounts %&gt;%
  bind_tf_idf(word, id, n) %&gt;%
  group_by(word) %&gt;% filter(max(tf_idf) &gt; .1) %&gt;% ungroup() %&gt;%
  select(title, short_title, id, lists, genres, word, tf_idf) %&gt;%
  print() -&gt; polybooks_tfidf</code></pre>
<pre><code>## # A tibble: 59,759 x 7
##    title       short_title       id lists     genres         word    tf_idf
##    &lt;chr&gt;       &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;
##  1 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… backdr… 0.189 
##  2 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… beloved 0.136 
##  3 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… celebr… 0.171 
##  4 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… de      0.136 
##  5 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… delica… 0.162 
##  6 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… flowers 0.189 
##  7 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… hot     0.0738
##  8 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… isla    0.212 
##  9 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… love    0.0573
## 10 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… matilde 0.235 
## # … with 59,749 more rows</code></pre>
<p>I’ll use these remaining 9,005 words to make a first pass at gauging the dimensionality of the corpus and visualizing the books and features, using classical PCA. This requires widening the table into a classical data matrix having one row per book, one column per word, and log-transformed tf-idf values (to better mimic normality). The words are capitalized to prevent conflicts with existing column names, and a separate tibble includes only the metadata.</p>
<pre class="r"><code>polybooks_tfidf %&gt;%
  mutate(log_tf_idf = log(tf_idf)) %&gt;%
  select(-tf_idf) %&gt;%
  mutate(word = toupper(word)) %&gt;%
  spread(word, log_tf_idf, fill = 0) -&gt;
  polybooks_tfidf_wide
polybooks_tfidf_wide %&gt;%
  select(title, short_title, id, lists, genres) -&gt;
  polybooks_meta</code></pre>
<p>R implementations of ordination methods tend to produce atrociously unreadable output when sample sizes and variable dimensions grow large, so this is an especially apt place to invoke <a href="https://github.com/corybrunson/ordr">ordr</a>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The ID numbers serve as unique identifiers, just in case duplicate entries for the same book have exactly the same title, and so short titles are bound back in after the PCA:</p>
<pre class="r"><code>library(ordr)
polybooks_tfidf_wide %&gt;%
  select(-title, -short_title, -lists, -genres) %&gt;%
  column_to_rownames(&quot;id&quot;) %&gt;%
  as.matrix() %&gt;%
  prcomp() %&gt;%
  as_tbl_ord() %&gt;%
  augment() %&gt;%
  bind_cols_u(select(polybooks_meta, short_title)) %&gt;%
  print() -&gt; polybooks_pca</code></pre>
<pre><code>## # A tbl_ord of class &#39;prcomp&#39;: (1170 x 1170) x (9005 x 1170)&#39;
## # 1170 coordinates: PC1, PC2, ..., PC1170
## # 
## # U: [ 1170 x 1170 | 2 ]
##     PC1     PC2      PC3 ... |   .name   short_title             
##                              |   &lt;chr&gt;   &lt;chr&gt;                   
## 1 -3.12  0.0527 -0.00351     | 1 11339   100 Love Sonnets        
## 2  2.69 -3.08    3.14    ... | 2 161489… 199 Ways To Improve You…
## 3  1.93  1.14    0.819       | 3 352771… 30th Century            
## 4  1.52 -0.958  -0.255       | 4 124467… A + E 4ever             
## 5  2.71  4.21   -1.04        | 5 272627… A Bear&#39;s Journey        
## # … with 1,165 more rows
## # 
## # V: [ 9005 x 1170 | 2 ]
##         PC1         PC2        PC3 ... |   .name       .center
##                                        |   &lt;chr&gt;         &lt;dbl&gt;
## 1  0.00340   0.0000956   0.00569       | 1 À         -0.0182  
## 2 -0.00138   0.00104    -0.000921  ... | 2 AARON     -0.00836 
## 3 -0.00236  -0.00257     0.00216       | 3 ABANDONED -0.0203  
## 4  0.000251  0.0000375  -0.0000183     | 4 ABBEY     -0.000521
## 5  0.000370  0.00000365  0.000444      | 5 ABBI      -0.00174 
## # … with 9,000 more rows</code></pre>
<p>The ubiquitous scree plot helps gauge the dimensionality of the tf-idf space, though for readability i’m restricting it to the principal components (PCs) that account for at least <span class="math inline">\(0.4\%\)</span> of the total variance each:</p>
<pre class="r"><code>polybooks_pca %&gt;%
  fortify(.matrix = &quot;coord&quot;) %&gt;%
  filter(.prop_var &gt; .004) %&gt;%
  ggplot(aes(x = .name, y = .prop_var)) +
  geom_bar(stat = &quot;identity&quot;) +
  labs(x = &quot;&quot;, y = &quot;Proportion of variance&quot;) +
  theme(axis.text.x = element_text(angle = 90))</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/pca%20scree%20plot-1.png" width="768" /></p>
<p>This is not promising: The first PC accounts for less than one fortieth of the total variation, though the remaining PCs are even less distinctive. A 1-dimensional biplot would make the most sense, but in order to add some annotation i’m extending it to 2 dimensions, with the caveat that the second, vertical dimension should be understood—for any specific slice along PC1—as a more or less arbitrary perspective on a more or less spherical cloud. I’ll highlight and label the convex hull of the projected cloud to help think about how the books are dispersed:</p>
<pre class="r"><code>ggbiplot(polybooks_pca) +
  geom_u_point(alpha = .5) +
  geom_u_point(stat = &quot;chull&quot;, color = &quot;red&quot;) +
  geom_u_label_repel(
    stat = &quot;chull&quot;, aes(label = short_title),
    color = &quot;red&quot;, alpha = .75, size = 3
  ) +
  scale_x_continuous(expand = expand_scale(mult = c(0.4, 0.1)))</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/pca%20biplot-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The biplot exhibits a common pattern, with the bulk of observations clumped together in a corner and an increasingly thin periphery pushing conically outward. This pattern tends to emerge when the underlying variables are better understood as <em>features</em> than as <em>spectra</em>: When two distinctive and mutually repulsive (not necessarily to say mutually exclusive) features describe a data set, PCA will tend to yield a boomerang shape along the first two PCs. A classic example of this is a set of clinical and laboratory test data <a href="https://link.springer.com/article/10.1007/BF00423145">collected by G.M. Reaven and R. Miller</a> for a diabetic cohort, <a href="https://cran.r-project.org/web/packages/candisc/vignettes/diabetes.html">illustrated here by Michael Friendly</a>. When more features are present and remain mutually repulsive, the resulting bouquets tend to project onto PC1 and PC2 as cones.
This is in contrast to settings in which the constituent variables are uncorrelated, in which point clouds, whether high-dimensional or projected onto PCs, tend to be spherical (as discussed in a <a href="../../../../2019/08/02/lda/">previous post</a>).</p>
<p>It might therefore make more sense to get a reading of the books farthest from the clump, i.e. with the most extreme scores along PC1, rather than those on the outskirts of the point cloud on PC1 and PC2 together.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> To that end, i’ll take the books with the top 12 scores along, and the words with the top 12 loadings onto, PC1:</p>
<pre class="r"><code>polybooks_pca %&gt;%
  tidy(.matrix = &quot;both&quot;, include = &quot;all&quot;) %&gt;%
  select(-starts_with(&quot;PC&quot;), PC1, -.center) %&gt;%
  group_by(.matrix) %&gt;%
  arrange(desc(PC1)) %&gt;% 
  top_n(12, PC1) %&gt;%
  mutate(name = ifelse(is.na(short_title), tolower(.name), short_title)) %&gt;%
  ggplot(aes(x = reorder(name, PC1), y = PC1)) +
  facet_wrap(~ .matrix, scales = &quot;free&quot;) +
  coord_flip() +
  geom_bar(stat = &quot;identity&quot;) +
  labs(x = &quot;Score / Loading&quot;)</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/unnamed-chunk-1-1.png" width="768" /></p>
<p>The scores and loadings are likewise not very discriminating, but they are suggestive of the varieties of polyamory or poly-adjacent literature that push up against its boundaries: Many of the titles suggest niche subgenres of fiction, alongside some general advice or lifestyle volumes. Words like “magical”, “vampire”, “murdered”, “ancient”, and, i’d say, even “city” and “begins” are indicative of the former, while “text” and “academy” may be of the latter.
Overall, though, this is not a very informative dimension. Rather than separating one genre from others, it’s detecting unique descriptions from a medley of genres. And this is just what one might expect from the conical shape of the point cloud.</p>
<p>So, it seems likely that discriminating features exist along which these descriptions are organized, though principal components—which by definition capture variation, not distinction—aren’t a good tool for detecting them.
This is still good news for my ultimate goal of identifying these features. In a follow-up post i’ll use a couple of different tactics, one from text mining and the other not yet widely used.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Though making word frequency data publicly available would presumably be straightforward to do and have if anything a positive impact on sales.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>An entry may have multiple editions, but these never introduced redundancies in my workflow.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>I make a habit of posting “combine requests” for the Goodreads Librarians Group whenever i come across such instances)<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Still very much a work in progress.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Though it is interesting to me that <em>Fahrenheit 451</em> appears so unremarkable through this lens.<a href="#fnref5" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><p>I’ve gained immensely from reading the handful of non-fiction books on polyamory i’ve made time to, including Dossie Easton and Janet Hardy’s <em>The Ethical Slut</em> and Franklin Veaux and Eve Rickert’s <em>More Than Two</em>. While i’ve identified as poly since i discovered the term in grad school, i exhibit at least my share of emotional immaturity, and in addition to actual experience building healthy relationships i know i’d benefit from funneling a bit more of this literature into my reading queue. So, with a small group of friends (which has reduced for the time being to someone i’m dating plus myself), i recently started a poly/kink book club.</p>
<p>From the start, i intended to slide fiction, fantasy/scifi, memoir, anthropology, history, and any other genres i could discover onto our shelf, to accrue a well-rounded appreciation for what was available. Though this of course put me to wondering how i could even learn the contours of the poly literature, in order to ensure that i sampled widely from it! Fortunes of timing provided me with three excellent resources:</p>
<ul>
<li><a href="https://www.goodreads.com/user/show/57466005">a Goodreads account</a>;</li>
<li><a href="https://maraaverick.rbind.io/2017/08/goodreads-part-i-rgoodreads/">a blog tutorial</a> by Mara Averick on using R packages to scrape and crunch Goodreads data; and</li>
<li><a href="https://www.goodreads.com/list/tag/polyamory">a book</a> by Julia Silge and David Robinson on doing text mining in tidyverse style.</li>
</ul>
<p>The tutorial and <a href="https://maraaverick.rbind.io/2017/10/goodreads-part-2/">its sequel</a> will get you up to speed; i’ll outline my web-scraping script and focus mostly on the analysis.</p>
<div id="scrape" class="section level2">
<h2>scrape</h2>
<p>It would be, let’s say, impractical to manually search out books with explicitly poly content or themes, and even then i’d likely miss a bunch whose descriptions don’t let on too clearly. Fortunately, Goodreads allows users both to curate thematic lists <em>and</em> to tag their lists with keywords! <a href="https://www.goodreads.com/list/tag/polyamory">Here is the collection of lists tagged “polyamory”</a>, numbering in the dozens. Helpfully, the lists span genres, including fiction, young adult, memoirs, specific configurations like triads, and space opera (natch). Less helpfully, they also range more broadly in topic and theme, for example exotica, sex positivity, and love. On the whole, though, the tag seems like a good candidate for a one-off look.</p>
<p>There are other relevant tags, of course, like <a href="https://www.goodreads.com/list/tag/open-relationships">“open-relationships”</a>. But the others i’ve found turn out to be far less sensitive and often less specific. For example, <a href="https://www.goodreads.com/list/tag/nonmonogamy">“nonmonogamy”</a> turns up two relevant lists but <a href="https://www.goodreads.com/list/tag/non-monogamy">“non monogamy”</a> turns up a third, and <a href="https://www.goodreads.com/list/tag/swinging">“swinging”</a> yields four lists, of which one is a conspicuous false positive. For simplicity, i’ll stick with the “polyamory” tag. This is something i can revisit if the results aren’t facially relevant.</p>
<p>My code (in <a href="../supplementary/">the supplementary folder</a>) scrapes first the list URLs from the meta-list (<code>polyamory_listopia</code>), then the book URLs from each list page (<code>polyamory_lists</code>), and finally metadata from each book page (<code>polyamory_books</code>): title, link, author(s), genre(s), and book description. While the authors and title serve as identifiers and the genres may serve as annotations, the descriptions constitute the raw material on which i’ll perform a text analysis. (The full texts of the books themselves are not so freely available,<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> the titles are unlikely to reliably encode recurring features, and the genres are likely too few to discriminate except between broad categories.)</p>
<p>It’s important to note that there are two tiers of redundancy in the book list, only one of which i eliminate. First, the same Goodreads <em>entry</em><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> may appear in multiple lists; second, the same <em>book</em> may have been erroneously entered into Goodreads multiple times (which may appear in different lists or even the same list). It would require a few hours of manual curation to resolve the latter problem,<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> and i haven’t put in that time here; but the first problem is easily handled using <code>group_by()</code> and <code>summarize()</code>.</p>
</div>
<div id="crunch" class="section level2">
<h2>crunch</h2>
<p>Here is the book list obtained from the scraping script, slightly tidied, using the identifying string of the URL as a unique identifier:</p>
<pre class="r"><code>library(tidyverse)
read_rds(here::here(&quot;supplementary/goodreads-polyamory-booklist.rds&quot;)) %&gt;%
  ungroup() %&gt;%
  select(title = title_page, id = link, lists, genres, description) %&gt;%
  mutate(short_title = str_replace(title, &quot;(: .+$)|( \\(.+$)&quot;, &quot;&quot;)) %&gt;%
  mutate(id = str_replace(id, &quot;/book/show/([0-9]+)[^0-9].*$&quot;, &quot;\\1&quot;)) %&gt;%
  mutate(id = as.integer(id)) %&gt;%
  print() -&gt; polyamory_booklist</code></pre>
<pre><code>## # A tibble: 1,179 x 6
##    title           id lists      genres      description     short_title   
##    &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;         
##  1 100 Love …  1.13e4 Books on … Poetry|Cla… Against the ba… 100 Love Sonn…
##  2 199 Ways …  1.61e7 Sex, Love… &quot;&quot;          199 Ways To Im… 199 Ways To I…
##  3 30th Cent…  3.53e7 Ménage Po… Science Fi… CAPTAIN JENNIF… 30th Century  
##  4 A + E 4ev…  1.24e7 The Most … Sequential… Asher Machnik … A + E 4ever   
##  5 A Bear&#39;s …  2.73e7 Ménage Po… Erotica|Me… Most days, Oli… A Bear&#39;s Jour…
##  6 A Bear&#39;s …  4.06e7 Polyfi Tr… Paranormal… Most days, Oli… A Bear&#39;s Jour…
##  7 A Bear&#39;s …  2.71e7 Ménage Po… Erotica|Me… Charlotte “Cha… A Bear&#39;s Mercy
##  8 A Bear&#39;s …  4.06e7 Polyfi Tr… Menage|M M… Charlotte “Cha… A Bear&#39;s Mercy
##  9 A Bear&#39;s …  2.68e7 Ménage Po… Erotica|Me… Quinn Taylor h… A Bear&#39;s Neme…
## 10 A Bear&#39;s …  4.05e7 Polyfi Tr… Fantasy|Pa… Quinn Taylor h… A Bear&#39;s Neme…
## # … with 1,169 more rows</code></pre>
<p>(Some of the duplicate entries are evident.) My goal here is to represent these 1,179 book entries as points (or vectors) in some low-dimensional space, based on the co-occurrence of words in their descriptions. Ideally, the coordinate dimenisons of this space will correspond to identifiable features that will help characterize the dimensions <em>and</em> allow the dimensions to characterize individual books in turn. If the number of dimensions is low enough, then it will also be possible to visualize the point cloud and coordinate vectors.</p>
<div id="word-counts" class="section level3">
<h3>word counts</h3>
<p>Adapting a workflow from <a href="https://www.tidytextmining.com/">the tidytext book</a>, i first “unnest” the book list—in <a href="https://tidyr.tidyverse.org/reference/unnest.html">the tidyr sense</a>, except using a tidytext method specific to strings of written language. “Stop” words are articles, prepositions, and other words that add little to no value to a text analysis; instances of them in the unnested data are excluded via an <a href="https://dplyr.tidyverse.org/reference/join.html">anti-join</a>. Finally, i count the number of times each word is used in each description as a new variable <span class="math inline">\(n\)</span>.</p>
<pre class="r"><code>library(tidytext)
polyamory_booklist %&gt;%
  mutate(clean_descr = str_replace_all(description, &quot;[^[:alpha:]\\s]&quot;, &quot; &quot;)) %&gt;%
  mutate(clean_descr = str_trim(clean_descr)) %&gt;%
  select(-description) %&gt;%
  unnest_tokens(word, clean_descr) %&gt;%
  anti_join(stop_words, by = &quot;word&quot;) %&gt;%
  count(title, short_title, id, lists, genres, word, name = &quot;n&quot;) %&gt;%
  print() -&gt; polybooks_wordcounts</code></pre>
<pre><code>## # A tibble: 76,194 x 7
##    title       short_title       id lists     genres         word         n
##    &lt;chr&gt;       &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;    &lt;int&gt;
##  1 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… backdrop     1
##  2 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… beloved      1
##  3 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… celebra…     1
##  4 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… de           1
##  5 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… delicate     1
##  6 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… flowers      1
##  7 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… hot          1
##  8 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… isla         1
##  9 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… love         2
## 10 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… matilde      1
## # … with 76,184 more rows</code></pre>
<p>A couple more word counts that will be useful downstream are the number <span class="math inline">\(d\)</span> of book descriptions that use each word and the total number <span class="math inline">\(m\)</span> of uses across the corpus.</p>
<pre class="r"><code>polybooks_wordcounts %&gt;%
  left_join(
    polybooks_wordcounts %&gt;%
      group_by(word) %&gt;%
      summarize(d = n(), m = sum(n)),
    by = c(&quot;word&quot;)
  ) %&gt;%
  filter(m &gt;= 12) %&gt;%
  arrange(desc(d), desc(n)) %&gt;%
  print() -&gt; polybooks_wordusages</code></pre>
<pre><code>## # A tibble: 48,020 x 9
##    title      short_title      id lists  genres     word      n     d     m
##    &lt;chr&gt;      &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 What Love… What Love Is 2.95e7 Best … Nonfictio… love     16   495  1226
##  2 The Four … The Four Lo… 3.06e4 Books… Christian… love     14   495  1226
##  3 The Futur… The Future … 7.21e5 Books… Relations… love     14   495  1226
##  4 Why We Lo… Why We Love  1.32e5 Sex, … Psycholog… love     14   495  1226
##  5 Longing f… Longing for… 1.73e7 Sex, … &quot;&quot;         love     13   495  1226
##  6 The Art o… The Art of … 1.41e4 Books… Psycholog… love     13   495  1226
##  7 Rogue Eve… Rogue Ever … 4.52e7 FFF+ … Romance|C… love     12   495  1226
##  8 Ardently:… Ardently     2.56e7 Books… Classics|… love     11   495  1226
##  9 Love Magi… Love Magic   1.36e7 Sex, … &quot;&quot;         love     10   495  1226
## 10 In Praise… In Praise o… 1.36e7 Books… Philosoph… love      9   495  1226
## # … with 48,010 more rows</code></pre>
<p>2,197 distinct words appear in these descriptions (omitting the stop words).
Happily, the most prevalent (and, it turns out, most frequent) word in these descriptions is “love”. &lt;83</p>
</div>
<div id="relative-frequencies" class="section level3">
<h3>relative frequencies</h3>
<p>Its ubiquity makes “love” unlikely to be an effective token for the purpose of mapping this book collection in coordinate space: If a feature describes everything, then it describes nothing. The traditional usefulness weighting on words is instead the <em>term frequency–inverse document frequency</em>, or <em>tf-idf</em>. This is the product of two quotients: the term frequency <span class="math inline">\(\frac{n}{N}\)</span> for a given book, where <span class="math inline">\(N\)</span> is the number of words in its description; and (the logarithm of) the inverse of the document frequency <span class="math inline">\(\frac{d}{D}\)</span>, where <span class="math inline">\(D\)</span> is the number of books (descriptions) in the corpus. The tidytext package provides a helper function for this step, <code>bind_tf_idf()</code>, which obviates the previous chunk. Additionally i’ve filtered out the less discriminating words (having maximum td-idf at most <span class="math inline">\(0.1\)</span>):</p>
<pre class="r"><code>polybooks_wordcounts %&gt;%
  bind_tf_idf(word, id, n) %&gt;%
  group_by(word) %&gt;% filter(max(tf_idf) &gt; .1) %&gt;% ungroup() %&gt;%
  select(title, short_title, id, lists, genres, word, tf_idf) %&gt;%
  print() -&gt; polybooks_tfidf</code></pre>
<pre><code>## # A tibble: 59,759 x 7
##    title       short_title       id lists     genres         word    tf_idf
##    &lt;chr&gt;       &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;
##  1 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… backdr… 0.189 
##  2 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… beloved 0.136 
##  3 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… celebr… 0.171 
##  4 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… de      0.136 
##  5 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… delica… 0.162 
##  6 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… flowers 0.189 
##  7 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… hot     0.0738
##  8 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… isla    0.212 
##  9 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… love    0.0573
## 10 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… matilde 0.235 
## # … with 59,749 more rows</code></pre>
<p>I’ll use these remaining 9,005 words to make a first pass at gauging the dimensionality of the corpus and visualizing the books and features, using classical PCA. This requires widening the table into a classical data matrix having one row per book, one column per word, and log-transformed tf-idf values (to better mimic normality). The words are capitalized to prevent conflicts with existing column names, and a separate tibble includes only the metadata.</p>
<pre class="r"><code>polybooks_tfidf %&gt;%
  mutate(log_tf_idf = log(tf_idf)) %&gt;%
  select(-tf_idf) %&gt;%
  mutate(word = toupper(word)) %&gt;%
  spread(word, log_tf_idf, fill = 0) -&gt;
  polybooks_tfidf_wide
polybooks_tfidf_wide %&gt;%
  select(title, short_title, id, lists, genres) -&gt;
  polybooks_meta</code></pre>
<p>R implementations of ordination methods tend to produce atrociously unreadable output when sample sizes and variable dimensions grow large, so this is an especially apt place to invoke <a href="https://github.com/corybrunson/ordr">ordr</a>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The ID numbers serve as unique identifiers, just in case duplicate entries for the same book have exactly the same title, and so short titles are bound back in after the PCA:</p>
<pre class="r"><code>library(ordr)
polybooks_tfidf_wide %&gt;%
  select(-title, -short_title, -lists, -genres) %&gt;%
  column_to_rownames(&quot;id&quot;) %&gt;%
  as.matrix() %&gt;%
  prcomp() %&gt;%
  as_tbl_ord() %&gt;%
  augment() %&gt;%
  bind_cols_u(select(polybooks_meta, short_title)) %&gt;%
  print() -&gt; polybooks_pca</code></pre>
<pre><code>## # A tbl_ord of class &#39;prcomp&#39;: (1170 x 1170) x (9005 x 1170)&#39;
## # 1170 coordinates: PC1, PC2, ..., PC1170
## # 
## # U: [ 1170 x 1170 | 2 ]
##     PC1     PC2      PC3 ... |   .name   short_title             
##                              |   &lt;chr&gt;   &lt;chr&gt;                   
## 1 -3.12  0.0527 -0.00351     | 1 11339   100 Love Sonnets        
## 2  2.69 -3.08    3.14    ... | 2 161489… 199 Ways To Improve You…
## 3  1.93  1.14    0.819       | 3 352771… 30th Century            
## 4  1.52 -0.958  -0.255       | 4 124467… A + E 4ever             
## 5  2.71  4.21   -1.04        | 5 272627… A Bear&#39;s Journey        
## # … with 1,165 more rows
## # 
## # V: [ 9005 x 1170 | 2 ]
##         PC1         PC2        PC3 ... |   .name       .center
##                                        |   &lt;chr&gt;         &lt;dbl&gt;
## 1  0.00340   0.0000956   0.00569       | 1 À         -0.0182  
## 2 -0.00138   0.00104    -0.000921  ... | 2 AARON     -0.00836 
## 3 -0.00236  -0.00257     0.00216       | 3 ABANDONED -0.0203  
## 4  0.000251  0.0000375  -0.0000183     | 4 ABBEY     -0.000521
## 5  0.000370  0.00000365  0.000444      | 5 ABBI      -0.00174 
## # … with 9,000 more rows</code></pre>
<p>The ubiquitous scree plot helps gauge the dimensionality of the tf-idf space, though for readability i’m restricting it to the principal components (PCs) that account for at least <span class="math inline">\(0.4\%\)</span> of the total variance each:</p>
<pre class="r"><code>polybooks_pca %&gt;%
  fortify(.matrix = &quot;coord&quot;) %&gt;%
  filter(.prop_var &gt; .004) %&gt;%
  ggplot(aes(x = .name, y = .prop_var)) +
  geom_bar(stat = &quot;identity&quot;) +
  labs(x = &quot;&quot;, y = &quot;Proportion of variance&quot;) +
  theme(axis.text.x = element_text(angle = 90))</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/pca%20scree%20plot-1.png" width="768" /></p>
<p>This is not promising: The first PC accounts for less than one fortieth of the total variation, though the remaining PCs are even less distinctive. A 1-dimensional biplot would make the most sense, but in order to add some annotation i’m extending it to 2 dimensions, with the caveat that the second, vertical dimension should be understood—for any specific slice along PC1—as a more or less arbitrary perspective on a more or less spherical cloud. I’ll highlight and label the convex hull of the projected cloud to help think about how the books are dispersed:</p>
<pre class="r"><code>ggbiplot(polybooks_pca) +
  geom_u_point(alpha = .5) +
  geom_u_point(stat = &quot;chull&quot;, color = &quot;red&quot;) +
  geom_u_label_repel(
    stat = &quot;chull&quot;, aes(label = short_title),
    color = &quot;red&quot;, alpha = .75, size = 3
  ) +
  scale_x_continuous(expand = expand_scale(mult = c(0.4, 0.1)))</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/pca%20biplot-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The biplot exhibits a common pattern, with the bulk of observations clumped together in a corner and an increasingly thin periphery pushing conically outward. This pattern tends to emerge when the underlying variables are better understood as <em>features</em> than as <em>spectra</em>: When two distinctive and mutually repulsive (not necessarily to say mutually exclusive) features describe a data set, PCA will tend to yield a boomerang shape along the first two PCs. A classic example of this is a set of clinical and laboratory test data <a href="https://link.springer.com/article/10.1007/BF00423145">collected by G.M. Reaven and R. Miller</a> for a diabetic cohort, <a href="https://cran.r-project.org/web/packages/candisc/vignettes/diabetes.html">illustrated here by Michael Friendly</a>. When more features are present and remain mutually repulsive, the resulting bouquets tend to project onto PC1 and PC2 as cones.
This is in contrast to settings in which the constituent variables are uncorrelated, in which point clouds, whether high-dimensional or projected onto PCs, tend to be spherical (as discussed in a <a href="../../../../2019/08/02/lda/">previous post</a>).</p>
<p>It might therefore make more sense to get a reading of the books farthest from the clump, i.e. with the most extreme scores along PC1, rather than those on the outskirts of the point cloud on PC1 and PC2 together.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> To that end, i’ll take the books with the top 12 scores along, and the words with the top 12 loadings onto, PC1:</p>
<pre class="r"><code>polybooks_pca %&gt;%
  tidy(.matrix = &quot;both&quot;, include = &quot;all&quot;) %&gt;%
  select(-starts_with(&quot;PC&quot;), PC1, -.center) %&gt;%
  group_by(.matrix) %&gt;%
  arrange(desc(PC1)) %&gt;% 
  top_n(12, PC1) %&gt;%
  mutate(name = ifelse(is.na(short_title), tolower(.name), short_title)) %&gt;%
  ggplot(aes(x = reorder(name, PC1), y = PC1)) +
  facet_wrap(~ .matrix, scales = &quot;free&quot;) +
  coord_flip() +
  geom_bar(stat = &quot;identity&quot;) +
  labs(x = &quot;Score / Loading&quot;)</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/unnamed-chunk-1-1.png" width="768" /></p>
<p>The scores and loadings are likewise not very discriminating, but they are suggestive of the varieties of polyamory or poly-adjacent literature that push up against its boundaries: Many of the titles suggest niche subgenres of fiction, alongside some general advice or lifestyle volumes. Words like “magical”, “vampire”, “murdered”, “ancient”, and, i’d say, even “city” and “begins” are indicative of the former, while “text” and “academy” may be of the latter.
Overall, though, this is not a very informative dimension. Rather than separating one genre from others, it’s detecting unique descriptions from a medley of genres. And this is just what one might expect from the conical shape of the point cloud.</p>
<p>So, it seems likely that discriminating features exist along which these descriptions are organized, though principal components—which by definition capture variation, not distinction—aren’t a good tool for detecting them.
This is still good news for my ultimate goal of identifying these features. In a follow-up post i’ll use a couple of different tactics, one from text mining and the other not yet widely used.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Though making word frequency data publicly available would presumably be straightforward to do and have if anything a positive impact on sales.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>An entry may have multiple editions, but these never introduced redundancies in my workflow.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>I make a habit of posting “combine requests” for the Goodreads Librarians Group whenever i come across such instances)<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Still very much a work in progress.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Though it is interesting to me that <em>Fahrenheit 451</em> appears so unremarkable through this lens.<a href="#fnref5" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>my reaction to Hidden Figures</title>
      <link>/2019/10/11/hidden-figures/</link>
      <pubDate>2019 Oct 11 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/11/hidden-figures/</guid>
      <description><p><em>I’m a bit overwhelmed and a bit sick this month, and i didn’t want to miss two fortnights in a row. I wrote up this reaction to seeing the film <em>Hidden Figures</em> a couple of years ago on Facebook, and several of my friends liked it then, so i’m sharing it here. It’s slightly edited. –Cory</em></p>
<p><em>Hidden Figures</em> was probably the best civil rights–focused film i’ve seen in a while—not to disparage <em>Selma</em> or <em>The Butler</em>, but my personal taste is for greater subtlety. The movie is most impressive to me, though, as a mathematician biopic.</p>
<p>In large part, this is because they did the math right, or as right as i would hope a movie to. Young Goble didn’t, in an early establishing scene, just solve an equation on the chalkboard; she explained, clearly, what she had done and why it was a reasonable approach.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> After asserting her way into a closed-door (read: white men–only) board meeting, she calculated a re-entry trajectory on demand and, without boring into every detail, provided enough insight into the key steps to give her reasonably intelligent audience a sense of how she was doing it. Most entertainingly for me (though i don’t know how historically accurately), she had the profound realization, not just on screen but out loud, that the (at the time) ancient curiosity of Euler’s method could fuse (parabolic) launch/re-entry and (elliptical) orbital trajectories into a complete, cohesive course. I don’t know that i ever would have thought to wonder whether i’d ever see (one step along) the momentous transition of applied mathematics from analytic to numerical dominance enacted as entertainment.</p>
<p>To my mind, however, its principal achievement was in rejecting the now-entrenched Hollywood stereotype of the ostracized, neuroatypical, and/or disabled mathematical genius.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> This is not to suggest that ostracized, neuroatypical, and/or disabled mathematicians don’t warrant at least their share of screen time, or that it’s a shame that their biopics came first. What’s problematic about the trend until now is that these traits have been presented as integral to the characters’ mathematical interest or ability, which reinforces the myth that mathematical talent is aberrant (one that happily appears to be fading) and does a disservice to the achievements these people did in the face of exceptional challenges.</p>
<p>Three brief examples: Leading this trend was <em>A Beautiful Mind</em>, which depicts Nash’s schizophrenic hallucinations (which i’ve since learned were contrived, as his hallucinations were neither visual nor central to his illness) inspiring him to disengage from his work for a night out, at which he has an epiphany that leads to his famous equilibrium theorem. While i opted not to see <em>The Imitation Game</em>, i understand that the title encapsulates the parallel—absent from the biography but again contrived by the filmmakers—between Turing’s reverse-engineering of the Enigma machine and his faltering attempts, from the autism spectrum, to decipher and emulate other people’s behavior. And the central mathematical conflict in <em>The Man Who Knew Infinity</em>—between Ramanujan’s reverence for the elegance of his own (sometimes false) assertions and his mentor Hardy’s demand for rigorous proofs—is framed as a proxy for the volatile conflict between Hardy’s stigmatized atheism and his student’s stigmatized religion.</p>
<p>While Goble faces serious social challenges, they are not conceived as somehow of a piece with her mathematical talent and work. Individual prejudice impedes her access to resources. Workplace segregation interferes with her performance. Structural discrimination prevents her colleagues from advancing their careers. These obstacles are also externally imposed; Goble herself is mature, competent, and graceful, and the film sees her through an emotional trajectory—making time for her children, falling in love, navigating an exciting and stressful work environment—that is refreshingly unrelated to her genius. And it’s not like Goble’s typicality was essential to this. None of Nash’s, Turing’s, Ramanujan’s, and Hardy’s stories called for the gimmicking they received.</p>
<p>To the extent that future biopics about mathematicians eschew such gimmicks, they’ll owe some credit to <em>Hidden Figures</em>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>She solved a quartic polynomial, presented as a product of quadratics, by factoring each factor into two binomials.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This is, of course, based on my own impressions and expectations.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><p><em>I’m a bit overwhelmed and a bit sick this month, and i didn’t want to miss two fortnights in a row. I wrote up this reaction to seeing the film <em>Hidden Figures</em> a couple of years ago on Facebook, and several of my friends liked it then, so i’m sharing it here. It’s slightly edited. –Cory</em></p>
<p><em>Hidden Figures</em> was probably the best civil rights–focused film i’ve seen in a while—not to disparage <em>Selma</em> or <em>The Butler</em>, but my personal taste is for greater subtlety. The movie is most impressive to me, though, as a mathematician biopic.</p>
<p>In large part, this is because they did the math right, or as right as i would hope a movie to. Young Goble didn’t, in an early establishing scene, just solve an equation on the chalkboard; she explained, clearly, what she had done and why it was a reasonable approach.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> After asserting her way into a closed-door (read: white men–only) board meeting, she calculated a re-entry trajectory on demand and, without boring into every detail, provided enough insight into the key steps to give her reasonably intelligent audience a sense of how she was doing it. Most entertainingly for me (though i don’t know how historically accurately), she had the profound realization, not just on screen but out loud, that the (at the time) ancient curiosity of Euler’s method could fuse (parabolic) launch/re-entry and (elliptical) orbital trajectories into a complete, cohesive course. I don’t know that i ever would have thought to wonder whether i’d ever see (one step along) the momentous transition of applied mathematics from analytic to numerical dominance enacted as entertainment.</p>
<p>To my mind, however, its principal achievement was in rejecting the now-entrenched Hollywood stereotype of the ostracized, neuroatypical, and/or disabled mathematical genius.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> This is not to suggest that ostracized, neuroatypical, and/or disabled mathematicians don’t warrant at least their share of screen time, or that it’s a shame that their biopics came first. What’s problematic about the trend until now is that these traits have been presented as integral to the characters’ mathematical interest or ability, which reinforces the myth that mathematical talent is aberrant (one that happily appears to be fading) and does a disservice to the achievements these people did in the face of exceptional challenges.</p>
<p>Three brief examples: Leading this trend was <em>A Beautiful Mind</em>, which depicts Nash’s schizophrenic hallucinations (which i’ve since learned were contrived, as his hallucinations were neither visual nor central to his illness) inspiring him to disengage from his work for a night out, at which he has an epiphany that leads to his famous equilibrium theorem. While i opted not to see <em>The Imitation Game</em>, i understand that the title encapsulates the parallel—absent from the biography but again contrived by the filmmakers—between Turing’s reverse-engineering of the Enigma machine and his faltering attempts, from the autism spectrum, to decipher and emulate other people’s behavior. And the central mathematical conflict in <em>The Man Who Knew Infinity</em>—between Ramanujan’s reverence for the elegance of his own (sometimes false) assertions and his mentor Hardy’s demand for rigorous proofs—is framed as a proxy for the volatile conflict between Hardy’s stigmatized atheism and his student’s stigmatized religion.</p>
<p>While Goble faces serious social challenges, they are not conceived as somehow of a piece with her mathematical talent and work. Individual prejudice impedes her access to resources. Workplace segregation interferes with her performance. Structural discrimination prevents her colleagues from advancing their careers. These obstacles are also externally imposed; Goble herself is mature, competent, and graceful, and the film sees her through an emotional trajectory—making time for her children, falling in love, navigating an exciting and stressful work environment—that is refreshingly unrelated to her genius. And it’s not like Goble’s typicality was essential to this. None of Nash’s, Turing’s, Ramanujan’s, and Hardy’s stories called for the gimmicking they received.</p>
<p>To the extent that future biopics about mathematicians eschew such gimmicks, they’ll owe some credit to <em>Hidden Figures</em>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>She solved a quartic polynomial, presented as a product of quadratics, by factoring each factor into two binomials.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This is, of course, based on my own impressions and expectations.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>defining and taxonomizing alluvial diagrams</title>
      <link>/2019/09/13/flow-taxonomy/</link>
      <pubDate>2019 Sep 13 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/13/flow-taxonomy/</guid>
      <description><div id="background" class="section level2">
<h2>Background</h2>
<p>I first encountered alluvial diagrams, so-called, in a widely-shared <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0008694">paper</a> by Martin Rosvall and Carl T. Bergstrom. They were investigating the shifting boundaries between distinct scientific fields (“modules”), as reconstructed from sequential years of journal citation data (“states”), and proposed a specialized flow diagram “to highlight the significant changes, fusions, and fissions that the modules undergo between each pair of successive states”. At the time, i was unfamiliar with Sankey diagrams and only passingly familiar with flow diagrams—mostly by way of my computational biologist colleagues and their literature-derived signal transduction networks—but i could imagine myriad uses for this type of visualization and eventually went searching for an implementation in R.</p>
<p>This led me to the <a href="https://github.com/mbojan/alluvial">alluvial</a> package, which Michał Bojanowski was actively developing. As i got more comfortable with and excited about the then-ascendant tidyverse, i took it upon myself to put together <a href="https://github.com/corybrunson/ggalluvial">a ggplot2 extension</a>, which has since become considerably widely used. I later learned that previous developers had released similar extensions, specifically “parallel sets plots” in Thomas Lin Pedersen’s <a href="https://ggforce.data-imaginist.com">ggforce</a> and in Heike Hofmann and Marie Vendettouli’s <a href="https://github.com/heike/ggparallel">ggparallel</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
With continual exposure to these diverse implementations, i began to notice some subtle but important distinctions between others’ and my design principles. It also became clear that there was no consensus distinction between <em>alluvial</em> diagrams/plots and the more well-established genres of <em>Sankey</em> diagrams and <em>parallel sets</em> plots—indeed, no consensus on whether a distinction existed! Based on <a href="https://github.com/corybrunson/ggalluvial/issues">the ggalluvial issues page</a> and <a href="https://stackoverflow.com/search?q=ggalluvial">an occasional query on Stack Overflow</a>, the lack of generally accepted terms surrounding these sorts of diagrams remains a source of confusion.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> My goal in this post is to propose some vocabulary and a taxonomy to <del>alluviate</del> alleviate this confusion, or at least serve as a point of reference for others to propose improvements!</p>
</div>
<div id="a-proposed-taxonomy-for-width-encoded-diagrams" class="section level2">
<h2>A proposed taxonomy for width-encoded diagrams</h2>
<p>Since the terms “diagram” and “plot” (along with “chart”) are sometimes used interchangeably and sometimes fiercely contested, i’ll adopt a convention here and invite suggestions to improve it: <strong>Diagrams</strong> visualize information, <strong>charts</strong> are diagrams whose information is stored as data, and <strong>plots</strong> are charts that are uniquely determined from data by a fixed set of plotting rules.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
In these terms, ggalluvial and the other R packages discussed here unambiguously produce <em>plots</em>.</p>
<p>Here are how i think these various diagrams—flow, Sankey, parallel sets, and alluvial—are related:</p>
<ol style="list-style-type: decimal">
<li><em>Flow diagrams encode directed flows.</em> Flow diagrams may use ribbons, arrows, or other graphical elements to represent flows—that is, directed processes. For example, directed network diagrams of resource transmission between nodes are flow diagrams. Note that flows are not necessarily between nodes: Many Sankey diagrams include incoming or outgoing arrows representing flows from or to elements outside the diagram.</li>
<li><em>Sankey diagrams are flow diagrams with flow weights encoded as ribbon widths.</em> Flow diagrams may be unweighted: Signal transduction networks, for example, usually are. While Sankey diagrams are extremely flexible, their defining characteristic is that weighted flows are represented by ribbons whose widths indicate the weights or “volumes” of flows through them.</li>
<li><a href="https://datascience.blog.wzb.eu/2016/09/27/parallel-coordinate-plots-for-discrete-and-categorical-data-in-r-a-comparison/">Parallel coordinates plots</a> depict cases in a data set by their coordinates along several continuous dimensions, i.e. their values at several continuous variables, arrayed along a discrete axis. <em>Parallel sets plots are analogous to parallel coordinates plots with discrete-valued classificatory dimensions in place of continuous-valued coordinate dimensions.</em> This requires that the plotting rules both determine the order of the classes along each dimension and preserve their relative sizes. In practice, this means that ribbon widths indicate either the absolute weights of the cases or their proportions of the total weight.</li>
<li><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em> This yields a plot with a meaningful continuous axis perpendicular to the discrete axis: The height of a stack at any class is the cumulative weight of the preceding classes, and the stacked sets at different dimensions can be directly compared as stacked bar plots. (Alluvial plots also tend to use splines rather than segments to delineate ribbons, though i think this is far less important than the rules that position the sets and ribbon boundaries.)</li>
</ol>
<div id="examples" class="section level3">
<h3>Examples</h3>
<p>I’m not rendering any plots in this post, so i’ll illustrate these distinctions by pointing to several examples of each, with an emphasis on examples that are “mislabeled” according to my taxonomy:</p>
<ol style="list-style-type: decimal">
<li>Flow diagrams that are <em>not</em> Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:Typical_Signal_Schedule_and_Traffic_Flow_Diagram,_North-South_across_Market_(1929).png">Typical Signal Schedule and Traffic Flow Diagram</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:VA_Business_Line_Finance_and_Accounting.jpg">VA Business Line Finance and Accounting</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F2/">Process flow diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:JIE_Sankey_V5_Fig1.png">The Thermal Efficiency of Steam Engines</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_of_US_Consumer_Expenditure_in_2012.jpg">Sankey Diagram of US Consumer Expenditure in 2012</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Earth_heat_balance_Sankey_diagram.svg">Earth heat balance Sankey diagram</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F3/">Sankey diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Parallel sets plots that are <em>not</em> alluvial plots:
<ul>
<li><a href="https://www.jasondavies.com/parallel-sets/">Titanic Survivors</a>, Jason Davies</li>
<li><a href="https://xeno.graphics/stacked-area-alluvial-diagram/">Stacked area alluvial diagram</a>, Xenographics (note that the vertical axis applies only to the area plot)</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_-_Income_Statement.jpg">Sankey Diagram - Income Statement</a>, Wikimedia Commons</li>
<li><a href="https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0008694.g003">Mapping change in science</a>, <em>PLoS ONE</em></li>
<li><a href="https://www.researchgate.net/figure/Figure-S1-Sankey-diagram-on-global-green-virtual-water-flows-Sankey_fig5_303306020">Sankey diagram on global green virtual water flows</a>, Serrano, Guan, Duarte, and Paavola (2016)</li>
</ul></li>
<li>Alluvial plots:
<ul>
<li><a href="https://www.theinformationlab.co.uk/2018/03/09/build-sankey-diagram-tableau-without-data-prep-beforehand/">Superstore’s Super Sankey</a>, The Information Lab</li>
<li><a href="https://www.theguardian.com/politics/2016/may/06/holyrood-elections-see-rise-of-team-ruth-and-demise-of-labour-vision">How Scotland’s political geography changed, seat by seat</a>, <em>The Guardian</em></li>
<li><a href="https://www.researchgate.net/figure/Alluvial-diagram-for-mapping-changes-in-the-Global-network-The-top-ten-communities_fig6_267734552">Alluvial diagram for mapping changes in the Global network</a>, Lu and Brelsford (2014)</li>
</ul></li>
</ol>
</div>
<div id="how-my-proposal-stacks-up" class="section level3">
<h3>How my proposal stacks up</h3>
<p>Without exhaustively surveying the Internet and technical literature, it’s worthwhile to benchmark my distinctions against those made by some popular chart catalogues, which are much more representative of usage patterns. Xenographics lists several at the end of <a href="https://xeno.graphics/articles/on-graphonyms-the-importance-of-chart-type-names/">their discussion of graphonyms</a>, and of these three make clear distinctions between some of the types described above:</p>
<ul>
<li><a href="https://datavizproject.com/">The DataViz Project</a> describes Sankey diagrams as i did above, and distinguishes alluvial plots from parallel sets plots only in terms of the orientation of the axes (which is horizontal versus vertical) and of the shapes of the connecting ribbons.</li>
<li><a href="https://datavizcatalogue.com">The Data Visualization Catalogue</a> distinguishes parallel sets plots from Sankey diagrams as not using arrows and binning the flows (ribbons) at regular intervals. (They don’t have an entry on alluvial plots.)</li>
<li><a href="http://visualizationuniverse.com/charts/">The Visualization Universe</a> has entries for Sankey diagrams and alluvial plots, but their descriptions are excerpts (or, at least, subsets) of those of the DataViz Project.</li>
</ul>
<p>Importantly, in my taxonomy, by and large, <strong>alluvial plots are not Sankey diagrams, nor even flow diagrams</strong>. This seems appropriate on reflection, since even the original alluvial diagrams did not represent the transmission of material or information between nodes but changes in classification over time.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> This is not to say that alluvial plots cannot represent flow data—several popular examples do—but that the plot elements are not <em>specific</em> to flow data; and that, as a result, the directedness of flow data may not be conveyed well in an alluvial plot.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> I seem to be in agreement with the popular catalogues here.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>A more subtle pattern of usage reflected in my proposal is that parallel sets plots may resize flows from dimension to dimension to reflect changes in proportion that do not necessarily correspond to chances in amount. This must be done carefully in such plots, but it would be anathema to a Sankey diagram.
I’ll also endorse Elijah Meeks’ point, <a href="https://medium.com/@Elijah_Meeks/alluvial-charts-and-their-discontents-10a77d55216b">from this Medium post</a>, that Sankey diagrams can include cycles, whereas parallel sets and alluvial plots would not be able to encode such patterns.</p>
<p>I’m also making a distinction between alluvial and parallel sets plots that the catalogues—and, in my experience, other developers—don’t make.
Indeed, i haven’t seen a discussion anywhere else of whether a parallel sets or alluvial plot puts repeated categories in the same order, or whether the parallel sets are stacked so that the perpendicular axis measures their cumulative size, or whether either of these features is relevant to the choice of which type of plot is better-suited to a given purpose.
Having spent hours on the problem of whether different orderings might benefit a plot, and having been prompted several times to implement gaps between the sets, i’ve come to take the strong position that this distinction matters and that the terminology should reflect it.
While good definitions describe usage rather than prescribe it, i think it’s worth making an argument for this particular technical distinction while the terminology has not yet, ahem, sedemented.</p>
</div>
</div>
<div id="a-prescription-for-distinguishing-alluvial-and-parallel-sets-plots" class="section level2">
<h2>A prescription for distinguishing alluvial and parallel sets plots</h2>
<p>To reiterate:</p>
<blockquote>
<p><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em></p>
</blockquote>
<div id="caveat" class="section level3">
<h3>Caveat</h3>
<p>Neither Rosvall and Bergstrom, who popularized alluvial diagrams, nor Bojanowski, on whose package i based ggalluvial, included a cumulative weight axis perpendicular to the dimensions axis. In fact, what originally prompted me to omit the gaps between strata in ggalluvial was that i didn’t know how to get rid of the vertical axis! Like every great idea i believe i’ve had, i arrived at this one via gradient descent.</p>
<p>That’s still not to say i was first: Hofmann and Vendettouli wrote ggparallel to stack the sets in each dimension and retain a vertical axis in their plots. There may well be other such implementations, but i’m most familiar with the R ecosystem.</p>
</div>
<div id="utility" class="section level3">
<h3>Utility</h3>
<p>First, i claim that the features that distinguish alluvial from parallel sets plots—consistent ordering of sets and a cumulative weight axis—have practical importance.
In particular, they have importance <em>beyond</em> the ability to visually distinguish the sets and compare their weights along each dimension, as can be done from any parallel sets plot.
The use cases i’ve surveyed reveal three distinct settings in which alluvial plots are superior to other parallel sets plots:<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p><strong>Repeated categorical measures data:</strong>
Several users have used alluvial plots to represent data consisting of partitions of cases into the same (or overlapping) classification schemes at different times, in particular before and after some intervention or other significant event.
See scholarly examples in <a href="https://www.sciencedirect.com/science/article/pii/S0378429018317337">Baudron, Ndoli, Habarurema, and Silva (2019)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0195925518302026">Kissinger and Reznik (2019)</a>, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/gec3.12441">Muenchow, Schäfer, and Krüger (2019)</a>, <a href="https://www.jneurosci.org/content/39/28/5534.abstract">Chong et al (2019)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/ejhf.1547">Schlotter et al (2019)</a>, and tweeted examples by <a href="https://twitter.com/KenSteif/status/1006542071375761408">KenSteif</a>, <a href="https://twitter.com/frau_dr_barber/status/1130167116164927488">frau_dr_barber</a>, <a href="https://twitter.com/ericpgreen/status/1133840554968666112">ericpgreen</a>, and <a href="https://twitter.com/5amStats/status/1135153961227509762">5amStats</a> (starboard image).
For these diagrams to communicate the data efficiently, it is essential that the classes be consistently ordered.
Some implementations of parallel sets plots default to this behavior, as <a href="https://matthewdharris.com/2017/11/11/a-brief-diversion-into-static-alluvial-sankey-diagrams-in-r/">showcased by Matt Harris</a>; but others may automatically sort the sets by size, and still others allow arbitrary orderings—which may be useful for interactive exploration, as with Meeks’ implementation, but inappropriate for static renderings.</p>
<p><strong>Multipartite network data:</strong>
A handful of users have used alluvial plots to represent multipartite graphs, which necessarily satisfy the constraint that the total degree of the nodes in each part is the same. In particular, genomic analyses produce one-to-one connections among stages in transcription processes (lncRNA, miRNA, and mRNA), which have been encoded into alluvial plots by <a href="https://peerj.com/articles/6091/">Zheng et al (2018)</a>, <a href="https://cancerci.biomedcentral.com/articles/10.1186/s12935-019-0817-y">Long et al (2019a)</a>, and <a href="https://www.frontiersin.org/articles/10.3389/fonc.2019.00649/full">Long et al (2019b)</a>. See <a href="https://www.frontiersin.org/articles/10.3389/fimmu.2019.00660/full">Vazquez Bernat et al (2019)</a> for a similar usage, and <a href="https://twitter.com/MyriamCTraub/status/1169236685160402946">MyriamCTraub</a> and <a href="https://twitter.com/BenMoretti/status/1100378930865827840">BenMoretti</a> on Twitter.
A similar principle is at work in <a href="https://watanabesmith.rbind.io/post/ranked-black-mirror/">Watanabe Smith’s illustration of ranked-choice voting</a>, especially with respect to those occasions when a voter lost influence by only ranking a few of the options (which segues into the next setting).
While these users excluded the cumulative weight axis, through the fixed heights of the stacked sets the plots communicate that the total degree at each stage is the same.</p>
<p><strong>Censored data:</strong>
Finally, something i think alluvial plots do exceptionally better than general parallel sets plots is accentuate censoredness in data. Check out scholarly articles by <a href="https://www.sciencedirect.com/science/article/pii/S1075996418301021">Seekatz et al (2018)</a> and <a href="https://journals.lww.com/ccmjournal/Fulltext/2019/01000/Evaluating_Delivery_of_Low_Tidal_Volume.8.aspx">Sjoding, Gong, Hass, and Iwashyna (2019)</a> and <a href="https://mdneuzerling.com/post/my-data-science-job-hunt/">David Neuzerling’s reflections on the job hunt</a>, which use alluvial plots to depict changes in subjects’ status across several time points or stages with a specific set (or blank space where it would be) for subjects who became unavailable later in the study. The cumulative weight axis and gridlines allow the reader to immediately discern the reduction in sample size at each step.
(Though they use network data, <a href="https://www.nature.com/articles/srep06773">Lu and Brelsford (2014)</a> make similar use of this property to visualize non-connections between sets together with connections.)</p>
<p>That’s the substance of my argument for the alluvial–parallel sets distinction, but i’ll finish with a bit of fluorish.</p>
</div>
<div id="connotativity" class="section level3">
<h3>Connotativity</h3>
<p>The special features of alluvial plots are connoted by their peculiar terminology: I’ve decided to call the rectangles representing the parallel sets “strata” to suggest that they are more stable than the crisscrossing alluvia, and indeed this stability (with respect to their order in the plot) is what users expect when many of the categorical dimensions classify subjects into the same categories. The term also suggests, as does “alluvia”, that the various sets into which the subjects are partitioned at each (usually horizontal) position along the dimension axis are themselves (vertically) positioned in accordance with gravity. That is, they have “settled” one atop another with no defiantly empty space in between.</p>
<p>Thus i deposit my case.</p>
</div>
</div>
<div id="coda" class="section level2">
<h2>Coda</h2>
<p>I am by no means an expert in data visualization! While i feel strongly that my taxonomy makes the best of the present scrambling of terms, i am quite open to countersuggestions and especially to use cases that undercut it. If you come across them, please do send them my way.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I only just discovered Yawei Ge and Hofmann’s <a href="https://yaweige.github.io/ggpcp/">ggpcp</a> package for general parallel coordinate plots, under active development!<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>That’s not to say that useful distinctions haven’t been made somewhere, e.g. the technical literature on data visualization, but i feel confident in claiming that they have had limited effects on practice!<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>In ggplot2, these rules are the stat, geom, coord, and scale layers. One of the great contributions of ggplot2, in my view, was to make them explicit to lay users like myself.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>It’s unfortunate that i named the ribbons between adjacent axes <em>flows</em> in ggalluvial, but i maintain that it was preferable to calling them <em>fans</em>.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>A similar misfit is a simplicial complex represented by a network diagram: The type of diagram is designed for a different type of data (pairwise-relational with possible directedness and multiplicity) and fails to convey essential data elements (higher-dimensional simplices).<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>A contrary example is RAWGraphs, which <a href="https://rawgraphs.io/learning/how-to-make-an-alluvial-diagram/">describes alluvial plots</a> as “a specific kind of Sankey diagrams”.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>I found most of these examples by searching for “ggalluvial” in Google Scholar or Twitter.<a href="#fnref7" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><div id="background" class="section level2">
<h2>Background</h2>
<p>I first encountered alluvial diagrams, so-called, in a widely-shared <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0008694">paper</a> by Martin Rosvall and Carl T. Bergstrom. They were investigating the shifting boundaries between distinct scientific fields (“modules”), as reconstructed from sequential years of journal citation data (“states”), and proposed a specialized flow diagram “to highlight the significant changes, fusions, and fissions that the modules undergo between each pair of successive states”. At the time, i was unfamiliar with Sankey diagrams and only passingly familiar with flow diagrams—mostly by way of my computational biologist colleagues and their literature-derived signal transduction networks—but i could imagine myriad uses for this type of visualization and eventually went searching for an implementation in R.</p>
<p>This led me to the <a href="https://github.com/mbojan/alluvial">alluvial</a> package, which Michał Bojanowski was actively developing. As i got more comfortable with and excited about the then-ascendant tidyverse, i took it upon myself to put together <a href="https://github.com/corybrunson/ggalluvial">a ggplot2 extension</a>, which has since become considerably widely used. I later learned that previous developers had released similar extensions, specifically “parallel sets plots” in Thomas Lin Pedersen’s <a href="https://ggforce.data-imaginist.com">ggforce</a> and in Heike Hofmann and Marie Vendettouli’s <a href="https://github.com/heike/ggparallel">ggparallel</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
With continual exposure to these diverse implementations, i began to notice some subtle but important distinctions between others’ and my design principles. It also became clear that there was no consensus distinction between <em>alluvial</em> diagrams/plots and the more well-established genres of <em>Sankey</em> diagrams and <em>parallel sets</em> plots—indeed, no consensus on whether a distinction existed! Based on <a href="https://github.com/corybrunson/ggalluvial/issues">the ggalluvial issues page</a> and <a href="https://stackoverflow.com/search?q=ggalluvial">an occasional query on Stack Overflow</a>, the lack of generally accepted terms surrounding these sorts of diagrams remains a source of confusion.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> My goal in this post is to propose some vocabulary and a taxonomy to <del>alluviate</del> alleviate this confusion, or at least serve as a point of reference for others to propose improvements!</p>
</div>
<div id="a-proposed-taxonomy-for-width-encoded-diagrams" class="section level2">
<h2>A proposed taxonomy for width-encoded diagrams</h2>
<p>Since the terms “diagram” and “plot” (along with “chart”) are sometimes used interchangeably and sometimes fiercely contested, i’ll adopt a convention here and invite suggestions to improve it: <strong>Diagrams</strong> visualize information, <strong>charts</strong> are diagrams whose information is stored as data, and <strong>plots</strong> are charts that are uniquely determined from data by a fixed set of plotting rules.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
In these terms, ggalluvial and the other R packages discussed here unambiguously produce <em>plots</em>.</p>
<p>Here are how i think these various diagrams—flow, Sankey, parallel sets, and alluvial—are related:</p>
<ol style="list-style-type: decimal">
<li><em>Flow diagrams encode directed flows.</em> Flow diagrams may use ribbons, arrows, or other graphical elements to represent flows—that is, directed processes. For example, directed network diagrams of resource transmission between nodes are flow diagrams. Note that flows are not necessarily between nodes: Many Sankey diagrams include incoming or outgoing arrows representing flows from or to elements outside the diagram.</li>
<li><em>Sankey diagrams are flow diagrams with flow weights encoded as ribbon widths.</em> Flow diagrams may be unweighted: Signal transduction networks, for example, usually are. While Sankey diagrams are extremely flexible, their defining characteristic is that weighted flows are represented by ribbons whose widths indicate the weights or “volumes” of flows through them.</li>
<li><a href="https://datascience.blog.wzb.eu/2016/09/27/parallel-coordinate-plots-for-discrete-and-categorical-data-in-r-a-comparison/">Parallel coordinates plots</a> depict cases in a data set by their coordinates along several continuous dimensions, i.e. their values at several continuous variables, arrayed along a discrete axis. <em>Parallel sets plots are analogous to parallel coordinates plots with discrete-valued classificatory dimensions in place of continuous-valued coordinate dimensions.</em> This requires that the plotting rules both determine the order of the classes along each dimension and preserve their relative sizes. In practice, this means that ribbon widths indicate either the absolute weights of the cases or their proportions of the total weight.</li>
<li><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em> This yields a plot with a meaningful continuous axis perpendicular to the discrete axis: The height of a stack at any class is the cumulative weight of the preceding classes, and the stacked sets at different dimensions can be directly compared as stacked bar plots. (Alluvial plots also tend to use splines rather than segments to delineate ribbons, though i think this is far less important than the rules that position the sets and ribbon boundaries.)</li>
</ol>
<div id="examples" class="section level3">
<h3>Examples</h3>
<p>I’m not rendering any plots in this post, so i’ll illustrate these distinctions by pointing to several examples of each, with an emphasis on examples that are “mislabeled” according to my taxonomy:</p>
<ol style="list-style-type: decimal">
<li>Flow diagrams that are <em>not</em> Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:Typical_Signal_Schedule_and_Traffic_Flow_Diagram,_North-South_across_Market_(1929).png">Typical Signal Schedule and Traffic Flow Diagram</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:VA_Business_Line_Finance_and_Accounting.jpg">VA Business Line Finance and Accounting</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F2/">Process flow diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:JIE_Sankey_V5_Fig1.png">The Thermal Efficiency of Steam Engines</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_of_US_Consumer_Expenditure_in_2012.jpg">Sankey Diagram of US Consumer Expenditure in 2012</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Earth_heat_balance_Sankey_diagram.svg">Earth heat balance Sankey diagram</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F3/">Sankey diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Parallel sets plots that are <em>not</em> alluvial plots:
<ul>
<li><a href="https://www.jasondavies.com/parallel-sets/">Titanic Survivors</a>, Jason Davies</li>
<li><a href="https://xeno.graphics/stacked-area-alluvial-diagram/">Stacked area alluvial diagram</a>, Xenographics (note that the vertical axis applies only to the area plot)</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_-_Income_Statement.jpg">Sankey Diagram - Income Statement</a>, Wikimedia Commons</li>
<li><a href="https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0008694.g003">Mapping change in science</a>, <em>PLoS ONE</em></li>
<li><a href="https://www.researchgate.net/figure/Figure-S1-Sankey-diagram-on-global-green-virtual-water-flows-Sankey_fig5_303306020">Sankey diagram on global green virtual water flows</a>, Serrano, Guan, Duarte, and Paavola (2016)</li>
</ul></li>
<li>Alluvial plots:
<ul>
<li><a href="https://www.theinformationlab.co.uk/2018/03/09/build-sankey-diagram-tableau-without-data-prep-beforehand/">Superstore’s Super Sankey</a>, The Information Lab</li>
<li><a href="https://www.theguardian.com/politics/2016/may/06/holyrood-elections-see-rise-of-team-ruth-and-demise-of-labour-vision">How Scotland’s political geography changed, seat by seat</a>, <em>The Guardian</em></li>
<li><a href="https://www.researchgate.net/figure/Alluvial-diagram-for-mapping-changes-in-the-Global-network-The-top-ten-communities_fig6_267734552">Alluvial diagram for mapping changes in the Global network</a>, Lu and Brelsford (2014)</li>
</ul></li>
</ol>
</div>
<div id="how-my-proposal-stacks-up" class="section level3">
<h3>How my proposal stacks up</h3>
<p>Without exhaustively surveying the Internet and technical literature, it’s worthwhile to benchmark my distinctions against those made by some popular chart catalogues, which are much more representative of usage patterns. Xenographics lists several at the end of <a href="https://xeno.graphics/articles/on-graphonyms-the-importance-of-chart-type-names/">their discussion of graphonyms</a>, and of these three make clear distinctions between some of the types described above:</p>
<ul>
<li><a href="https://datavizproject.com/">The DataViz Project</a> describes Sankey diagrams as i did above, and distinguishes alluvial plots from parallel sets plots only in terms of the orientation of the axes (which is horizontal versus vertical) and of the shapes of the connecting ribbons.</li>
<li><a href="https://datavizcatalogue.com">The Data Visualization Catalogue</a> distinguishes parallel sets plots from Sankey diagrams as not using arrows and binning the flows (ribbons) at regular intervals. (They don’t have an entry on alluvial plots.)</li>
<li><a href="http://visualizationuniverse.com/charts/">The Visualization Universe</a> has entries for Sankey diagrams and alluvial plots, but their descriptions are excerpts (or, at least, subsets) of those of the DataViz Project.</li>
</ul>
<p>Importantly, in my taxonomy, by and large, <strong>alluvial plots are not Sankey diagrams, nor even flow diagrams</strong>. This seems appropriate on reflection, since even the original alluvial diagrams did not represent the transmission of material or information between nodes but changes in classification over time.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> This is not to say that alluvial plots cannot represent flow data—several popular examples do—but that the plot elements are not <em>specific</em> to flow data; and that, as a result, the directedness of flow data may not be conveyed well in an alluvial plot.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> I seem to be in agreement with the popular catalogues here.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>A more subtle pattern of usage reflected in my proposal is that parallel sets plots may resize flows from dimension to dimension to reflect changes in proportion that do not necessarily correspond to chances in amount. This must be done carefully in such plots, but it would be anathema to a Sankey diagram.
I’ll also endorse Elijah Meeks’ point, <a href="https://medium.com/@Elijah_Meeks/alluvial-charts-and-their-discontents-10a77d55216b">from this Medium post</a>, that Sankey diagrams can include cycles, whereas parallel sets and alluvial plots would not be able to encode such patterns.</p>
<p>I’m also making a distinction between alluvial and parallel sets plots that the catalogues—and, in my experience, other developers—don’t make.
Indeed, i haven’t seen a discussion anywhere else of whether a parallel sets or alluvial plot puts repeated categories in the same order, or whether the parallel sets are stacked so that the perpendicular axis measures their cumulative size, or whether either of these features is relevant to the choice of which type of plot is better-suited to a given purpose.
Having spent hours on the problem of whether different orderings might benefit a plot, and having been prompted several times to implement gaps between the sets, i’ve come to take the strong position that this distinction matters and that the terminology should reflect it.
While good definitions describe usage rather than prescribe it, i think it’s worth making an argument for this particular technical distinction while the terminology has not yet, ahem, sedemented.</p>
</div>
</div>
<div id="a-prescription-for-distinguishing-alluvial-and-parallel-sets-plots" class="section level2">
<h2>A prescription for distinguishing alluvial and parallel sets plots</h2>
<p>To reiterate:</p>
<blockquote>
<p><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em></p>
</blockquote>
<div id="caveat" class="section level3">
<h3>Caveat</h3>
<p>Neither Rosvall and Bergstrom, who popularized alluvial diagrams, nor Bojanowski, on whose package i based ggalluvial, included a cumulative weight axis perpendicular to the dimensions axis. In fact, what originally prompted me to omit the gaps between strata in ggalluvial was that i didn’t know how to get rid of the vertical axis! Like every great idea i believe i’ve had, i arrived at this one via gradient descent.</p>
<p>That’s still not to say i was first: Hofmann and Vendettouli wrote ggparallel to stack the sets in each dimension and retain a vertical axis in their plots. There may well be other such implementations, but i’m most familiar with the R ecosystem.</p>
</div>
<div id="utility" class="section level3">
<h3>Utility</h3>
<p>First, i claim that the features that distinguish alluvial from parallel sets plots—consistent ordering of sets and a cumulative weight axis—have practical importance.
In particular, they have importance <em>beyond</em> the ability to visually distinguish the sets and compare their weights along each dimension, as can be done from any parallel sets plot.
The use cases i’ve surveyed reveal three distinct settings in which alluvial plots are superior to other parallel sets plots:<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p><strong>Repeated categorical measures data:</strong>
Several users have used alluvial plots to represent data consisting of partitions of cases into the same (or overlapping) classification schemes at different times, in particular before and after some intervention or other significant event.
See scholarly examples in <a href="https://www.sciencedirect.com/science/article/pii/S0378429018317337">Baudron, Ndoli, Habarurema, and Silva (2019)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0195925518302026">Kissinger and Reznik (2019)</a>, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/gec3.12441">Muenchow, Schäfer, and Krüger (2019)</a>, <a href="https://www.jneurosci.org/content/39/28/5534.abstract">Chong et al (2019)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/ejhf.1547">Schlotter et al (2019)</a>, and tweeted examples by <a href="https://twitter.com/KenSteif/status/1006542071375761408">KenSteif</a>, <a href="https://twitter.com/frau_dr_barber/status/1130167116164927488">frau_dr_barber</a>, <a href="https://twitter.com/ericpgreen/status/1133840554968666112">ericpgreen</a>, and <a href="https://twitter.com/5amStats/status/1135153961227509762">5amStats</a> (starboard image).
For these diagrams to communicate the data efficiently, it is essential that the classes be consistently ordered.
Some implementations of parallel sets plots default to this behavior, as <a href="https://matthewdharris.com/2017/11/11/a-brief-diversion-into-static-alluvial-sankey-diagrams-in-r/">showcased by Matt Harris</a>; but others may automatically sort the sets by size, and still others allow arbitrary orderings—which may be useful for interactive exploration, as with Meeks’ implementation, but inappropriate for static renderings.</p>
<p><strong>Multipartite network data:</strong>
A handful of users have used alluvial plots to represent multipartite graphs, which necessarily satisfy the constraint that the total degree of the nodes in each part is the same. In particular, genomic analyses produce one-to-one connections among stages in transcription processes (lncRNA, miRNA, and mRNA), which have been encoded into alluvial plots by <a href="https://peerj.com/articles/6091/">Zheng et al (2018)</a>, <a href="https://cancerci.biomedcentral.com/articles/10.1186/s12935-019-0817-y">Long et al (2019a)</a>, and <a href="https://www.frontiersin.org/articles/10.3389/fonc.2019.00649/full">Long et al (2019b)</a>. See <a href="https://www.frontiersin.org/articles/10.3389/fimmu.2019.00660/full">Vazquez Bernat et al (2019)</a> for a similar usage, and <a href="https://twitter.com/MyriamCTraub/status/1169236685160402946">MyriamCTraub</a> and <a href="https://twitter.com/BenMoretti/status/1100378930865827840">BenMoretti</a> on Twitter.
A similar principle is at work in <a href="https://watanabesmith.rbind.io/post/ranked-black-mirror/">Watanabe Smith’s illustration of ranked-choice voting</a>, especially with respect to those occasions when a voter lost influence by only ranking a few of the options (which segues into the next setting).
While these users excluded the cumulative weight axis, through the fixed heights of the stacked sets the plots communicate that the total degree at each stage is the same.</p>
<p><strong>Censored data:</strong>
Finally, something i think alluvial plots do exceptionally better than general parallel sets plots is accentuate censoredness in data. Check out scholarly articles by <a href="https://www.sciencedirect.com/science/article/pii/S1075996418301021">Seekatz et al (2018)</a> and <a href="https://journals.lww.com/ccmjournal/Fulltext/2019/01000/Evaluating_Delivery_of_Low_Tidal_Volume.8.aspx">Sjoding, Gong, Hass, and Iwashyna (2019)</a> and <a href="https://mdneuzerling.com/post/my-data-science-job-hunt/">David Neuzerling’s reflections on the job hunt</a>, which use alluvial plots to depict changes in subjects’ status across several time points or stages with a specific set (or blank space where it would be) for subjects who became unavailable later in the study. The cumulative weight axis and gridlines allow the reader to immediately discern the reduction in sample size at each step.
(Though they use network data, <a href="https://www.nature.com/articles/srep06773">Lu and Brelsford (2014)</a> make similar use of this property to visualize non-connections between sets together with connections.)</p>
<p>That’s the substance of my argument for the alluvial–parallel sets distinction, but i’ll finish with a bit of fluorish.</p>
</div>
<div id="connotativity" class="section level3">
<h3>Connotativity</h3>
<p>The special features of alluvial plots are connoted by their peculiar terminology: I’ve decided to call the rectangles representing the parallel sets “strata” to suggest that they are more stable than the crisscrossing alluvia, and indeed this stability (with respect to their order in the plot) is what users expect when many of the categorical dimensions classify subjects into the same categories. The term also suggests, as does “alluvia”, that the various sets into which the subjects are partitioned at each (usually horizontal) position along the dimension axis are themselves (vertically) positioned in accordance with gravity. That is, they have “settled” one atop another with no defiantly empty space in between.</p>
<p>Thus i deposit my case.</p>
</div>
</div>
<div id="coda" class="section level2">
<h2>Coda</h2>
<p>I am by no means an expert in data visualization! While i feel strongly that my taxonomy makes the best of the present scrambling of terms, i am quite open to countersuggestions and especially to use cases that undercut it. If you come across them, please do send them my way.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I only just discovered Yawei Ge and Hofmann’s <a href="https://yaweige.github.io/ggpcp/">ggpcp</a> package for general parallel coordinate plots, under active development!<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>That’s not to say that useful distinctions haven’t been made somewhere, e.g. the technical literature on data visualization, but i feel confident in claiming that they have had limited effects on practice!<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>In ggplot2, these rules are the stat, geom, coord, and scale layers. One of the great contributions of ggplot2, in my view, was to make them explicit to lay users like myself.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>It’s unfortunate that i named the ribbons between adjacent axes <em>flows</em> in ggalluvial, but i maintain that it was preferable to calling them <em>fans</em>.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>A similar misfit is a simplicial complex represented by a network diagram: The type of diagram is designed for a different type of data (pairwise-relational with possible directedness and multiplicity) and fails to convey essential data elements (higher-dimensional simplices).<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>A contrary example is RAWGraphs, which <a href="https://rawgraphs.io/learning/how-to-make-an-alluvial-diagram/">describes alluvial plots</a> as “a specific kind of Sankey diagrams”.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>I found most of these examples by searching for “ggalluvial” in Google Scholar or Twitter.<a href="#fnref7" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>comparable pairwise and multivariate associations from presence-absence data</title>
      <link>/2019/08/30/presence-absence/</link>
      <pubDate>2019 Aug 30 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/30/presence-absence/</guid>
      <description><p>A project i’ve had in the works for years, and “almost done” <a href="https://www.siam.org/Conferences/CM/Conference/ns18">since last summer</a>, is a sensitivity and robustness analysis of several techniques used in studies of “comorbidity networks” (also “disease graphs”, “disease maps”, etc.) that have appeared over the past 20 years. As i begin the abstract:</p>
<blockquote>
<p>Comorbidity network analysis (CNA) is an increasingly popular approach in systems medicine, in which mathematical graphs encode epidemiological correlations (links) between diseases (nodes) inferred from their occurrence in an underlying patient population.</p>
</blockquote>
<p>An essential part of this project is a comparison of pairwise versus multivariate network construction. A <strong>pairwise</strong> (or, more generally, “motif-wise”) construction involves aggregating the network from links determined from some measure of association between pairs (or among motifs) of coded disorders. While many such measures are simply defined in terms of data, others, most notably correlation coefficients, are assumed to have latent values that must be estimated from data. In some studies, these estimates control for patient-level covariates such as age, sex, and ethnic group. A <strong>multivariate</strong> construction involves controlling these estimates for <em>other disorders</em>, whose various associations are also being estimated. (Multivariate constructions may but needn’t also control for patient-level covariates.)</p>
<p>To understand the problem of identifying suitable multivariate models, it’s important to know that the underlying data are binary, having the structure of presence–absence data.
<em>Presence–absence data</em> constitute a case–condition matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> whose each entry <span class="math inline">\(x_{ij}\in\{0,1\}\)</span> is one if case <span class="math inline">\(i\)</span> satisfies condition <span class="math inline">\(j\)</span> and zero if not. The term “presence–absence” derives from ecology, where the cases and conditions are sites and species and <span class="math inline">\(x_{ij}=1\)</span> indicates that species <span class="math inline">\(j\)</span> was observed at site <span class="math inline">\(i\)</span>.</p>
<p>A variety of binary association measures emerged in earlier ecology studies—check out <a href="https://www.researchgate.net/profile/Zdenek_Hubalek/publication/229695992_Coefficients_of_Association_and_Similarity_Based_on_Binary_Presence-Absence_Data_An_Evaluation/links/5a2e5ef445851552ae7f1ddc/Coefficients-of-Association-and-Similarity-Based-on-Binary-Presence-Absence-Data-An-Evaluation.pdf">the survey, taxonomy, and comparison by Hubálek from 1982</a>—and a handful, including the correlation coefficient <span class="math inline">\(\phi\)</span> (A<sub>30</sub>) attributed independently to Yule and to Pearson and Heron and Forbes’ coefficient of association (A<sub>40</sub>), made their way into comorbidity network analysis, together with the odds ratio more widely used in medical research. By and large, these statistics do not generalize to summaries of 3 or more variables; those that do tend to be correlation coefficients.</p>
<div id="multivariate-techniques-for-network-construction" class="section level3">
<h3>multivariate techniques for network construction</h3>
<p>I adopted two multivariate approaches to compare to the pairwise approach: One, which uses partial correlation coefficients, has been introduced in psychology and was the subject of <a href="https://arxiv.org/abs/1607.01367">a 2017 tutorial by Epskamp and Fried</a>. If <span class="math inline">\(m\)</span> variables <span class="math inline">\(y_i\mid 1\leq i\leq m\)</span> have standard deviations <span class="math inline">\(\sigma_i\)</span>, then the <em>full partial correlation</em> of <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is the standardized regression coefficient <span class="math inline">\(\displaystyle r&#39;_{ij}=\frac{\sigma_j}{\sigma_i}\beta_{ij}\)</span> from the linear model predicting <span class="math inline">\(y_i\)</span> from all other variables—or, equivalently, <span class="math inline">\(\displaystyle r&#39;_{ji}=\frac{\sigma_i}{\sigma_j}\beta_{ji}\)</span> from the model predicting <span class="math inline">\(y_j\)</span>. The <em>partial correlation network</em> <span class="math inline">\(G&#39;\)</span> is aggregated from the <span class="math inline">\(r&#39;_{ij}\)</span>. Since these correlations are controlled for the effects of other variables, <span class="math inline">\(G&#39;\)</span> should include far fewer <a href="https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/">transitive correlations</a> than a pairwise network.</p>
<p>Conveniently, partial correlations can be calculated from whatever pairwise correlations one begins with.</p>
<p>I also wanted to borrow from the current ecology literature. On my reading, ecology has always been at the cutting edge of multivariate statistical analysis, and lately there have been proposed several correlation and network models to overcome the well-documented limitations of pairwise techniques. These proposals have been motivated by a desire to capture a variety of interactions among species, geography, and environment. The reviews i’ve found aren’t recent enough to have surveyed the most interesting examples, which include <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.2007.0030-1299.16173.x">Ulrich &amp; Gotelli (2007)</a>, <a href="https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-0173.1">Ovaskainen, Hottola, &amp; Siitonen (2010)</a>, <a href="https://link.springer.com/article/10.1007%2Fs12080-015-0281-9">Cazelles, Araújo, Mouquet, &amp; Gravel (2016)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.01892">Morueta-Holme et al (2015)</a>.</p>
<p>Mostly because the authors included a tutorial for their method, implemented in R, in their supporting information, i adopted the <em>joint distribution model</em> (JDM) <a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12180">proposed by Pollock and colleagues</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Though the model was developed to handle both variable interactions (“endogenous” effects) and case-level covariates (“exogenous” effects), for the coming illustration i’m only concerned with endogenous information.</p>
<p>This simplified JDM assumes that each row of a <span class="math inline">\(0,1\)</span>-matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> is obtained from a latent multivariate normal distribution with center <span class="math inline">\(\vec\mu=[\,\mu_1\,\cdots\,\mu_m\,]\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. The <span class="math inline">\(\mu_j\)</span> encode the prevalences of the conditions while <span class="math inline">\(\Sigma\)</span> encodes their correlations. If <span class="math inline">\(Z\sim N(\vec\mu,\Sigma)\)</span>, then the rows of <span class="math inline">\(X\)</span> are assumed to have been generated from samples <span class="math inline">\(\vec z_i=[\,z_1\,\cdots\,z_m\,]\)</span> as <span class="math display">\[x_{ij}=\begin{cases} 0 &amp; z_{ij}\leq 0 \\ 1 &amp; z_{ij}&gt;0 \end{cases}\text.\]</span> The model is hierarchical with respect to the meta-parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> and is fit using Bayesian methods. I’ll denote the correlation matrix estimated this way as <span class="math inline">\(\hat{\mathrm{P}}=(\hat\rho_{ij})\)</span>, since Pollock and colleagues designate the underlying correlation matrix <span class="math inline">\(\mathrm{P}\)</span> (capital <span class="math inline">\(\rho\)</span>).</p>
</div>
<div id="a-comparable-pairwise-construction" class="section level3">
<h3>a comparable pairwise construction</h3>
<p>Since partial correlations can be calculated from any sample correlation data, a grand comparison of these three approaches now requires only a (pairwise) correlation coefficient that meaningfully compares to <span class="math inline">\(\hat\rho_{ij}\)</span>. The JDM relies on a latent multivariate normal, so my natural choice—once i’d found it—was a correlation coefficient based on a latent <em>bivariate</em> normal: the <em>tetrachoric correlation coefficient</em>, often denoted <span class="math inline">\(r_t\)</span>.</p>
<p>The setup for <span class="math inline">\(r_t\)</span> is a distribution <span class="math inline">\(N(\mu,\Sigma)\)</span> with means <span class="math inline">\(\vec\mu=[\,\mu_1,\mu_2\,]\)</span> and covariance <span class="math display">\[\Sigma=\left(\begin{array}{cc} \!{\sigma_1}^2\! &amp; \!\rho\sigma_1\sigma_2\! \\ \!\rho\sigma_1\sigma_2\! &amp; \!{\sigma_2}^2\! \end{array}\right)\text.\]</span>
The upshot is that, as in the JDM, the value of the statistic is the maximum-likelihood estimate of the latent correlation coefficient <span class="math inline">\(\rho\)</span>. Several estimation techniques are discussion in <a href="https://www.researchgate.net/publication/313196484_Polychoric_and_polyserial_correlations">Drasgow’s entry for the <em>Encyclopedia of Statistical Sciences</em></a>.</p>
<p>Implementations of all three statistics are available but require a bit of lead-in, so i’ll come back to this topic—and a grand comparison—in a future post.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Pollock and colleagues call this a “joint species distribution model”, but since i’m applying the method outside ecology, and since the structure of the model clarifies which distributions its name refers to, i’m leaving out the domain-specific qualifier.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><p>A project i’ve had in the works for years, and “almost done” <a href="https://www.siam.org/Conferences/CM/Conference/ns18">since last summer</a>, is a sensitivity and robustness analysis of several techniques used in studies of “comorbidity networks” (also “disease graphs”, “disease maps”, etc.) that have appeared over the past 20 years. As i begin the abstract:</p>
<blockquote>
<p>Comorbidity network analysis (CNA) is an increasingly popular approach in systems medicine, in which mathematical graphs encode epidemiological correlations (links) between diseases (nodes) inferred from their occurrence in an underlying patient population.</p>
</blockquote>
<p>An essential part of this project is a comparison of pairwise versus multivariate network construction. A <strong>pairwise</strong> (or, more generally, “motif-wise”) construction involves aggregating the network from links determined from some measure of association between pairs (or among motifs) of coded disorders. While many such measures are simply defined in terms of data, others, most notably correlation coefficients, are assumed to have latent values that must be estimated from data. In some studies, these estimates control for patient-level covariates such as age, sex, and ethnic group. A <strong>multivariate</strong> construction involves controlling these estimates for <em>other disorders</em>, whose various associations are also being estimated. (Multivariate constructions may but needn’t also control for patient-level covariates.)</p>
<p>To understand the problem of identifying suitable multivariate models, it’s important to know that the underlying data are binary, having the structure of presence–absence data.
<em>Presence–absence data</em> constitute a case–condition matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> whose each entry <span class="math inline">\(x_{ij}\in\{0,1\}\)</span> is one if case <span class="math inline">\(i\)</span> satisfies condition <span class="math inline">\(j\)</span> and zero if not. The term “presence–absence” derives from ecology, where the cases and conditions are sites and species and <span class="math inline">\(x_{ij}=1\)</span> indicates that species <span class="math inline">\(j\)</span> was observed at site <span class="math inline">\(i\)</span>.</p>
<p>A variety of binary association measures emerged in earlier ecology studies—check out <a href="https://www.researchgate.net/profile/Zdenek_Hubalek/publication/229695992_Coefficients_of_Association_and_Similarity_Based_on_Binary_Presence-Absence_Data_An_Evaluation/links/5a2e5ef445851552ae7f1ddc/Coefficients-of-Association-and-Similarity-Based-on-Binary-Presence-Absence-Data-An-Evaluation.pdf">the survey, taxonomy, and comparison by Hubálek from 1982</a>—and a handful, including the correlation coefficient <span class="math inline">\(\phi\)</span> (A<sub>30</sub>) attributed independently to Yule and to Pearson and Heron and Forbes’ coefficient of association (A<sub>40</sub>), made their way into comorbidity network analysis, together with the odds ratio more widely used in medical research. By and large, these statistics do not generalize to summaries of 3 or more variables; those that do tend to be correlation coefficients.</p>
<div id="multivariate-techniques-for-network-construction" class="section level3">
<h3>multivariate techniques for network construction</h3>
<p>I adopted two multivariate approaches to compare to the pairwise approach: One, which uses partial correlation coefficients, has been introduced in psychology and was the subject of <a href="https://arxiv.org/abs/1607.01367">a 2017 tutorial by Epskamp and Fried</a>. If <span class="math inline">\(m\)</span> variables <span class="math inline">\(y_i\mid 1\leq i\leq m\)</span> have standard deviations <span class="math inline">\(\sigma_i\)</span>, then the <em>full partial correlation</em> of <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is the standardized regression coefficient <span class="math inline">\(\displaystyle r&#39;_{ij}=\frac{\sigma_j}{\sigma_i}\beta_{ij}\)</span> from the linear model predicting <span class="math inline">\(y_i\)</span> from all other variables—or, equivalently, <span class="math inline">\(\displaystyle r&#39;_{ji}=\frac{\sigma_i}{\sigma_j}\beta_{ji}\)</span> from the model predicting <span class="math inline">\(y_j\)</span>. The <em>partial correlation network</em> <span class="math inline">\(G&#39;\)</span> is aggregated from the <span class="math inline">\(r&#39;_{ij}\)</span>. Since these correlations are controlled for the effects of other variables, <span class="math inline">\(G&#39;\)</span> should include far fewer <a href="https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/">transitive correlations</a> than a pairwise network.</p>
<p>Conveniently, partial correlations can be calculated from whatever pairwise correlations one begins with.</p>
<p>I also wanted to borrow from the current ecology literature. On my reading, ecology has always been at the cutting edge of multivariate statistical analysis, and lately there have been proposed several correlation and network models to overcome the well-documented limitations of pairwise techniques. These proposals have been motivated by a desire to capture a variety of interactions among species, geography, and environment. The reviews i’ve found aren’t recent enough to have surveyed the most interesting examples, which include <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.2007.0030-1299.16173.x">Ulrich &amp; Gotelli (2007)</a>, <a href="https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-0173.1">Ovaskainen, Hottola, &amp; Siitonen (2010)</a>, <a href="https://link.springer.com/article/10.1007%2Fs12080-015-0281-9">Cazelles, Araújo, Mouquet, &amp; Gravel (2016)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.01892">Morueta-Holme et al (2015)</a>.</p>
<p>Mostly because the authors included a tutorial for their method, implemented in R, in their supporting information, i adopted the <em>joint distribution model</em> (JDM) <a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12180">proposed by Pollock and colleagues</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Though the model was developed to handle both variable interactions (“endogenous” effects) and case-level covariates (“exogenous” effects), for the coming illustration i’m only concerned with endogenous information.</p>
<p>This simplified JDM assumes that each row of a <span class="math inline">\(0,1\)</span>-matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> is obtained from a latent multivariate normal distribution with center <span class="math inline">\(\vec\mu=[\,\mu_1\,\cdots\,\mu_m\,]\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. The <span class="math inline">\(\mu_j\)</span> encode the prevalences of the conditions while <span class="math inline">\(\Sigma\)</span> encodes their correlations. If <span class="math inline">\(Z\sim N(\vec\mu,\Sigma)\)</span>, then the rows of <span class="math inline">\(X\)</span> are assumed to have been generated from samples <span class="math inline">\(\vec z_i=[\,z_1\,\cdots\,z_m\,]\)</span> as <span class="math display">\[x_{ij}=\begin{cases} 0 &amp; z_{ij}\leq 0 \\ 1 &amp; z_{ij}&gt;0 \end{cases}\text.\]</span> The model is hierarchical with respect to the meta-parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> and is fit using Bayesian methods. I’ll denote the correlation matrix estimated this way as <span class="math inline">\(\hat{\mathrm{P}}=(\hat\rho_{ij})\)</span>, since Pollock and colleagues designate the underlying correlation matrix <span class="math inline">\(\mathrm{P}\)</span> (capital <span class="math inline">\(\rho\)</span>).</p>
</div>
<div id="a-comparable-pairwise-construction" class="section level3">
<h3>a comparable pairwise construction</h3>
<p>Since partial correlations can be calculated from any sample correlation data, a grand comparison of these three approaches now requires only a (pairwise) correlation coefficient that meaningfully compares to <span class="math inline">\(\hat\rho_{ij}\)</span>. The JDM relies on a latent multivariate normal, so my natural choice—once i’d found it—was a correlation coefficient based on a latent <em>bivariate</em> normal: the <em>tetrachoric correlation coefficient</em>, often denoted <span class="math inline">\(r_t\)</span>.</p>
<p>The setup for <span class="math inline">\(r_t\)</span> is a distribution <span class="math inline">\(N(\mu,\Sigma)\)</span> with means <span class="math inline">\(\vec\mu=[\,\mu_1,\mu_2\,]\)</span> and covariance <span class="math display">\[\Sigma=\left(\begin{array}{cc} \!{\sigma_1}^2\! &amp; \!\rho\sigma_1\sigma_2\! \\ \!\rho\sigma_1\sigma_2\! &amp; \!{\sigma_2}^2\! \end{array}\right)\text.\]</span>
The upshot is that, as in the JDM, the value of the statistic is the maximum-likelihood estimate of the latent correlation coefficient <span class="math inline">\(\rho\)</span>. Several estimation techniques are discussion in <a href="https://www.researchgate.net/publication/313196484_Polychoric_and_polyserial_correlations">Drasgow’s entry for the <em>Encyclopedia of Statistical Sciences</em></a>.</p>
<p>Implementations of all three statistics are available but require a bit of lead-in, so i’ll come back to this topic—and a grand comparison—in a future post.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Pollock and colleagues call this a “joint species distribution model”, but since i’m applying the method outside ecology, and since the structure of the model clarifies which distributions its name refers to, i’m leaving out the domain-specific qualifier.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
  </channel>
</rss>
