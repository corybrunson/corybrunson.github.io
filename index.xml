<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>murmuring in the background</title>
    <link>http://corybrunson.github.io/</link>
    <description>Recent content on murmuring in the background</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>2019 Oct 11 (Fri), 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://corybrunson.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>my reaction to _Hidden Figures_</title>
      <link>http://corybrunson.github.io/2019/10/11/hidden-figures/</link>
      <pubDate>2019 Oct 11 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>http://corybrunson.github.io/2019/10/11/hidden-figures/</guid>
      <description>


<p><em>I’m a bit overwhelmed and a bit sick this month, and i didn’t want to miss two fortnights in a row. I wrote up this reaction to seeing the film <em>Hidden Figures</em> a couple of years ago on Facebook, and several of my friends liked it then, so i’m sharing it here. It’s slightly edited. –Cory</em></p>
<p><em>Hidden Figures</em> was probably the best civil rights–focused film i’ve seen in a while—not to disparage <em>Selma</em> or <em>The Butler</em>, but my personal taste is for greater subtlety. The movie is most impressive to me, though, as a mathematician biopic.</p>
<p>In large part, this is because they did the math right, or as right as i would hope a movie to. Young Goble didn’t, in an early establishing scene, just solve an equation on the chalkboard; she explained, clearly, what she had done and why it was a reasonable approach.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> After asserting her way into a closed-door (read: white men–only) board meeting, she calculated a re-entry trajectory on demand and, without boring into every detail, provided enough insight into the key steps to give her reasonably intelligent audience a sense of how she was doing it. Most entertainingly for me (though i don’t know how historically accurately), she had the profound realization, not just on screen but out loud, that the (at the time) ancient curiosity of Euler’s method could fuse (parabolic) launch/re-entry and (elliptical) orbital trajectories into a complete, cohesive course. I don’t know that i ever would have thought to wonder whether i’d ever see (one step along) the momentous transition of applied mathematics from analytic to numerical dominance enacted as entertainment.</p>
<p>To my mind, however, its principal achievement was in rejecting the now-entrenched Hollywood stereotype of the ostracized, neuroatypical, and/or disabled mathematical genius.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> This is not to suggest that ostracized, neuroatypical, and/or disabled mathematicians don’t warrant at least their share of screen time, or that it’s a shame that their biopics came first. What’s problematic about the trend until now is that these traits have been presented as integral to the characters’ mathematical interest or ability, which reinforces the myth that mathematical talent is aberrant (one that happily appears to be fading) and does a disservice to the achievements these people did in the face of exceptional challenges.</p>
<p>Three brief examples: Leading this trend was <em>A Beautiful Mind</em>, which depicts Nash’s schizophrenic hallucinations (which i’ve since learned were contrived, as his hallucinations were neither visual nor central to his illness) inspiring him to disengage from his work for a night out, at which he has an epiphany that leads to his famous equilibrium theorem. While i opted not to see <em>The Imitation Game</em>, i understand that the title encapsulates the parallel—absent from the biography but again contrived by the filmmakers—between Turing’s reverse-engineering of the Enigma machine and his faltering attempts, from the autism spectrum, to decipher and emulate other people’s behavior. And the central mathematical conflict in <em>The Man Who Knew Infinity</em>—between Ramanujan’s reverence for the elegance of his own (sometimes false) assertions and his mentor Hardy’s demand for rigorous proofs—is framed as a proxy for the volatile conflict between Hardy’s stigmatized atheism and his student’s stigmatized religion.</p>
<p>While Goble faces serious social challenges, they are not conceived as somehow of a piece with her mathematical talent and work. Individual prejudice impedes her access to resources. Workplace segregation interferes with her performance. Structural discrimination prevents her colleagues from advancing their careers. These obstacles are also externally imposed; Goble herself is mature, competent, and graceful, and the film sees her through an emotional trajectory—making time for her children, falling in love, navigating an exciting and stressful work environment—that is refreshingly unrelated to her genius. And it’s not like Goble’s typicality was essential to this. None of Nash’s, Turing’s, Ramanujan’s, and Hardy’s stories called for the gimmicking they received.</p>
<p>To the extent that future biopics about mathematicians eschew such gimmicks, they’ll owe some credit to <em>Hidden Figures</em>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>She solved a quartic polynomial, presented as a product of quadratics, by factoring each factor into two binomials.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This is, of course, based on my own impressions and expectations.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded>


<p><em>I’m a bit overwhelmed and a bit sick this month, and i didn’t want to miss two fortnights in a row. I wrote up this reaction to seeing the film <em>Hidden Figures</em> a couple of years ago on Facebook, and several of my friends liked it then, so i’m sharing it here. It’s slightly edited. –Cory</em></p>
<p><em>Hidden Figures</em> was probably the best civil rights–focused film i’ve seen in a while—not to disparage <em>Selma</em> or <em>The Butler</em>, but my personal taste is for greater subtlety. The movie is most impressive to me, though, as a mathematician biopic.</p>
<p>In large part, this is because they did the math right, or as right as i would hope a movie to. Young Goble didn’t, in an early establishing scene, just solve an equation on the chalkboard; she explained, clearly, what she had done and why it was a reasonable approach.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> After asserting her way into a closed-door (read: white men–only) board meeting, she calculated a re-entry trajectory on demand and, without boring into every detail, provided enough insight into the key steps to give her reasonably intelligent audience a sense of how she was doing it. Most entertainingly for me (though i don’t know how historically accurately), she had the profound realization, not just on screen but out loud, that the (at the time) ancient curiosity of Euler’s method could fuse (parabolic) launch/re-entry and (elliptical) orbital trajectories into a complete, cohesive course. I don’t know that i ever would have thought to wonder whether i’d ever see (one step along) the momentous transition of applied mathematics from analytic to numerical dominance enacted as entertainment.</p>
<p>To my mind, however, its principal achievement was in rejecting the now-entrenched Hollywood stereotype of the ostracized, neuroatypical, and/or disabled mathematical genius.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> This is not to suggest that ostracized, neuroatypical, and/or disabled mathematicians don’t warrant at least their share of screen time, or that it’s a shame that their biopics came first. What’s problematic about the trend until now is that these traits have been presented as integral to the characters’ mathematical interest or ability, which reinforces the myth that mathematical talent is aberrant (one that happily appears to be fading) and does a disservice to the achievements these people did in the face of exceptional challenges.</p>
<p>Three brief examples: Leading this trend was <em>A Beautiful Mind</em>, which depicts Nash’s schizophrenic hallucinations (which i’ve since learned were contrived, as his hallucinations were neither visual nor central to his illness) inspiring him to disengage from his work for a night out, at which he has an epiphany that leads to his famous equilibrium theorem. While i opted not to see <em>The Imitation Game</em>, i understand that the title encapsulates the parallel—absent from the biography but again contrived by the filmmakers—between Turing’s reverse-engineering of the Enigma machine and his faltering attempts, from the autism spectrum, to decipher and emulate other people’s behavior. And the central mathematical conflict in <em>The Man Who Knew Infinity</em>—between Ramanujan’s reverence for the elegance of his own (sometimes false) assertions and his mentor Hardy’s demand for rigorous proofs—is framed as a proxy for the volatile conflict between Hardy’s stigmatized atheism and his student’s stigmatized religion.</p>
<p>While Goble faces serious social challenges, they are not conceived as somehow of a piece with her mathematical talent and work. Individual prejudice impedes her access to resources. Workplace segregation interferes with her performance. Structural discrimination prevents her colleagues from advancing their careers. These obstacles are also externally imposed; Goble herself is mature, competent, and graceful, and the film sees her through an emotional trajectory—making time for her children, falling in love, navigating an exciting and stressful work environment—that is refreshingly unrelated to her genius. And it’s not like Goble’s typicality was essential to this. None of Nash’s, Turing’s, Ramanujan’s, and Hardy’s stories called for the gimmicking they received.</p>
<p>To the extent that future biopics about mathematicians eschew such gimmicks, they’ll owe some credit to <em>Hidden Figures</em>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>She solved a quartic polynomial, presented as a product of quadratics, by factoring each factor into two binomials.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This is, of course, based on my own impressions and expectations.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>defining and taxonomizing alluvial diagrams</title>
      <link>http://corybrunson.github.io/2019/09/13/flow-taxonomy/</link>
      <pubDate>2019 Sep 13 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>http://corybrunson.github.io/2019/09/13/flow-taxonomy/</guid>
      <description>


<div id="background" class="section level2">
<h2>Background</h2>
<p>I first encountered alluvial diagrams, so-called, in a widely-shared <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0008694">paper</a> by Martin Rosvall and Carl T. Bergstrom. They were investigating the shifting boundaries between distinct scientific fields (“modules”), as reconstructed from sequential years of journal citation data (“states”), and proposed a specialized flow diagram “to highlight the significant changes, fusions, and fissions that the modules undergo between each pair of successive states”. At the time, i was unfamiliar with Sankey diagrams and only passingly familiar with flow diagrams—mostly by way of my computational biologist colleagues and their literature-derived signal transduction networks—but i could imagine myriad uses for this type of visualization and eventually went searching for an implementation in R.</p>
<p>This led me to the <a href="https://github.com/mbojan/alluvial">alluvial</a> package, which Michał Bojanowski was actively developing. As i got more comfortable with and excited about the then-ascendant tidyverse, i took it upon myself to put together <a href="https://github.com/corybrunson/ggalluvial">a ggplot2 extension</a>, which has since become considerably widely used. I later learned that previous developers had released similar extensions, specifically “parallel sets plots” in Thomas Lin Pedersen’s <a href="https://ggforce.data-imaginist.com">ggforce</a> and in Heike Hofmann and Marie Vendettouli’s <a href="https://github.com/heike/ggparallel">ggparallel</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
With continual exposure to these diverse implementations, i began to notice some subtle but important distinctions between others’ and my design principles. It also became clear that there was no consensus distinction between <em>alluvial</em> diagrams/plots and the more well-established genres of <em>Sankey</em> diagrams and <em>parallel sets</em> plots—indeed, no consensus on whether a distinction existed! Based on <a href="https://github.com/corybrunson/ggalluvial/issues">the ggalluvial issues page</a> and <a href="https://stackoverflow.com/search?q=ggalluvial">an occasional query on Stack Overflow</a>, the lack of generally accepted terms surrounding these sorts of diagrams remains a source of confusion.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> My goal in this post is to propose some vocabulary and a taxonomy to <del>alluviate</del> alleviate this confusion, or at least serve as a point of reference for others to propose improvements!</p>
</div>
<div id="a-proposed-taxonomy-for-width-encoded-diagrams" class="section level2">
<h2>A proposed taxonomy for width-encoded diagrams</h2>
<p>Since the terms “diagram” and “plot” (along with “chart”) are sometimes used interchangeably and sometimes fiercely contested, i’ll adopt a convention here and invite suggestions to improve it: <strong>Diagrams</strong> visualize information, <strong>charts</strong> are diagrams whose information is stored as data, and <strong>plots</strong> are charts that are uniquely determined from data by a fixed set of plotting rules.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
In these terms, ggalluvial and the other R packages discussed here unambiguously produce <em>plots</em>.</p>
<p>Here are how i think these various diagrams—flow, Sankey, parallel sets, and alluvial—are related:</p>
<ol style="list-style-type: decimal">
<li><em>Flow diagrams encode directed flows.</em> Flow diagrams may use ribbons, arrows, or other graphical elements to represent flows—that is, directed processes. For example, directed network diagrams of resource transmission between nodes are flow diagrams. Note that flows are not necessarily between nodes: Many Sankey diagrams include incoming or outgoing arrows representing flows from or to elements outside the diagram.</li>
<li><em>Sankey diagrams are flow diagrams with flow weights encoded as ribbon widths.</em> Flow diagrams may be unweighted: Signal transduction networks, for example, usually are. While Sankey diagrams are extremely flexible, their defining characteristic is that weighted flows are represented by ribbons whose widths indicate the weights or “volumes” of flows through them.</li>
<li><a href="https://datascience.blog.wzb.eu/2016/09/27/parallel-coordinate-plots-for-discrete-and-categorical-data-in-r-a-comparison/">Parallel coordinates plots</a> depict cases in a data set by their coordinates along several continuous dimensions, i.e. their values at several continuous variables, arrayed along a discrete axis. <em>Parallel sets plots are analogous to parallel coordinates plots with discrete-valued classificatory dimensions in place of continuous-valued coordinate dimensions.</em> This requires that the plotting rules both determine the order of the classes along each dimension and preserve their relative sizes. In practice, this means that ribbon widths indicate either the absolute weights of the cases or their proportions of the total weight.</li>
<li><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em> This yields a plot with a meaningful continuous axis perpendicular to the discrete axis: The height of a stack at any class is the cumulative weight of the preceding classes, and the stacked sets at different dimensions can be directly compared as stacked bar plots. (Alluvial plots also tend to use splines rather than segments to delineate ribbons, though i think this is far less important than the rules that position the sets and ribbon boundaries.)</li>
</ol>
<div id="examples" class="section level3">
<h3>Examples</h3>
<p>I’m not rendering any plots in this post, so i’ll illustrate these distinctions by pointing to several examples of each, with an emphasis on examples that are “mislabeled” according to my taxonomy:</p>
<ol style="list-style-type: decimal">
<li>Flow diagrams that are <em>not</em> Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:Typical_Signal_Schedule_and_Traffic_Flow_Diagram,_North-South_across_Market_(1929).png">Typical Signal Schedule and Traffic Flow Diagram</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:VA_Business_Line_Finance_and_Accounting.jpg">VA Business Line Finance and Accounting</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F2/">Process flow diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:JIE_Sankey_V5_Fig1.png">The Thermal Efficiency of Steam Engines</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_of_US_Consumer_Expenditure_in_2012.jpg">Sankey Diagram of US Consumer Expenditure in 2012</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Earth_heat_balance_Sankey_diagram.svg">Earth heat balance Sankey diagram</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F3/">Sankey diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Parallel sets plots that are <em>not</em> alluvial plots:
<ul>
<li><a href="https://www.jasondavies.com/parallel-sets/">Titanic Survivors</a>, Jason Davies</li>
<li><a href="https://xeno.graphics/stacked-area-alluvial-diagram/">Stacked area alluvial diagram</a>, Xenographics (note that the vertical axis applies only to the area plot)</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_-_Income_Statement.jpg">Sankey Diagram - Income Statement</a>, Wikimedia Commons</li>
<li><a href="https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0008694.g003">Mapping change in science</a>, <em>PLoS ONE</em></li>
<li><a href="https://www.researchgate.net/figure/Figure-S1-Sankey-diagram-on-global-green-virtual-water-flows-Sankey_fig5_303306020">Sankey diagram on global green virtual water flows</a>, Serrano, Guan, Duarte, and Paavola (2016)</li>
</ul></li>
<li>Alluvial plots:
<ul>
<li><a href="https://www.theinformationlab.co.uk/2018/03/09/build-sankey-diagram-tableau-without-data-prep-beforehand/">Superstore’s Super Sankey</a>, The Information Lab</li>
<li><a href="https://www.theguardian.com/politics/2016/may/06/holyrood-elections-see-rise-of-team-ruth-and-demise-of-labour-vision">How Scotland’s political geography changed, seat by seat</a>, <em>The Guardian</em></li>
<li><a href="https://www.researchgate.net/figure/Alluvial-diagram-for-mapping-changes-in-the-Global-network-The-top-ten-communities_fig6_267734552">Alluvial diagram for mapping changes in the Global network</a>, Lu and Brelsford (2014)</li>
</ul></li>
</ol>
</div>
<div id="how-my-proposal-stacks-up" class="section level3">
<h3>How my proposal stacks up</h3>
<p>Without exhaustively surveying the Internet and technical literature, it’s worthwhile to benchmark my distinctions against those made by some popular chart catalogues, which are much more representative of usage patterns. Xenographics lists several at the end of <a href="https://xeno.graphics/articles/on-graphonyms-the-importance-of-chart-type-names/">their discussion of graphonyms</a>, and of these three make clear distinctions between some of the types described above:</p>
<ul>
<li><a href="https://datavizproject.com/">The DataViz Project</a> describes Sankey diagrams as i did above, and distinguishes alluvial plots from parallel sets plots only in terms of the orientation of the axes (which is horizontal versus vertical) and of the shapes of the connecting ribbons.</li>
<li><a href="https://datavizcatalogue.com">The Data Visualization Catalogue</a> distinguishes parallel sets plots from Sankey diagrams as not using arrows and binning the flows (ribbons) at regular intervals. (They don’t have an entry on alluvial plots.)</li>
<li><a href="http://visualizationuniverse.com/charts/">The Visualization Universe</a> has entries for Sankey diagrams and alluvial plots, but their descriptions are excerpts (or, at least, subsets) of those of the DataViz Project.</li>
</ul>
<p>Importantly, in my taxonomy, by and large, <strong>alluvial plots are not Sankey diagrams, nor even flow diagrams</strong>. This seems appropriate on reflection, since even the original alluvial diagrams did not represent the transmission of material or information between nodes but changes in classification over time.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> This is not to say that alluvial plots cannot represent flow data—several popular examples do—but that the plot elements are not <em>specific</em> to flow data; and that, as a result, the directedness of flow data may not be conveyed well in an alluvial plot.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> I seem to be in agreement with the popular catalogues here.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>A more subtle pattern of usage reflected in my proposal is that parallel sets plots may resize flows from dimension to dimension to reflect changes in proportion that do not necessarily correspond to chances in amount. This must be done carefully in such plots, but it would be anathema to a Sankey diagram.
I’ll also endorse Elijah Meeks’ point, <a href="https://medium.com/@Elijah_Meeks/alluvial-charts-and-their-discontents-10a77d55216b">from this Medium post</a>, that Sankey diagrams can include cycles, whereas parallel sets and alluvial plots would not be able to encode such patterns.</p>
<p>I’m also making a distinction between alluvial and parallel sets plots that the catalogues—and, in my experience, other developers—don’t make.
Indeed, i haven’t seen a discussion anywhere else of whether a parallel sets or alluvial plot puts repeated categories in the same order, or whether the parallel sets are stacked so that the perpendicular axis measures their cumulative size, or whether either of these features is relevant to the choice of which type of plot is better-suited to a given purpose.
Having spent hours on the problem of whether different orderings might benefit a plot, and having been prompted several times to implement gaps between the sets, i’ve come to take the strong position that this distinction matters and that the terminology should reflect it.
While good definitions describe usage rather than prescribe it, i think it’s worth making an argument for this particular technical distinction while the terminology has not yet, ahem, sedemented.</p>
</div>
</div>
<div id="a-prescription-for-distinguishing-alluvial-and-parallel-sets-plots" class="section level2">
<h2>A prescription for distinguishing alluvial and parallel sets plots</h2>
<p>To reiterate:</p>
<blockquote>
<p><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em></p>
</blockquote>
<div id="caveat" class="section level3">
<h3>Caveat</h3>
<p>Neither Rosvall and Bergstrom, who popularized alluvial diagrams, nor Bojanowski, on whose package i based ggalluvial, included a cumulative weight axis perpendicular to the dimensions axis. In fact, what originally prompted me to omit the gaps between strata in ggalluvial was that i didn’t know how to get rid of the vertical axis! Like every great idea i believe i’ve had, i arrived at this one via gradient descent.</p>
<p>That’s still not to say i was first: Hofmann and Vendettouli wrote ggparallel to stack the sets in each dimension and retain a vertical axis in their plots. There may well be other such implementations, but i’m most familiar with the R ecosystem.</p>
</div>
<div id="utility" class="section level3">
<h3>Utility</h3>
<p>First, i claim that the features that distinguish alluvial from parallel sets plots—consistent ordering of sets and a cumulative weight axis—have practical importance.
In particular, they have importance <em>beyond</em> the ability to visually distinguish the sets and compare their weights along each dimension, as can be done from any parallel sets plot.
The use cases i’ve surveyed reveal three distinct settings in which alluvial plots are superior to other parallel sets plots:<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p><strong>Repeated categorical measures data:</strong>
Several users have used alluvial plots to represent data consisting of partitions of cases into the same (or overlapping) classification schemes at different times, in particular before and after some intervention or other significant event.
See scholarly examples in <a href="https://www.sciencedirect.com/science/article/pii/S0378429018317337">Baudron, Ndoli, Habarurema, and Silva (2019)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0195925518302026">Kissinger and Reznik (2019)</a>, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/gec3.12441">Muenchow, Schäfer, and Krüger (2019)</a>, <a href="https://www.jneurosci.org/content/39/28/5534.abstract">Chong et al (2019)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/ejhf.1547">Schlotter et al (2019)</a>, and tweeted examples by <a href="https://twitter.com/KenSteif/status/1006542071375761408">KenSteif</a>, <a href="https://twitter.com/frau_dr_barber/status/1130167116164927488">frau_dr_barber</a>, <a href="https://twitter.com/ericpgreen/status/1133840554968666112">ericpgreen</a>, and <a href="https://twitter.com/5amStats/status/1135153961227509762">5amStats</a> (starboard image).
For these diagrams to communicate the data efficiently, it is essential that the classes be consistently ordered.
Some implementations of parallel sets plots default to this behavior, as <a href="https://matthewdharris.com/2017/11/11/a-brief-diversion-into-static-alluvial-sankey-diagrams-in-r/">showcased by Matt Harris</a>; but others may automatically sort the sets by size, and still others allow arbitrary orderings—which may be useful for interactive exploration, as with Meeks’ implementation, but inappropriate for static renderings.</p>
<p><strong>Multipartite network data:</strong>
A handful of users have used alluvial plots to represent multipartite graphs, which necessarily satisfy the constraint that the total degree of the nodes in each part is the same. In particular, genomic analyses produce one-to-one connections among stages in transcription processes (lncRNA, miRNA, and mRNA), which have been encoded into alluvial plots by <a href="https://peerj.com/articles/6091/">Zheng et al (2018)</a>, <a href="https://cancerci.biomedcentral.com/articles/10.1186/s12935-019-0817-y">Long et al (2019a)</a>, and <a href="https://www.frontiersin.org/articles/10.3389/fonc.2019.00649/full">Long et al (2019b)</a>. See <a href="https://www.frontiersin.org/articles/10.3389/fimmu.2019.00660/full">Vazquez Bernat et al (2019)</a> for a similar usage, and <a href="https://twitter.com/MyriamCTraub/status/1169236685160402946">MyriamCTraub</a> and <a href="https://twitter.com/BenMoretti/status/1100378930865827840">BenMoretti</a> on Twitter.
A similar principle is at work in <a href="https://watanabesmith.rbind.io/post/ranked-black-mirror/">Watanabe Smith’s illustration of ranked-choice voting</a>, especially with respect to those occasions when a voter lost influence by only ranking a few of the options (which segues into the next setting).
While these users excluded the cumulative weight axis, through the fixed heights of the stacked sets the plots communicate that the total degree at each stage is the same.</p>
<p><strong>Censored data:</strong>
Finally, something i think alluvial plots do exceptionally better than general parallel sets plots is accentuate censoredness in data. Check out scholarly articles by <a href="https://www.sciencedirect.com/science/article/pii/S1075996418301021">Seekatz et al (2018)</a> and <a href="https://journals.lww.com/ccmjournal/Fulltext/2019/01000/Evaluating_Delivery_of_Low_Tidal_Volume.8.aspx">Sjoding, Gong, Hass, and Iwashyna (2019)</a> and <a href="https://mdneuzerling.com/post/my-data-science-job-hunt/">David Neuzerling’s reflections on the job hunt</a>, which use alluvial plots to depict changes in subjects’ status across several time points or stages with a specific set (or blank space where it would be) for subjects who became unavailable later in the study. The cumulative weight axis and gridlines allow the reader to immediately discern the reduction in sample size at each step.
(Though they use network data, <a href="https://www.nature.com/articles/srep06773">Lu and Brelsford (2014)</a> make similar use of this property to visualize non-connections between sets together with connections.)</p>
<p>That’s the substance of my argument for the alluvial–parallel sets distinction, but i’ll finish with a bit of fluorish.</p>
</div>
<div id="connotativity" class="section level3">
<h3>Connotativity</h3>
<p>The special features of alluvial plots are connoted by their peculiar terminology: I’ve decided to call the rectangles representing the parallel sets “strata” to suggest that they are more stable than the crisscrossing alluvia, and indeed this stability (with respect to their order in the plot) is what users expect when many of the categorical dimensions classify subjects into the same categories. The term also suggests, as does “alluvia”, that the various sets into which the subjects are partitioned at each (usually horizontal) position along the dimension axis are themselves (vertically) positioned in accordance with gravity. That is, they have “settled” one atop another with no defiantly empty space in between.</p>
<p>Thus i deposit my case.</p>
</div>
</div>
<div id="coda" class="section level2">
<h2>Coda</h2>
<p>I am by no means an expert in data visualization! While i feel strongly that my taxonomy makes the best of the present scrambling of terms, i am quite open to countersuggestions and especially to use cases that undercut it. If you come across them, please do send them my way.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I only just discovered Yawei Ge and Hofmann’s <a href="https://yaweige.github.io/ggpcp/">ggpcp</a> package for general parallel coordinate plots, under active development!<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>That’s not to say that useful distinctions haven’t been made somewhere, e.g. the technical literature on data visualization, but i feel confident in claiming that they have had limited effects on practice!<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>In ggplot2, these rules are the stat, geom, coord, and scale layers. One of the great contributions of ggplot2, in my view, was to make them explicit to lay users like myself.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>It’s unfortunate that i named the ribbons between adjacent axes <em>flows</em> in ggalluvial, but i maintain that it was preferable to calling them <em>fans</em>.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>A similar misfit is a simplicial complex represented by a network diagram: The type of diagram is designed for a different type of data (pairwise-relational with possible directedness and multiplicity) and fails to convey essential data elements (higher-dimensional simplices).<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>A contrary example is RAWGraphs, which <a href="https://rawgraphs.io/learning/how-to-make-an-alluvial-diagram/">describes alluvial plots</a> as “a specific kind of Sankey diagrams”.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>I found most of these examples by searching for “ggalluvial” in Google Scholar or Twitter.<a href="#fnref7" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded>


<div id="background" class="section level2">
<h2>Background</h2>
<p>I first encountered alluvial diagrams, so-called, in a widely-shared <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0008694">paper</a> by Martin Rosvall and Carl T. Bergstrom. They were investigating the shifting boundaries between distinct scientific fields (“modules”), as reconstructed from sequential years of journal citation data (“states”), and proposed a specialized flow diagram “to highlight the significant changes, fusions, and fissions that the modules undergo between each pair of successive states”. At the time, i was unfamiliar with Sankey diagrams and only passingly familiar with flow diagrams—mostly by way of my computational biologist colleagues and their literature-derived signal transduction networks—but i could imagine myriad uses for this type of visualization and eventually went searching for an implementation in R.</p>
<p>This led me to the <a href="https://github.com/mbojan/alluvial">alluvial</a> package, which Michał Bojanowski was actively developing. As i got more comfortable with and excited about the then-ascendant tidyverse, i took it upon myself to put together <a href="https://github.com/corybrunson/ggalluvial">a ggplot2 extension</a>, which has since become considerably widely used. I later learned that previous developers had released similar extensions, specifically “parallel sets plots” in Thomas Lin Pedersen’s <a href="https://ggforce.data-imaginist.com">ggforce</a> and in Heike Hofmann and Marie Vendettouli’s <a href="https://github.com/heike/ggparallel">ggparallel</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
With continual exposure to these diverse implementations, i began to notice some subtle but important distinctions between others’ and my design principles. It also became clear that there was no consensus distinction between <em>alluvial</em> diagrams/plots and the more well-established genres of <em>Sankey</em> diagrams and <em>parallel sets</em> plots—indeed, no consensus on whether a distinction existed! Based on <a href="https://github.com/corybrunson/ggalluvial/issues">the ggalluvial issues page</a> and <a href="https://stackoverflow.com/search?q=ggalluvial">an occasional query on Stack Overflow</a>, the lack of generally accepted terms surrounding these sorts of diagrams remains a source of confusion.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> My goal in this post is to propose some vocabulary and a taxonomy to <del>alluviate</del> alleviate this confusion, or at least serve as a point of reference for others to propose improvements!</p>
</div>
<div id="a-proposed-taxonomy-for-width-encoded-diagrams" class="section level2">
<h2>A proposed taxonomy for width-encoded diagrams</h2>
<p>Since the terms “diagram” and “plot” (along with “chart”) are sometimes used interchangeably and sometimes fiercely contested, i’ll adopt a convention here and invite suggestions to improve it: <strong>Diagrams</strong> visualize information, <strong>charts</strong> are diagrams whose information is stored as data, and <strong>plots</strong> are charts that are uniquely determined from data by a fixed set of plotting rules.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
In these terms, ggalluvial and the other R packages discussed here unambiguously produce <em>plots</em>.</p>
<p>Here are how i think these various diagrams—flow, Sankey, parallel sets, and alluvial—are related:</p>
<ol style="list-style-type: decimal">
<li><em>Flow diagrams encode directed flows.</em> Flow diagrams may use ribbons, arrows, or other graphical elements to represent flows—that is, directed processes. For example, directed network diagrams of resource transmission between nodes are flow diagrams. Note that flows are not necessarily between nodes: Many Sankey diagrams include incoming or outgoing arrows representing flows from or to elements outside the diagram.</li>
<li><em>Sankey diagrams are flow diagrams with flow weights encoded as ribbon widths.</em> Flow diagrams may be unweighted: Signal transduction networks, for example, usually are. While Sankey diagrams are extremely flexible, their defining characteristic is that weighted flows are represented by ribbons whose widths indicate the weights or “volumes” of flows through them.</li>
<li><a href="https://datascience.blog.wzb.eu/2016/09/27/parallel-coordinate-plots-for-discrete-and-categorical-data-in-r-a-comparison/">Parallel coordinates plots</a> depict cases in a data set by their coordinates along several continuous dimensions, i.e. their values at several continuous variables, arrayed along a discrete axis. <em>Parallel sets plots are analogous to parallel coordinates plots with discrete-valued classificatory dimensions in place of continuous-valued coordinate dimensions.</em> This requires that the plotting rules both determine the order of the classes along each dimension and preserve their relative sizes. In practice, this means that ribbon widths indicate either the absolute weights of the cases or their proportions of the total weight.</li>
<li><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em> This yields a plot with a meaningful continuous axis perpendicular to the discrete axis: The height of a stack at any class is the cumulative weight of the preceding classes, and the stacked sets at different dimensions can be directly compared as stacked bar plots. (Alluvial plots also tend to use splines rather than segments to delineate ribbons, though i think this is far less important than the rules that position the sets and ribbon boundaries.)</li>
</ol>
<div id="examples" class="section level3">
<h3>Examples</h3>
<p>I’m not rendering any plots in this post, so i’ll illustrate these distinctions by pointing to several examples of each, with an emphasis on examples that are “mislabeled” according to my taxonomy:</p>
<ol style="list-style-type: decimal">
<li>Flow diagrams that are <em>not</em> Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:Typical_Signal_Schedule_and_Traffic_Flow_Diagram,_North-South_across_Market_(1929).png">Typical Signal Schedule and Traffic Flow Diagram</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:VA_Business_Line_Finance_and_Accounting.jpg">VA Business Line Finance and Accounting</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F2/">Process flow diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:JIE_Sankey_V5_Fig1.png">The Thermal Efficiency of Steam Engines</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_of_US_Consumer_Expenditure_in_2012.jpg">Sankey Diagram of US Consumer Expenditure in 2012</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Earth_heat_balance_Sankey_diagram.svg">Earth heat balance Sankey diagram</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F3/">Sankey diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Parallel sets plots that are <em>not</em> alluvial plots:
<ul>
<li><a href="https://www.jasondavies.com/parallel-sets/">Titanic Survivors</a>, Jason Davies</li>
<li><a href="https://xeno.graphics/stacked-area-alluvial-diagram/">Stacked area alluvial diagram</a>, Xenographics (note that the vertical axis applies only to the area plot)</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_-_Income_Statement.jpg">Sankey Diagram - Income Statement</a>, Wikimedia Commons</li>
<li><a href="https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0008694.g003">Mapping change in science</a>, <em>PLoS ONE</em></li>
<li><a href="https://www.researchgate.net/figure/Figure-S1-Sankey-diagram-on-global-green-virtual-water-flows-Sankey_fig5_303306020">Sankey diagram on global green virtual water flows</a>, Serrano, Guan, Duarte, and Paavola (2016)</li>
</ul></li>
<li>Alluvial plots:
<ul>
<li><a href="https://www.theinformationlab.co.uk/2018/03/09/build-sankey-diagram-tableau-without-data-prep-beforehand/">Superstore’s Super Sankey</a>, The Information Lab</li>
<li><a href="https://www.theguardian.com/politics/2016/may/06/holyrood-elections-see-rise-of-team-ruth-and-demise-of-labour-vision">How Scotland’s political geography changed, seat by seat</a>, <em>The Guardian</em></li>
<li><a href="https://www.researchgate.net/figure/Alluvial-diagram-for-mapping-changes-in-the-Global-network-The-top-ten-communities_fig6_267734552">Alluvial diagram for mapping changes in the Global network</a>, Lu and Brelsford (2014)</li>
</ul></li>
</ol>
</div>
<div id="how-my-proposal-stacks-up" class="section level3">
<h3>How my proposal stacks up</h3>
<p>Without exhaustively surveying the Internet and technical literature, it’s worthwhile to benchmark my distinctions against those made by some popular chart catalogues, which are much more representative of usage patterns. Xenographics lists several at the end of <a href="https://xeno.graphics/articles/on-graphonyms-the-importance-of-chart-type-names/">their discussion of graphonyms</a>, and of these three make clear distinctions between some of the types described above:</p>
<ul>
<li><a href="https://datavizproject.com/">The DataViz Project</a> describes Sankey diagrams as i did above, and distinguishes alluvial plots from parallel sets plots only in terms of the orientation of the axes (which is horizontal versus vertical) and of the shapes of the connecting ribbons.</li>
<li><a href="https://datavizcatalogue.com">The Data Visualization Catalogue</a> distinguishes parallel sets plots from Sankey diagrams as not using arrows and binning the flows (ribbons) at regular intervals. (They don’t have an entry on alluvial plots.)</li>
<li><a href="http://visualizationuniverse.com/charts/">The Visualization Universe</a> has entries for Sankey diagrams and alluvial plots, but their descriptions are excerpts (or, at least, subsets) of those of the DataViz Project.</li>
</ul>
<p>Importantly, in my taxonomy, by and large, <strong>alluvial plots are not Sankey diagrams, nor even flow diagrams</strong>. This seems appropriate on reflection, since even the original alluvial diagrams did not represent the transmission of material or information between nodes but changes in classification over time.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> This is not to say that alluvial plots cannot represent flow data—several popular examples do—but that the plot elements are not <em>specific</em> to flow data; and that, as a result, the directedness of flow data may not be conveyed well in an alluvial plot.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> I seem to be in agreement with the popular catalogues here.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>A more subtle pattern of usage reflected in my proposal is that parallel sets plots may resize flows from dimension to dimension to reflect changes in proportion that do not necessarily correspond to chances in amount. This must be done carefully in such plots, but it would be anathema to a Sankey diagram.
I’ll also endorse Elijah Meeks’ point, <a href="https://medium.com/@Elijah_Meeks/alluvial-charts-and-their-discontents-10a77d55216b">from this Medium post</a>, that Sankey diagrams can include cycles, whereas parallel sets and alluvial plots would not be able to encode such patterns.</p>
<p>I’m also making a distinction between alluvial and parallel sets plots that the catalogues—and, in my experience, other developers—don’t make.
Indeed, i haven’t seen a discussion anywhere else of whether a parallel sets or alluvial plot puts repeated categories in the same order, or whether the parallel sets are stacked so that the perpendicular axis measures their cumulative size, or whether either of these features is relevant to the choice of which type of plot is better-suited to a given purpose.
Having spent hours on the problem of whether different orderings might benefit a plot, and having been prompted several times to implement gaps between the sets, i’ve come to take the strong position that this distinction matters and that the terminology should reflect it.
While good definitions describe usage rather than prescribe it, i think it’s worth making an argument for this particular technical distinction while the terminology has not yet, ahem, sedemented.</p>
</div>
</div>
<div id="a-prescription-for-distinguishing-alluvial-and-parallel-sets-plots" class="section level2">
<h2>A prescription for distinguishing alluvial and parallel sets plots</h2>
<p>To reiterate:</p>
<blockquote>
<p><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em></p>
</blockquote>
<div id="caveat" class="section level3">
<h3>Caveat</h3>
<p>Neither Rosvall and Bergstrom, who popularized alluvial diagrams, nor Bojanowski, on whose package i based ggalluvial, included a cumulative weight axis perpendicular to the dimensions axis. In fact, what originally prompted me to omit the gaps between strata in ggalluvial was that i didn’t know how to get rid of the vertical axis! Like every great idea i believe i’ve had, i arrived at this one via gradient descent.</p>
<p>That’s still not to say i was first: Hofmann and Vendettouli wrote ggparallel to stack the sets in each dimension and retain a vertical axis in their plots. There may well be other such implementations, but i’m most familiar with the R ecosystem.</p>
</div>
<div id="utility" class="section level3">
<h3>Utility</h3>
<p>First, i claim that the features that distinguish alluvial from parallel sets plots—consistent ordering of sets and a cumulative weight axis—have practical importance.
In particular, they have importance <em>beyond</em> the ability to visually distinguish the sets and compare their weights along each dimension, as can be done from any parallel sets plot.
The use cases i’ve surveyed reveal three distinct settings in which alluvial plots are superior to other parallel sets plots:<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p><strong>Repeated categorical measures data:</strong>
Several users have used alluvial plots to represent data consisting of partitions of cases into the same (or overlapping) classification schemes at different times, in particular before and after some intervention or other significant event.
See scholarly examples in <a href="https://www.sciencedirect.com/science/article/pii/S0378429018317337">Baudron, Ndoli, Habarurema, and Silva (2019)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0195925518302026">Kissinger and Reznik (2019)</a>, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/gec3.12441">Muenchow, Schäfer, and Krüger (2019)</a>, <a href="https://www.jneurosci.org/content/39/28/5534.abstract">Chong et al (2019)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/ejhf.1547">Schlotter et al (2019)</a>, and tweeted examples by <a href="https://twitter.com/KenSteif/status/1006542071375761408">KenSteif</a>, <a href="https://twitter.com/frau_dr_barber/status/1130167116164927488">frau_dr_barber</a>, <a href="https://twitter.com/ericpgreen/status/1133840554968666112">ericpgreen</a>, and <a href="https://twitter.com/5amStats/status/1135153961227509762">5amStats</a> (starboard image).
For these diagrams to communicate the data efficiently, it is essential that the classes be consistently ordered.
Some implementations of parallel sets plots default to this behavior, as <a href="https://matthewdharris.com/2017/11/11/a-brief-diversion-into-static-alluvial-sankey-diagrams-in-r/">showcased by Matt Harris</a>; but others may automatically sort the sets by size, and still others allow arbitrary orderings—which may be useful for interactive exploration, as with Meeks’ implementation, but inappropriate for static renderings.</p>
<p><strong>Multipartite network data:</strong>
A handful of users have used alluvial plots to represent multipartite graphs, which necessarily satisfy the constraint that the total degree of the nodes in each part is the same. In particular, genomic analyses produce one-to-one connections among stages in transcription processes (lncRNA, miRNA, and mRNA), which have been encoded into alluvial plots by <a href="https://peerj.com/articles/6091/">Zheng et al (2018)</a>, <a href="https://cancerci.biomedcentral.com/articles/10.1186/s12935-019-0817-y">Long et al (2019a)</a>, and <a href="https://www.frontiersin.org/articles/10.3389/fonc.2019.00649/full">Long et al (2019b)</a>. See <a href="https://www.frontiersin.org/articles/10.3389/fimmu.2019.00660/full">Vazquez Bernat et al (2019)</a> for a similar usage, and <a href="https://twitter.com/MyriamCTraub/status/1169236685160402946">MyriamCTraub</a> and <a href="https://twitter.com/BenMoretti/status/1100378930865827840">BenMoretti</a> on Twitter.
A similar principle is at work in <a href="https://watanabesmith.rbind.io/post/ranked-black-mirror/">Watanabe Smith’s illustration of ranked-choice voting</a>, especially with respect to those occasions when a voter lost influence by only ranking a few of the options (which segues into the next setting).
While these users excluded the cumulative weight axis, through the fixed heights of the stacked sets the plots communicate that the total degree at each stage is the same.</p>
<p><strong>Censored data:</strong>
Finally, something i think alluvial plots do exceptionally better than general parallel sets plots is accentuate censoredness in data. Check out scholarly articles by <a href="https://www.sciencedirect.com/science/article/pii/S1075996418301021">Seekatz et al (2018)</a> and <a href="https://journals.lww.com/ccmjournal/Fulltext/2019/01000/Evaluating_Delivery_of_Low_Tidal_Volume.8.aspx">Sjoding, Gong, Hass, and Iwashyna (2019)</a> and <a href="https://mdneuzerling.com/post/my-data-science-job-hunt/">David Neuzerling’s reflections on the job hunt</a>, which use alluvial plots to depict changes in subjects’ status across several time points or stages with a specific set (or blank space where it would be) for subjects who became unavailable later in the study. The cumulative weight axis and gridlines allow the reader to immediately discern the reduction in sample size at each step.
(Though they use network data, <a href="https://www.nature.com/articles/srep06773">Lu and Brelsford (2014)</a> make similar use of this property to visualize non-connections between sets together with connections.)</p>
<p>That’s the substance of my argument for the alluvial–parallel sets distinction, but i’ll finish with a bit of fluorish.</p>
</div>
<div id="connotativity" class="section level3">
<h3>Connotativity</h3>
<p>The special features of alluvial plots are connoted by their peculiar terminology: I’ve decided to call the rectangles representing the parallel sets “strata” to suggest that they are more stable than the crisscrossing alluvia, and indeed this stability (with respect to their order in the plot) is what users expect when many of the categorical dimensions classify subjects into the same categories. The term also suggests, as does “alluvia”, that the various sets into which the subjects are partitioned at each (usually horizontal) position along the dimension axis are themselves (vertically) positioned in accordance with gravity. That is, they have “settled” one atop another with no defiantly empty space in between.</p>
<p>Thus i deposit my case.</p>
</div>
</div>
<div id="coda" class="section level2">
<h2>Coda</h2>
<p>I am by no means an expert in data visualization! While i feel strongly that my taxonomy makes the best of the present scrambling of terms, i am quite open to countersuggestions and especially to use cases that undercut it. If you come across them, please do send them my way.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I only just discovered Yawei Ge and Hofmann’s <a href="https://yaweige.github.io/ggpcp/">ggpcp</a> package for general parallel coordinate plots, under active development!<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>That’s not to say that useful distinctions haven’t been made somewhere, e.g. the technical literature on data visualization, but i feel confident in claiming that they have had limited effects on practice!<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>In ggplot2, these rules are the stat, geom, coord, and scale layers. One of the great contributions of ggplot2, in my view, was to make them explicit to lay users like myself.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>It’s unfortunate that i named the ribbons between adjacent axes <em>flows</em> in ggalluvial, but i maintain that it was preferable to calling them <em>fans</em>.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>A similar misfit is a simplicial complex represented by a network diagram: The type of diagram is designed for a different type of data (pairwise-relational with possible directedness and multiplicity) and fails to convey essential data elements (higher-dimensional simplices).<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>A contrary example is RAWGraphs, which <a href="https://rawgraphs.io/learning/how-to-make-an-alluvial-diagram/">describes alluvial plots</a> as “a specific kind of Sankey diagrams”.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>I found most of these examples by searching for “ggalluvial” in Google Scholar or Twitter.<a href="#fnref7" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>comparable pairwise and multivariate associations from presence-absence data</title>
      <link>http://corybrunson.github.io/2019/08/30/presence-absence/</link>
      <pubDate>2019 Aug 30 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>http://corybrunson.github.io/2019/08/30/presence-absence/</guid>
      <description>


<p>A project i’ve had in the works for years, and “almost done” <a href="https://www.siam.org/Conferences/CM/Conference/ns18">since last summer</a>, is a sensitivity and robustness analysis of several techniques used in studies of “comorbidity networks” (also “disease graphs”, “disease maps”, etc.) that have appeared over the past 20 years. As i begin the abstract:</p>
<blockquote>
<p>Comorbidity network analysis (CNA) is an increasingly popular approach in systems medicine, in which mathematical graphs encode epidemiological correlations (links) between diseases (nodes) inferred from their occurrence in an underlying patient population.</p>
</blockquote>
<p>An essential part of this project is a comparison of pairwise versus multivariate network construction. A <strong>pairwise</strong> (or, more generally, “motif-wise”) construction involves aggregating the network from links determined from some measure of association between pairs (or among motifs) of coded disorders. While many such measures are simply defined in terms of data, others, most notably correlation coefficients, are assumed to have latent values that must be estimated from data. In some studies, these estimates control for patient-level covariates such as age, sex, and ethnic group. A <strong>multivariate</strong> construction involves controlling these estimates for <em>other disorders</em>, whose various associations are also being estimated. (Multivariate constructions may but needn’t also control for patient-level covariates.)</p>
<p>To understand the problem of identifying suitable multivariate models, it’s important to know that the underlying data are binary, having the structure of presence–absence data.
<em>Presence–absence data</em> constitute a case–condition matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> whose each entry <span class="math inline">\(x_{ij}\in\{0,1\}\)</span> is one if case <span class="math inline">\(i\)</span> satisfies condition <span class="math inline">\(j\)</span> and zero if not. The term “presence–absence” derives from ecology, where the cases and conditions are sites and species and <span class="math inline">\(x_{ij}=1\)</span> indicates that species <span class="math inline">\(j\)</span> was observed at site <span class="math inline">\(i\)</span>.</p>
<p>A variety of binary association measures emerged in earlier ecology studies—check out <a href="https://www.researchgate.net/profile/Zdenek_Hubalek/publication/229695992_Coefficients_of_Association_and_Similarity_Based_on_Binary_Presence-Absence_Data_An_Evaluation/links/5a2e5ef445851552ae7f1ddc/Coefficients-of-Association-and-Similarity-Based-on-Binary-Presence-Absence-Data-An-Evaluation.pdf">the survey, taxonomy, and comparison by Hubálek from 1982</a>—and a handful, including the correlation coefficient <span class="math inline">\(\phi\)</span> (A<sub>30</sub>) attributed independently to Yule and to Pearson and Heron and Forbes’ coefficient of association (A<sub>40</sub>), made their way into comorbidity network analysis, together with the odds ratio more widely used in medical research. By and large, these statistics do not generalize to summaries of 3 or more variables; those that do tend to be correlation coefficients.</p>
<div id="multivariate-techniques-for-network-construction" class="section level3">
<h3>multivariate techniques for network construction</h3>
<p>I adopted two multivariate approaches to compare to the pairwise approach: One, which uses partial correlation coefficients, has been introduced in psychology and was the subject of <a href="https://arxiv.org/abs/1607.01367">a 2017 tutorial by Epskamp and Fried</a>. If <span class="math inline">\(m\)</span> variables <span class="math inline">\(y_i\mid 1\leq i\leq m\)</span> have standard deviations <span class="math inline">\(\sigma_i\)</span>, then the <em>full partial correlation</em> of <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is the standardized regression coefficient <span class="math inline">\(\displaystyle r&#39;_{ij}=\frac{\sigma_j}{\sigma_i}\beta_{ij}\)</span> from the linear model predicting <span class="math inline">\(y_i\)</span> from all other variables—or, equivalently, <span class="math inline">\(\displaystyle r&#39;_{ji}=\frac{\sigma_i}{\sigma_j}\beta_{ji}\)</span> from the model predicting <span class="math inline">\(y_j\)</span>. The <em>partial correlation network</em> <span class="math inline">\(G&#39;\)</span> is aggregated from the <span class="math inline">\(r&#39;_{ij}\)</span>. Since these correlations are controlled for the effects of other variables, <span class="math inline">\(G&#39;\)</span> should include far fewer <a href="https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/">transitive correlations</a> than a pairwise network.</p>
<p>Conveniently, partial correlations can be calculated from whatever pairwise correlations one begins with.</p>
<p>I also wanted to borrow from the current ecology literature. On my reading, ecology has always been at the cutting edge of multivariate statistical analysis, and lately there have been proposed several correlation and network models to overcome the well-documented limitations of pairwise techniques. These proposals have been motivated by a desire to capture a variety of interactions among species, geography, and environment. The reviews i’ve found aren’t recent enough to have surveyed the most interesting examples, which include <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.2007.0030-1299.16173.x">Ulrich &amp; Gotelli (2007)</a>, <a href="https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-0173.1">Ovaskainen, Hottola, &amp; Siitonen (2010)</a>, <a href="https://link.springer.com/article/10.1007%2Fs12080-015-0281-9">Cazelles, Araújo, Mouquet, &amp; Gravel (2016)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.01892">Morueta-Holme et al (2015)</a>.</p>
<p>Mostly because the authors included a tutorial for their method, implemented in R, in their supporting information, i adopted the <em>joint distribution model</em> (JDM) <a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12180">proposed by Pollock and colleagues</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Though the model was developed to handle both variable interactions (“endogenous” effects) and case-level covariates (“exogenous” effects), for the coming illustration i’m only concerned with endogenous information.</p>
<p>This simplified JDM assumes that each row of a <span class="math inline">\(0,1\)</span>-matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> is obtained from a latent multivariate normal distribution with center <span class="math inline">\(\vec\mu=[\,\mu_1\,\cdots\,\mu_m\,]\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. The <span class="math inline">\(\mu_j\)</span> encode the prevalences of the conditions while <span class="math inline">\(\Sigma\)</span> encodes their correlations. If <span class="math inline">\(Z\sim N(\vec\mu,\Sigma)\)</span>, then the rows of <span class="math inline">\(X\)</span> are assumed to have been generated from samples <span class="math inline">\(\vec z_i=[\,z_1\,\cdots\,z_m\,]\)</span> as <span class="math display">\[x_{ij}=\begin{cases} 0 &amp; z_{ij}\leq 0 \\ 1 &amp; z_{ij}&gt;0 \end{cases}\text.\]</span> The model is hierarchical with respect to the meta-parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> and is fit using Bayesian methods. I’ll denote the correlation matrix estimated this way as <span class="math inline">\(\hat{\mathrm{P}}=(\hat\rho_{ij})\)</span>, since Pollock and colleagues designate the underlying correlation matrix <span class="math inline">\(\mathrm{P}\)</span> (capital <span class="math inline">\(\rho\)</span>).</p>
</div>
<div id="a-comparable-pairwise-construction" class="section level3">
<h3>a comparable pairwise construction</h3>
<p>Since partial correlations can be calculated from any sample correlation data, a grand comparison of these three approaches now requires only a (pairwise) correlation coefficient that meaningfully compares to <span class="math inline">\(\hat\rho_{ij}\)</span>. The JDM relies on a latent multivariate normal, so my natural choice—once i’d found it—was a correlation coefficient based on a latent <em>bivariate</em> normal: the <em>tetrachoric correlation coefficient</em>, often denoted <span class="math inline">\(r_t\)</span>.</p>
<p>The setup for <span class="math inline">\(r_t\)</span> is a distribution <span class="math inline">\(N(\mu,\Sigma)\)</span> with means <span class="math inline">\(\vec\mu=[\,\mu_1,\mu_2\,]\)</span> and covariance <span class="math display">\[\Sigma=\left(\begin{array}{cc} \!{\sigma_1}^2\! &amp; \!\rho\sigma_1\sigma_2\! \\ \!\rho\sigma_1\sigma_2\! &amp; \!{\sigma_2}^2\! \end{array}\right)\text.\]</span>
The upshot is that, as in the JDM, the value of the statistic is the maximum-likelihood estimate of the latent correlation coefficient <span class="math inline">\(\rho\)</span>. Several estimation techniques are discussion in <a href="https://www.researchgate.net/publication/313196484_Polychoric_and_polyserial_correlations">Drasgow’s entry for the <em>Encyclopedia of Statistical Sciences</em></a>.</p>
<p>Implementations of all three statistics are available but require a bit of lead-in, so i’ll come back to this topic—and a grand comparison—in a future post.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Pollock and colleagues call this a “joint species distribution model”, but since i’m applying the method outside ecology, and since the structure of the model clarifies which distributions its name refers to, i’m leaving out the domain-specific qualifier.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded>


<p>A project i’ve had in the works for years, and “almost done” <a href="https://www.siam.org/Conferences/CM/Conference/ns18">since last summer</a>, is a sensitivity and robustness analysis of several techniques used in studies of “comorbidity networks” (also “disease graphs”, “disease maps”, etc.) that have appeared over the past 20 years. As i begin the abstract:</p>
<blockquote>
<p>Comorbidity network analysis (CNA) is an increasingly popular approach in systems medicine, in which mathematical graphs encode epidemiological correlations (links) between diseases (nodes) inferred from their occurrence in an underlying patient population.</p>
</blockquote>
<p>An essential part of this project is a comparison of pairwise versus multivariate network construction. A <strong>pairwise</strong> (or, more generally, “motif-wise”) construction involves aggregating the network from links determined from some measure of association between pairs (or among motifs) of coded disorders. While many such measures are simply defined in terms of data, others, most notably correlation coefficients, are assumed to have latent values that must be estimated from data. In some studies, these estimates control for patient-level covariates such as age, sex, and ethnic group. A <strong>multivariate</strong> construction involves controlling these estimates for <em>other disorders</em>, whose various associations are also being estimated. (Multivariate constructions may but needn’t also control for patient-level covariates.)</p>
<p>To understand the problem of identifying suitable multivariate models, it’s important to know that the underlying data are binary, having the structure of presence–absence data.
<em>Presence–absence data</em> constitute a case–condition matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> whose each entry <span class="math inline">\(x_{ij}\in\{0,1\}\)</span> is one if case <span class="math inline">\(i\)</span> satisfies condition <span class="math inline">\(j\)</span> and zero if not. The term “presence–absence” derives from ecology, where the cases and conditions are sites and species and <span class="math inline">\(x_{ij}=1\)</span> indicates that species <span class="math inline">\(j\)</span> was observed at site <span class="math inline">\(i\)</span>.</p>
<p>A variety of binary association measures emerged in earlier ecology studies—check out <a href="https://www.researchgate.net/profile/Zdenek_Hubalek/publication/229695992_Coefficients_of_Association_and_Similarity_Based_on_Binary_Presence-Absence_Data_An_Evaluation/links/5a2e5ef445851552ae7f1ddc/Coefficients-of-Association-and-Similarity-Based-on-Binary-Presence-Absence-Data-An-Evaluation.pdf">the survey, taxonomy, and comparison by Hubálek from 1982</a>—and a handful, including the correlation coefficient <span class="math inline">\(\phi\)</span> (A<sub>30</sub>) attributed independently to Yule and to Pearson and Heron and Forbes’ coefficient of association (A<sub>40</sub>), made their way into comorbidity network analysis, together with the odds ratio more widely used in medical research. By and large, these statistics do not generalize to summaries of 3 or more variables; those that do tend to be correlation coefficients.</p>
<div id="multivariate-techniques-for-network-construction" class="section level3">
<h3>multivariate techniques for network construction</h3>
<p>I adopted two multivariate approaches to compare to the pairwise approach: One, which uses partial correlation coefficients, has been introduced in psychology and was the subject of <a href="https://arxiv.org/abs/1607.01367">a 2017 tutorial by Epskamp and Fried</a>. If <span class="math inline">\(m\)</span> variables <span class="math inline">\(y_i\mid 1\leq i\leq m\)</span> have standard deviations <span class="math inline">\(\sigma_i\)</span>, then the <em>full partial correlation</em> of <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is the standardized regression coefficient <span class="math inline">\(\displaystyle r&#39;_{ij}=\frac{\sigma_j}{\sigma_i}\beta_{ij}\)</span> from the linear model predicting <span class="math inline">\(y_i\)</span> from all other variables—or, equivalently, <span class="math inline">\(\displaystyle r&#39;_{ji}=\frac{\sigma_i}{\sigma_j}\beta_{ji}\)</span> from the model predicting <span class="math inline">\(y_j\)</span>. The <em>partial correlation network</em> <span class="math inline">\(G&#39;\)</span> is aggregated from the <span class="math inline">\(r&#39;_{ij}\)</span>. Since these correlations are controlled for the effects of other variables, <span class="math inline">\(G&#39;\)</span> should include far fewer <a href="https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/">transitive correlations</a> than a pairwise network.</p>
<p>Conveniently, partial correlations can be calculated from whatever pairwise correlations one begins with.</p>
<p>I also wanted to borrow from the current ecology literature. On my reading, ecology has always been at the cutting edge of multivariate statistical analysis, and lately there have been proposed several correlation and network models to overcome the well-documented limitations of pairwise techniques. These proposals have been motivated by a desire to capture a variety of interactions among species, geography, and environment. The reviews i’ve found aren’t recent enough to have surveyed the most interesting examples, which include <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.2007.0030-1299.16173.x">Ulrich &amp; Gotelli (2007)</a>, <a href="https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-0173.1">Ovaskainen, Hottola, &amp; Siitonen (2010)</a>, <a href="https://link.springer.com/article/10.1007%2Fs12080-015-0281-9">Cazelles, Araújo, Mouquet, &amp; Gravel (2016)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.01892">Morueta-Holme et al (2015)</a>.</p>
<p>Mostly because the authors included a tutorial for their method, implemented in R, in their supporting information, i adopted the <em>joint distribution model</em> (JDM) <a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12180">proposed by Pollock and colleagues</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Though the model was developed to handle both variable interactions (“endogenous” effects) and case-level covariates (“exogenous” effects), for the coming illustration i’m only concerned with endogenous information.</p>
<p>This simplified JDM assumes that each row of a <span class="math inline">\(0,1\)</span>-matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> is obtained from a latent multivariate normal distribution with center <span class="math inline">\(\vec\mu=[\,\mu_1\,\cdots\,\mu_m\,]\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. The <span class="math inline">\(\mu_j\)</span> encode the prevalences of the conditions while <span class="math inline">\(\Sigma\)</span> encodes their correlations. If <span class="math inline">\(Z\sim N(\vec\mu,\Sigma)\)</span>, then the rows of <span class="math inline">\(X\)</span> are assumed to have been generated from samples <span class="math inline">\(\vec z_i=[\,z_1\,\cdots\,z_m\,]\)</span> as <span class="math display">\[x_{ij}=\begin{cases} 0 &amp; z_{ij}\leq 0 \\ 1 &amp; z_{ij}&gt;0 \end{cases}\text.\]</span> The model is hierarchical with respect to the meta-parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> and is fit using Bayesian methods. I’ll denote the correlation matrix estimated this way as <span class="math inline">\(\hat{\mathrm{P}}=(\hat\rho_{ij})\)</span>, since Pollock and colleagues designate the underlying correlation matrix <span class="math inline">\(\mathrm{P}\)</span> (capital <span class="math inline">\(\rho\)</span>).</p>
</div>
<div id="a-comparable-pairwise-construction" class="section level3">
<h3>a comparable pairwise construction</h3>
<p>Since partial correlations can be calculated from any sample correlation data, a grand comparison of these three approaches now requires only a (pairwise) correlation coefficient that meaningfully compares to <span class="math inline">\(\hat\rho_{ij}\)</span>. The JDM relies on a latent multivariate normal, so my natural choice—once i’d found it—was a correlation coefficient based on a latent <em>bivariate</em> normal: the <em>tetrachoric correlation coefficient</em>, often denoted <span class="math inline">\(r_t\)</span>.</p>
<p>The setup for <span class="math inline">\(r_t\)</span> is a distribution <span class="math inline">\(N(\mu,\Sigma)\)</span> with means <span class="math inline">\(\vec\mu=[\,\mu_1,\mu_2\,]\)</span> and covariance <span class="math display">\[\Sigma=\left(\begin{array}{cc} \!{\sigma_1}^2\! &amp; \!\rho\sigma_1\sigma_2\! \\ \!\rho\sigma_1\sigma_2\! &amp; \!{\sigma_2}^2\! \end{array}\right)\text.\]</span>
The upshot is that, as in the JDM, the value of the statistic is the maximum-likelihood estimate of the latent correlation coefficient <span class="math inline">\(\rho\)</span>. Several estimation techniques are discussion in <a href="https://www.researchgate.net/publication/313196484_Polychoric_and_polyserial_correlations">Drasgow’s entry for the <em>Encyclopedia of Statistical Sciences</em></a>.</p>
<p>Implementations of all three statistics are available but require a bit of lead-in, so i’ll come back to this topic—and a grand comparison—in a future post.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Pollock and colleagues call this a “joint species distribution model”, but since i’m applying the method outside ecology, and since the structure of the model clarifies which distributions its name refers to, i’m leaving out the domain-specific qualifier.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>multidimensional scaling of variables, and rank correlations</title>
      <link>http://corybrunson.github.io/2019/08/16/rank-correlations/</link>
      <pubDate>2019 Aug 16 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>http://corybrunson.github.io/2019/08/16/rank-correlations/</guid>
      <description>


<p>A fundamental idea in biplot methodology is the <em>conference of inertia</em>, a phrase i picked up from <a href="https://stats.stackexchange.com/a/141755/68743">an SO answer by ttnphns</a> and quickly <a href="https://github.com/corybrunson/ordr/blob/master/R/ord-conference.r">incorporated into ordr</a>. The basic idea arises from the central properties of a biplot, illustrated here for principal components analysis: A case–variable data matrix <span class="math inline">\(X\in\mathbb{R}^{n\times m}\)</span> of ratio variables<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> is singular-value decomposed as <span class="math inline">\(X=UDV^\top\)</span>, for example the <code>mtcars</code> data set:</p>
<pre class="r"><code>x &lt;- scale(mtcars, center = TRUE, scale = TRUE)
s &lt;- svd(x)
r &lt;- length(s$d)</code></pre>
<p>Under this convention, <span class="math inline">\(U\in\mathbb{R}^{n\times r}\)</span> and <span class="math inline">\(V\in\mathbb{R}^{m\times r}\)</span> arise from eigendecompositions of <span class="math inline">\(XX^\top\)</span> and of <span class="math inline">\(X^\top X\)</span>, respectively, and <span class="math inline">\(D\in\mathbb{R}^{r\times r}\)</span> is the diagonal matrix of the square roots of their (common) eigenvalues. The matrix factors may be biplotted in three conventional ways:</p>
<ul>
<li>with <em>principal</em> case coordinates <span class="math inline">\(UD\)</span> and <em>standardized</em> variable coordinates <span class="math inline">\(V\)</span>;</li>
<li>with standardized case coordinates <span class="math inline">\(U\)</span> and principal variable coordinates <span class="math inline">\(VD\)</span>;</li>
<li>with <em>symmetric</em> case and variable coordinates <span class="math inline">\(UD^{1/2}\)</span> and <span class="math inline">\(VD^{1/2}\)</span>.</li>
</ul>
<p>Because both sets of eigenvectors <span class="math inline">\(U=\left[\,u_1\,\cdots\,u_r\,\right]\)</span> and <span class="math inline">\(V=\left[\,v_1\,\cdots\,v_r\,\right]\)</span> are orthonormal, <span class="math inline">\(U^\top U=I_r=V^\top V\)</span> and the total inertia (variance) in each matrix is <span class="math inline">\(\sum_{j=1}^{r}{ {v_j}^2 }=r=\sum_{j=1}^{r}{ {v_j}^2 }\)</span>. Meanwhile, <span class="math inline">\(D\)</span> contains all of the inertia of <span class="math inline">\(X\)</span>:</p>
<pre class="r"><code># inertia of the (scaled) data
sum(x^2)</code></pre>
<pre><code>## [1] 341</code></pre>
<pre class="r"><code># inertia of the case and variable factors
sum(s$u^2)</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code>sum(s$v^2)</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code># inertia of the diagonal factor
sum(s$d^2)</code></pre>
<pre><code>## [1] 341</code></pre>
<p>This inertia can then be <em>conferred</em> unto the standardized case or variable coordinates, transforming one or the other into principal coordinates (the first two options above) or both halfway there (the symmetric option). Each of these options confers the inertia in such a way that the sums of the exponents of <span class="math inline">\(D\)</span> in the transformed sets of case (<span class="math inline">\(F=UD^p\)</span>) and variable (<span class="math inline">\(G=VD^q\)</span>) coordinates is <span class="math inline">\(p+q=1\)</span>, which ensures the <em>inner product property</em> <span class="math inline">\(FG^\top=X\)</span> between them. This recovers any entry <span class="math inline">\(x_{ij}\)</span> of <span class="math inline">\(X\)</span> as the inner product <span class="math inline">\(f_i\cdot g_i\)</span> of its case and variable coordinates <span class="math inline">\(f_i=[\,f_{i,1}\,\cdots\,f_{i,r}\,]\)</span> and <span class="math inline">\(g_i=[\,g_{i,1}\,\cdots\,g_{i,r}\,]\)</span>.</p>
<p>By conferring the inertia entirely to the cases or to the variables, we preserve (or best approximate) the geometric configurations of the cases or of the variables. In PCA, the geometry of the cases is usually construed as the distances between them. Here their pairwise distances <span class="math inline">\(\sqrt{(f_{j,1}-f_{i,1})^2+(f_{j,2}-f_{i,2})^2}\)</span> in the first two PCA dimensions are plotted against their “true” distances <span class="math inline">\(\left(\sum_{k=1}^{m}{(x_{j,k}-x_{i,k})^2}\right)^{1/2}\)</span> in the variable space:</p>
<pre class="r"><code># distances between cases
x.dist &lt;- dist(x)
# distances between cases (principal coordinates)
s.dist &lt;- dist(s$u[, 1:2] %*% diag(s$d[1:2]))
# scatterplot
plot(
  x = as.vector(x.dist), y = as.vector(s.dist),
  asp = 1, pch = 19, cex = .5,
  xlab = &quot;Case distances along variable coordinates&quot;,
  ylab = &quot;Case distances in two principal coordinates&quot;
)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/case%20geometry-1.png" width="528" /></p>
<p>Meanwhile, the geometry of the variables is usually understood through their covariances or correlations. Writing <span class="math inline">\(X=[\,y_1\,\cdots\,y_m\,]\)</span> as an array of column variables, the covariance between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is proportional to their inner product <span class="math display">\[\textstyle \operatorname{cov}(y_i,y_j)=\frac{1}{n}y_i\cdot y_j=\frac{1}{n}\lVert y_i\rVert\lVert y_j\rVert\cos\theta_{ij}\text,\]</span> so that the cosine of the angle <span class="math inline">\(\theta_{ij}\)</span> between them equals their correlation:
<span class="math display">\[\cos\theta_{ij}=\frac{\operatorname{cov}(y_i,y_j)}{\sqrt{\operatorname{cov}(y_i,y_i)\operatorname{cov}(y_j,y_j)}/n}=\frac{\operatorname{cov}(y_i,y_j)}{\sigma_i\sigma_j}=r_{ij}\]</span>
Here the cosines <span class="math inline">\(\frac{g_i}{\lVert g_i\rVert}\cdot\frac{g_j}{\lVert g_j\rVert}\)</span> between the variable vectors in the first two PCA dimensions are plotted against their correlations <span class="math inline">\(r_{ij}\)</span> across the original cases:</p>
<pre class="r"><code># correlations between variables
x.cor &lt;- cor(x)
# magnitudes of variable vectors
s.len &lt;- apply(s$v[, 1:2] %*% diag(s$d[1:2]), 1, norm, &quot;2&quot;)
# cosines between variables (principal coordinates)
s.cor &lt;- (s$v[, 1:2] / s.len) %*% diag(s$d[1:2]^2) %*% t(s$v[, 1:2] / s.len)
# scatterplot
plot(
  x = as.vector(x.cor[lower.tri(x.cor)]),
  y = as.vector(s.cor[lower.tri(s.cor)]),
  asp = 1, pch = 19, cex = .5,
  xlab = &quot;Variable correlations among cases&quot;,
  ylab = &quot;Cosines between variable vectors in two principal coordinates&quot;
)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/variable%20geometry-1.png" width="528" /></p>
<div id="multidimensional-scaling-of-variables" class="section level2">
<h2>multidimensional scaling of variables</h2>
<p>The faithful approximation of inter-case distances by principal coordinates is the idea behind <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling">(classical)</a> <em>multidimensional scaling</em> (MDS), which can be applied to a data set of distances <span class="math inline">\(\delta_{ij},\ 1\leq i,j\leq n\)</span> in the absence of coordinates. This technique is based on the eigendecomposition of a doubly-centered matrix of squared distances, which produces matrix <span class="math inline">\(U\Lambda^{1/2}\)</span> whose first <span class="math inline">\(r\)</span> coordinates—for any <span class="math inline">\(r\leq n\)</span>—recover a best approximation of the inter-case distances in terms of the sum of squared errors, i.e. the variance of <span class="math inline">\((U\Lambda^{1/2})(U\Lambda^{1/2})^\top-\Delta=U\Lambda U^\top-\Delta\)</span>, where <span class="math inline">\(\Delta=(\delta_{ij})\in\mathbb{R}^{n\times n}\)</span>. In practice, the goal is usually to position points representing the <span class="math inline">\(n\)</span> cases in a 2-dimensional scatterplot so that their distances <span class="math inline">\(\sqrt{(x_j-x_i)^2+(y_j-y_i)^2}\)</span> approximate their original distances <span class="math inline">\(\delta_{ij}\)</span>, as in this example using road distances between U.S. cities to approximate their geographic arrangement:</p>
<pre class="r"><code>d &lt;- as.matrix(UScitiesD)
cent &lt;- diag(1, nrow(d)) - matrix(1/nrow(d), nrow(d), nrow(d))
d.cent &lt;- -.5 * cent %*% (d^2) %*% cent
d.mds &lt;- svd(d.cent)
d.coord &lt;- d.mds$u[, 1:2] %*% diag(sqrt(d.mds$d[1:2]))
plot(d.coord, pch = NA, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;)
text(d.coord, labels = rownames(d))</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/multidimensional%20scaling-1.png" width="816" /></p>
<p>The faithful approximation of inter-variable covariances by the inner products of their principal coordinate vectors suggests a complementary technique that i haven’t found explicitly discussed in my own background reading. Suppose we have data that consist not of distances between cases but of covariances <span class="math inline">\(\operatorname{cov}(y_i,y_j),\ 1\leq i,j\leq m\)</span> between variables. Again the data are coordinate-free, so PCA cannot be applied. Were the data to have derived from a <em>centered</em> case–variable matrix <span class="math inline">\(X\)</span>, then the covariance matrix <span class="math inline">\(C=(\operatorname{cov}(y_i,y_j))\)</span> would have been obtained as <span class="math inline">\(C=\frac{1}{n}X^\top X\)</span>, which is (up to scalar) the matrix whose eigenvectors would be given by <span class="math inline">\(V\)</span> in the SVD <span class="math inline">\(X=UDV^\top\)</span>. Therefore, we can fabricate coordinates for the <span class="math inline">\(m\)</span> variables that approximate what we know of their geometry—in this case, thinking of the variables as unknown vectors, whose magnitudes and pairwise angles are encoded in <span class="math inline">\(C\)</span>—via an eigendecomposition <span class="math inline">\(C=V\Lambda V^\top\)</span>: Take <span class="math inline">\(Y=V\Lambda^{1/2}\in\mathbb{R}^{m\times r}\)</span>, so that <span class="math inline">\(Y^\top Y\approx C\)</span>.</p>
<p>I’ll validate this line of reasoning by taking the <code>mtcars</code> data set for a spin:</p>
<pre class="r"><code># covariances and standard deviations
c &lt;- cov(mtcars)
s &lt;- diag(sqrt(diag(c)))
# centered data
x &lt;- as.matrix(scale(mtcars, center = TRUE, scale = FALSE))
# eigendecomposition of covariance matrix
c.eigen &lt;- eigen(c)
# artificial coordinates
c.coord &lt;- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
# validate covariance recovery (up to sign)
all.equal(
  as.vector(c.coord %*% t(c.coord)),
  as.vector(c),
  tolerance = 1e-12
)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Thus, whereas <em>MDS of cases</em> is used to represent distances, <em>MDS of variables</em> can be used to represent covariances.
A use case for this technique is a situation in which covariance data exist without variable values. This may of course be the case because original data has become unavailable.</p>
<p>A more interesting setting that gives rise to this situation is the analysis of multiple rankings of the same set of objects in terms of their <em>concordance</em>. Rankings’ concordance is often measured using rank correlations such as Kendall’s <span class="math inline">\(\tau\)</span>, which may be <em>general correlation coefficients</em> in <a href="https://en.wikipedia.org/wiki/Rank_correlation#General_correlation_coefficient">the sense proposed by Kendall</a> but are not associated with an underlying (Euclidean) geometry. Nevertheless, we can use MDS to represent these rankings as unit vectors in Euclidean space whose pairwise cosines approximate their rank correlations!</p>
</div>
<div id="example-rankings-of-universities" class="section level2">
<h2>example: rankings of universities</h2>
<p>A real-world example is provided by the <a href="https://www.topuniversities.com/qs-world-university-rankings/methodology">Quacquarelli Symonds Top University Rankings</a>, which include rankings of hundreds of world universities on six “metrics”: academic reputation, employer reputation, faculty–student ratio, citations per faculty, international faculty ratio, and international student ratio. QS weight these rankings differently in their overall assessment, but our analysis will compare the rankings to each other, so these weights are irrelevant. I restricted the data from the year 2020 to universities in the United States for which integer rankings (i.e. not “400+” placeholders) were available in all four years:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<pre class="r"><code>qswurus20 &lt;- readRDS(here::here(&quot;sandbox/qswurus20.rds&quot;))
head(qswurus20)</code></pre>
<pre><code>##   year                                  institution size focus res age
## 1 2020  MASSACHUSETTS INSTITUTE OF TECHNOLOGY (MIT)    M    CO  VH   5
## 2 2020                          STANFORD UNIVERSITY    L    FC  VH   5
## 3 2020                           HARVARD UNIVERSITY    L    FC  VH   5
## 4 2020 CALIFORNIA INSTITUTE OF TECHNOLOGY (CALTECH)    S    CO  VH   5
## 5 2020                        UNIVERSITY OF CHICAGO    L    FC  VH   5
## 6 2020                         PRINCETON UNIVERSITY    M    CO  VH   5
##   status rk_academic rk_employer rk_ratio rk_citations rk_intl_faculty
## 1      B           5           4       15            7              43
## 2      B           4           5       12           13              62
## 3      B           1           1       40            8             186
## 4      B          23          74        4            4              72
## 5      B          13          37       54           60             249
## 6      B          10          19      192            3             272
##   rk_intl_students
## 1               87
## 2              196
## 3              221
## 4              121
## 5              143
## 6              197</code></pre>
<p>Since the integer rankings were subsetted from the full international data set, they are not contiguous (i.e. some integers between rankings never appear). To resolve this, i’ll recalibrate the rankings by matching each vector of ranks to the vector of its sorted unique values:</p>
<pre class="r"><code>library(dplyr)
qswurus20 %&gt;%
  select(institution, starts_with(&quot;rk_&quot;)) %&gt;%
  mutate_at(
    vars(starts_with(&quot;rk_&quot;)),
    ~ match(., sort(unique(as.numeric(.))))
  ) %&gt;%
  print() -&gt; qswurus20</code></pre>
<pre><code>## # A tibble: 38 x 7
##    institution rk_academic rk_employer rk_ratio rk_citations
##    &lt;chr&gt;             &lt;int&gt;       &lt;int&gt;    &lt;int&gt;        &lt;int&gt;
##  1 MASSACHUSE…           3           2        6            3
##  2 STANFORD U…           2           3        4            5
##  3 HARVARD UN…           1           1       11            4
##  4 CALIFORNIA…          12          14        1            2
##  5 UNIVERSITY…           9          11       13           12
##  6 PRINCETON …           7           7       18            1
##  7 CORNELL UN…          11          13       22            7
##  8 UNIVERSITY…          14          12        8           17
##  9 YALE UNIVE…           6           4        2           24
## 10 COLUMBIA U…           8           8        7           25
## # … with 28 more rows, and 2 more variables: rk_intl_faculty &lt;int&gt;,
## #   rk_intl_students &lt;int&gt;</code></pre>
<p>This subset of universities is now contiguously ranked along the six dimensions described above. The Kendall correlation <span class="math inline">\(\tau_{ij}\)</span> between two rankings measures their concordance. To calculate it, every pair of universities contributes either <span class="math inline">\(+1\)</span> or <span class="math inline">\(-1\)</span> according as the rankings <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> place that pair in the same order, and the sum is scaled down by the number of pairs <span class="math inline">\({n\choose 2}\)</span> so that the result lies between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. We interpret <span class="math inline">\(\tau_{ij}=1\)</span> as perfect concordance (the rankings are equivalent), <span class="math inline">\(\tau_{ij}=-1\)</span> as perfect discordance (the rankings are reversed), and <span class="math inline">\(\tau_{ij}=0\)</span> as independence (the rankings are unrelated).</p>
<p>The QS rankings are not variations on a theme, like different measures of guideline adherence or positive affect, but they do all seem potentially sensitive to a university’s resources, including funding and prestige. I intuit that the two reputational metrics should be positively correlated, and that the two international ratios should be as well. I also wonder if the faculty–student ratio might be anti-correlated with the number of citations per faculty, separating more research-focused institutions from more teaching-focused ones.</p>
<div id="correlation-heatmap" class="section level3">
<h3>correlation heatmap</h3>
<p>A common way to visualize correlation matrices is the heatmap, so i’ll use that technique first (see below). While the rankings by academic and employer reputations are highly concordant, those by international faculty and student ratios are less so; and, the faculty–student ratio and faculty citation rankings have the weakest concordance of all, but are nevertheless positively correlated.</p>
<pre class="r"><code>c &lt;- cor(select(qswurus20, starts_with(&quot;rk_&quot;)), method = &quot;kendall&quot;)
corrplot::corrplot(c)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/Kendall%20rank%20correlations-1.png" width="672" /></p>
<p>This visualization is useful, but it’s very busy: To compare any pair of rankings, i have to find the cell in the grid corresponding to that pair and refer back to the color scale to assess its meaning. I can’t rely on the nearby cells for context, because they may be stronger or weaker than average and skew my interpretation. For example, the visibly weak associations between the faculty–student ratio and other rankings (the third row or column) happen to be arranged so that the slightly stronger among them, with the two reputational variables, are sandwiched between the <em>even stronger</em> associations between the two reputational rankings and between them and the faculty citations ranking; whereas its weaker associations are sandwiched between more typical, but still comparatively stronger, associations. A different ordering of the variables might “obscure” this pattern and “reveal” others.</p>
<p>The plot is also strictly pairwise: Every correlation between two rankings occupies its own cell—two, in fact, making almost half of the plot duplicative. This means that a subset analysis of, say, three rankings requires focusing on three cells at the corners of a right triangle while ignoring all the surrounding cells. This is not an easy visual task. It would be straightforward to create a new plot for any subset, but then the larger context of the remaining rankings would be lost.</p>
</div>
<div id="correlation-biplot" class="section level3">
<h3>correlation biplot</h3>
<p>MDS of variables offers a natural alternative visualization: the biplot. As with MDS of cases, the point isn’t to overlay the case scores and variable loadings from a singular value decomposition, but to use the scores or loadings alone to endow the cases or variables with a Euclidean geometry they didn’t yet have. To that end, i’ll plot the variables as vectors with tails at the origin and heads at their fabricated coordinates <span class="math inline">\(Y=V\Lambda^{1/2}\)</span>:</p>
<pre class="r"><code>c.eigen &lt;- eigen(c)
c.coord &lt;- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
plot(c.coord, pch = NA, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;)
arrows(0, 0, c.coord[, 1], c.coord[, 2])
text(c.coord, labels = rownames(c))</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/multidimensional%20scaling%20of%20variables-1.png" width="672" /></p>
<p>A more elegant ggplot2-style graphic can be rendered with <a href="https://github.com/corybrunson/ordr">ordr</a>, with a unit circle included for reference:</p>
<pre class="r"><code>library(ordr)
eigen_ord(c) %&gt;%
  as_tbl_ord() %&gt;%
  augment() %&gt;%
  mutate_u(metric = stringr::str_remove(.name, &quot;rk_&quot;)) %&gt;%
  confer_inertia(1) %&gt;%
  negate_to_nonneg_orthant(&quot;u&quot;) -&gt;
  c_eigen
c_eigen %&gt;%
  ggbiplot() +
  theme_minimal() +
  geom_unit_circle() +
  geom_u_vector() +
  geom_u_text_radiate(aes(label = metric)) +
  scale_x_continuous(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .2)) +
  ggtitle(&quot;MDS of Kendall correlations between university rankings&quot;)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>With respect to the pairwise correlations, the biplot is significantly less precise: Though the vectors all have unit length in <span class="math inline">\(\mathbb{R}^r\)</span> (<span class="math inline">\(r\leq m=6\)</span>), their projections onto the first two principal coordinates are much shorter, indicating that much of the geometric configuration requires additional dimensions to represent. Indeed, these coordinates capture only <span class="math inline">\(48.2\%+14.3\%=62.5\%\)</span> of the inertia in the full representation. This means that the angles between the vectors must be interpreted with caution: For example, it looks like the academic and employer reputation rankings are extremely correlated, but the apparent alignment of the vectors could be an artifact of the projection, when in fact they “rise” and “fall” in opposite directions along the remaining dimensions. The correlation heatmap leaves no such ambiguity.</p>
<p>However, the biplot far surpasses the heatmap at parsimony: Each variable is represented by a single vector, and the angle cosines between the variable vectors roughly approximate their correlations. For instance, the rankings based on international student and faculty ratios have correlation around <span class="math inline">\(\cos(\frac{\pi}{4})=\frac{1}{\sqrt{2}}\)</span>, corresponding to either explaining half the “variance” in the other—not technically meaningful in the ranking context but a useful conceptual anchor. Meanwhile, the faculty–student ratio ranking is nearly independent of the faculty citation ranking, contrary to my intuition that these rankings would reflect a <em>reverse</em> association between research- and teaching-oriented institutions. The convenience of recognizing correlations as cosines may be worth the significant risk of error, especially since that error (the residual <span class="math inline">\(37.5\%\)</span> of inertia) can be exactly quantified.</p>
<p>Moreover, the principal coordinates of the variable vectors indicate their loadings onto the first and second principal moments of inertia—the two dimensions that capture the most variation in the data. For example, the first principal coordinate is most aligned with the two reputational rankings, suggesting that a general prestige ranking is the strongest overall component of the several specific rankings. In contrast, the faculty–student ratio and faculty citation rankings load most strongly onto the second principal coordinate, suggesting that the divide between research- and teaching-focused institutions may yet be important to understanding how universities compare along these different metrics. These observations, provisional though they are, would be difficult to discern from the heatmap. More importantly, unlike the secondary patterns visible in the heatmap, these are in no sense artifacts of the layout but arise directly from the (correlational) data.</p>
<p>This last point means that observations made from a biplot can be validated from the MDS coordinates. In particular, we can examine the variables’ loadings onto the third principal coordinate, and we can check whether the reputational rankings are aligned or misaligned along it.</p>
<pre class="r"><code>c_eigen %&gt;%
  tidy(.matrix = &quot;u&quot;) %&gt;%
  select(-.name, -.matrix)</code></pre>
<pre><code>## # A tibble: 6 x 7
##     EV1     EV2     EV3     EV4     EV5      EV6 metric       
##   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        
## 1 0.834 -0.0907 -0.412   0.0430  0.0206 -0.351   academic     
## 2 0.795 -0.0964 -0.477  -0.0416  0.181   0.311   employer     
## 3 0.517  0.771   0.0480  0.331  -0.158   0.0372  ratio        
## 4 0.731 -0.352   0.239  -0.0278 -0.528   0.0685  citations    
## 5 0.631 -0.233   0.521   0.392   0.352  -0.00783 intl_faculty 
## 6 0.603  0.262   0.324  -0.665   0.140  -0.0312  intl_students</code></pre>
<pre class="r"><code>c_eigen %&gt;%
  tidy(.matrix = &quot;coord&quot;)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   .name .values .inertia .prop_var
##   &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 EV1     2.89     2.89     0.482 
## 2 EV2     0.858    0.858    0.143 
## 3 EV3     0.833    0.833    0.139 
## 4 EV4     0.709    0.709    0.118 
## 5 EV5     0.480    0.480    0.0799
## 6 EV6     0.227    0.227    0.0379</code></pre>
<p>Based on the third principal coordinates, the reputational rankings are aligned, as we knew already from the correlation matrix and heatmap. What’s a bit more interesting is that this component seems to separate these two rankings from those having to do with faculty citation rates and the international compositions of the faculty and student body. Based on the decomposition of inertia, this third principal coordinate is nearly as important as the second! It therefore makes sense to plot the two together:</p>
<pre class="r"><code>c_eigen %&gt;%
  ggbiplot(aes(x = 2, y = 3)) +
  theme_minimal() +
  geom_unit_circle() +
  geom_u_vector() +
  geom_u_text_radiate(aes(label = metric)) +
  scale_x_continuous(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .4)) +
  ggtitle(&quot;MDS of Kendall correlations between university rankings&quot;,
          &quot;Second and third principal coordinates&quot;)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The primary antitheses of the reputational rankings, after removing the first principal coordinate, are the two rankings based on international composition—and this axis is largely independent of the axis apparently distinguishing research- from teaching-oriented institutions. From my own limited knowledge, i’d hazard a guess that this reflects two tiers of international representation among students and faculty, one expressed by the most prestigious institutions that recruit highly qualified applicants from all over the world, and the other expressed by institutions that are not especially prestigious but are located in communities or regions with high percentages of international residents.</p>
<p>This is of course no more than idle speculation on my part! But a visualization scheme that encourages hypothesis generation is worth having on hand.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>That is to say, if the variables don’t have meaningful zero values and/or commensurate scales, then they should be centered to zero mean and/or scaled to unit variance.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This leaves us with only 38 universities, so my inferences must be taken with extreme caution!<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded>


<p>A fundamental idea in biplot methodology is the <em>conference of inertia</em>, a phrase i picked up from <a href="https://stats.stackexchange.com/a/141755/68743">an SO answer by ttnphns</a> and quickly <a href="https://github.com/corybrunson/ordr/blob/master/R/ord-conference.r">incorporated into ordr</a>. The basic idea arises from the central properties of a biplot, illustrated here for principal components analysis: A case–variable data matrix <span class="math inline">\(X\in\mathbb{R}^{n\times m}\)</span> of ratio variables<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> is singular-value decomposed as <span class="math inline">\(X=UDV^\top\)</span>, for example the <code>mtcars</code> data set:</p>
<pre class="r"><code>x &lt;- scale(mtcars, center = TRUE, scale = TRUE)
s &lt;- svd(x)
r &lt;- length(s$d)</code></pre>
<p>Under this convention, <span class="math inline">\(U\in\mathbb{R}^{n\times r}\)</span> and <span class="math inline">\(V\in\mathbb{R}^{m\times r}\)</span> arise from eigendecompositions of <span class="math inline">\(XX^\top\)</span> and of <span class="math inline">\(X^\top X\)</span>, respectively, and <span class="math inline">\(D\in\mathbb{R}^{r\times r}\)</span> is the diagonal matrix of the square roots of their (common) eigenvalues. The matrix factors may be biplotted in three conventional ways:</p>
<ul>
<li>with <em>principal</em> case coordinates <span class="math inline">\(UD\)</span> and <em>standardized</em> variable coordinates <span class="math inline">\(V\)</span>;</li>
<li>with standardized case coordinates <span class="math inline">\(U\)</span> and principal variable coordinates <span class="math inline">\(VD\)</span>;</li>
<li>with <em>symmetric</em> case and variable coordinates <span class="math inline">\(UD^{1/2}\)</span> and <span class="math inline">\(VD^{1/2}\)</span>.</li>
</ul>
<p>Because both sets of eigenvectors <span class="math inline">\(U=\left[\,u_1\,\cdots\,u_r\,\right]\)</span> and <span class="math inline">\(V=\left[\,v_1\,\cdots\,v_r\,\right]\)</span> are orthonormal, <span class="math inline">\(U^\top U=I_r=V^\top V\)</span> and the total inertia (variance) in each matrix is <span class="math inline">\(\sum_{j=1}^{r}{ {v_j}^2 }=r=\sum_{j=1}^{r}{ {v_j}^2 }\)</span>. Meanwhile, <span class="math inline">\(D\)</span> contains all of the inertia of <span class="math inline">\(X\)</span>:</p>
<pre class="r"><code># inertia of the (scaled) data
sum(x^2)</code></pre>
<pre><code>## [1] 341</code></pre>
<pre class="r"><code># inertia of the case and variable factors
sum(s$u^2)</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code>sum(s$v^2)</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code># inertia of the diagonal factor
sum(s$d^2)</code></pre>
<pre><code>## [1] 341</code></pre>
<p>This inertia can then be <em>conferred</em> unto the standardized case or variable coordinates, transforming one or the other into principal coordinates (the first two options above) or both halfway there (the symmetric option). Each of these options confers the inertia in such a way that the sums of the exponents of <span class="math inline">\(D\)</span> in the transformed sets of case (<span class="math inline">\(F=UD^p\)</span>) and variable (<span class="math inline">\(G=VD^q\)</span>) coordinates is <span class="math inline">\(p+q=1\)</span>, which ensures the <em>inner product property</em> <span class="math inline">\(FG^\top=X\)</span> between them. This recovers any entry <span class="math inline">\(x_{ij}\)</span> of <span class="math inline">\(X\)</span> as the inner product <span class="math inline">\(f_i\cdot g_i\)</span> of its case and variable coordinates <span class="math inline">\(f_i=[\,f_{i,1}\,\cdots\,f_{i,r}\,]\)</span> and <span class="math inline">\(g_i=[\,g_{i,1}\,\cdots\,g_{i,r}\,]\)</span>.</p>
<p>By conferring the inertia entirely to the cases or to the variables, we preserve (or best approximate) the geometric configurations of the cases or of the variables. In PCA, the geometry of the cases is usually construed as the distances between them. Here their pairwise distances <span class="math inline">\(\sqrt{(f_{j,1}-f_{i,1})^2+(f_{j,2}-f_{i,2})^2}\)</span> in the first two PCA dimensions are plotted against their “true” distances <span class="math inline">\(\left(\sum_{k=1}^{m}{(x_{j,k}-x_{i,k})^2}\right)^{1/2}\)</span> in the variable space:</p>
<pre class="r"><code># distances between cases
x.dist &lt;- dist(x)
# distances between cases (principal coordinates)
s.dist &lt;- dist(s$u[, 1:2] %*% diag(s$d[1:2]))
# scatterplot
plot(
  x = as.vector(x.dist), y = as.vector(s.dist),
  asp = 1, pch = 19, cex = .5,
  xlab = &quot;Case distances along variable coordinates&quot;,
  ylab = &quot;Case distances in two principal coordinates&quot;
)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/case%20geometry-1.png" width="528" /></p>
<p>Meanwhile, the geometry of the variables is usually understood through their covariances or correlations. Writing <span class="math inline">\(X=[\,y_1\,\cdots\,y_m\,]\)</span> as an array of column variables, the covariance between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is proportional to their inner product <span class="math display">\[\textstyle \operatorname{cov}(y_i,y_j)=\frac{1}{n}y_i\cdot y_j=\frac{1}{n}\lVert y_i\rVert\lVert y_j\rVert\cos\theta_{ij}\text,\]</span> so that the cosine of the angle <span class="math inline">\(\theta_{ij}\)</span> between them equals their correlation:
<span class="math display">\[\cos\theta_{ij}=\frac{\operatorname{cov}(y_i,y_j)}{\sqrt{\operatorname{cov}(y_i,y_i)\operatorname{cov}(y_j,y_j)}/n}=\frac{\operatorname{cov}(y_i,y_j)}{\sigma_i\sigma_j}=r_{ij}\]</span>
Here the cosines <span class="math inline">\(\frac{g_i}{\lVert g_i\rVert}\cdot\frac{g_j}{\lVert g_j\rVert}\)</span> between the variable vectors in the first two PCA dimensions are plotted against their correlations <span class="math inline">\(r_{ij}\)</span> across the original cases:</p>
<pre class="r"><code># correlations between variables
x.cor &lt;- cor(x)
# magnitudes of variable vectors
s.len &lt;- apply(s$v[, 1:2] %*% diag(s$d[1:2]), 1, norm, &quot;2&quot;)
# cosines between variables (principal coordinates)
s.cor &lt;- (s$v[, 1:2] / s.len) %*% diag(s$d[1:2]^2) %*% t(s$v[, 1:2] / s.len)
# scatterplot
plot(
  x = as.vector(x.cor[lower.tri(x.cor)]),
  y = as.vector(s.cor[lower.tri(s.cor)]),
  asp = 1, pch = 19, cex = .5,
  xlab = &quot;Variable correlations among cases&quot;,
  ylab = &quot;Cosines between variable vectors in two principal coordinates&quot;
)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/variable%20geometry-1.png" width="528" /></p>
<div id="multidimensional-scaling-of-variables" class="section level2">
<h2>multidimensional scaling of variables</h2>
<p>The faithful approximation of inter-case distances by principal coordinates is the idea behind <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling">(classical)</a> <em>multidimensional scaling</em> (MDS), which can be applied to a data set of distances <span class="math inline">\(\delta_{ij},\ 1\leq i,j\leq n\)</span> in the absence of coordinates. This technique is based on the eigendecomposition of a doubly-centered matrix of squared distances, which produces matrix <span class="math inline">\(U\Lambda^{1/2}\)</span> whose first <span class="math inline">\(r\)</span> coordinates—for any <span class="math inline">\(r\leq n\)</span>—recover a best approximation of the inter-case distances in terms of the sum of squared errors, i.e. the variance of <span class="math inline">\((U\Lambda^{1/2})(U\Lambda^{1/2})^\top-\Delta=U\Lambda U^\top-\Delta\)</span>, where <span class="math inline">\(\Delta=(\delta_{ij})\in\mathbb{R}^{n\times n}\)</span>. In practice, the goal is usually to position points representing the <span class="math inline">\(n\)</span> cases in a 2-dimensional scatterplot so that their distances <span class="math inline">\(\sqrt{(x_j-x_i)^2+(y_j-y_i)^2}\)</span> approximate their original distances <span class="math inline">\(\delta_{ij}\)</span>, as in this example using road distances between U.S. cities to approximate their geographic arrangement:</p>
<pre class="r"><code>d &lt;- as.matrix(UScitiesD)
cent &lt;- diag(1, nrow(d)) - matrix(1/nrow(d), nrow(d), nrow(d))
d.cent &lt;- -.5 * cent %*% (d^2) %*% cent
d.mds &lt;- svd(d.cent)
d.coord &lt;- d.mds$u[, 1:2] %*% diag(sqrt(d.mds$d[1:2]))
plot(d.coord, pch = NA, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;)
text(d.coord, labels = rownames(d))</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/multidimensional%20scaling-1.png" width="816" /></p>
<p>The faithful approximation of inter-variable covariances by the inner products of their principal coordinate vectors suggests a complementary technique that i haven’t found explicitly discussed in my own background reading. Suppose we have data that consist not of distances between cases but of covariances <span class="math inline">\(\operatorname{cov}(y_i,y_j),\ 1\leq i,j\leq m\)</span> between variables. Again the data are coordinate-free, so PCA cannot be applied. Were the data to have derived from a <em>centered</em> case–variable matrix <span class="math inline">\(X\)</span>, then the covariance matrix <span class="math inline">\(C=(\operatorname{cov}(y_i,y_j))\)</span> would have been obtained as <span class="math inline">\(C=\frac{1}{n}X^\top X\)</span>, which is (up to scalar) the matrix whose eigenvectors would be given by <span class="math inline">\(V\)</span> in the SVD <span class="math inline">\(X=UDV^\top\)</span>. Therefore, we can fabricate coordinates for the <span class="math inline">\(m\)</span> variables that approximate what we know of their geometry—in this case, thinking of the variables as unknown vectors, whose magnitudes and pairwise angles are encoded in <span class="math inline">\(C\)</span>—via an eigendecomposition <span class="math inline">\(C=V\Lambda V^\top\)</span>: Take <span class="math inline">\(Y=V\Lambda^{1/2}\in\mathbb{R}^{m\times r}\)</span>, so that <span class="math inline">\(Y^\top Y\approx C\)</span>.</p>
<p>I’ll validate this line of reasoning by taking the <code>mtcars</code> data set for a spin:</p>
<pre class="r"><code># covariances and standard deviations
c &lt;- cov(mtcars)
s &lt;- diag(sqrt(diag(c)))
# centered data
x &lt;- as.matrix(scale(mtcars, center = TRUE, scale = FALSE))
# eigendecomposition of covariance matrix
c.eigen &lt;- eigen(c)
# artificial coordinates
c.coord &lt;- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
# validate covariance recovery (up to sign)
all.equal(
  as.vector(c.coord %*% t(c.coord)),
  as.vector(c),
  tolerance = 1e-12
)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Thus, whereas <em>MDS of cases</em> is used to represent distances, <em>MDS of variables</em> can be used to represent covariances.
A use case for this technique is a situation in which covariance data exist without variable values. This may of course be the case because original data has become unavailable.</p>
<p>A more interesting setting that gives rise to this situation is the analysis of multiple rankings of the same set of objects in terms of their <em>concordance</em>. Rankings’ concordance is often measured using rank correlations such as Kendall’s <span class="math inline">\(\tau\)</span>, which may be <em>general correlation coefficients</em> in <a href="https://en.wikipedia.org/wiki/Rank_correlation#General_correlation_coefficient">the sense proposed by Kendall</a> but are not associated with an underlying (Euclidean) geometry. Nevertheless, we can use MDS to represent these rankings as unit vectors in Euclidean space whose pairwise cosines approximate their rank correlations!</p>
</div>
<div id="example-rankings-of-universities" class="section level2">
<h2>example: rankings of universities</h2>
<p>A real-world example is provided by the <a href="https://www.topuniversities.com/qs-world-university-rankings/methodology">Quacquarelli Symonds Top University Rankings</a>, which include rankings of hundreds of world universities on six “metrics”: academic reputation, employer reputation, faculty–student ratio, citations per faculty, international faculty ratio, and international student ratio. QS weight these rankings differently in their overall assessment, but our analysis will compare the rankings to each other, so these weights are irrelevant. I restricted the data from the year 2020 to universities in the United States for which integer rankings (i.e. not “400+” placeholders) were available in all four years:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<pre class="r"><code>qswurus20 &lt;- readRDS(here::here(&quot;sandbox/qswurus20.rds&quot;))
head(qswurus20)</code></pre>
<pre><code>##   year                                  institution size focus res age
## 1 2020  MASSACHUSETTS INSTITUTE OF TECHNOLOGY (MIT)    M    CO  VH   5
## 2 2020                          STANFORD UNIVERSITY    L    FC  VH   5
## 3 2020                           HARVARD UNIVERSITY    L    FC  VH   5
## 4 2020 CALIFORNIA INSTITUTE OF TECHNOLOGY (CALTECH)    S    CO  VH   5
## 5 2020                        UNIVERSITY OF CHICAGO    L    FC  VH   5
## 6 2020                         PRINCETON UNIVERSITY    M    CO  VH   5
##   status rk_academic rk_employer rk_ratio rk_citations rk_intl_faculty
## 1      B           5           4       15            7              43
## 2      B           4           5       12           13              62
## 3      B           1           1       40            8             186
## 4      B          23          74        4            4              72
## 5      B          13          37       54           60             249
## 6      B          10          19      192            3             272
##   rk_intl_students
## 1               87
## 2              196
## 3              221
## 4              121
## 5              143
## 6              197</code></pre>
<p>Since the integer rankings were subsetted from the full international data set, they are not contiguous (i.e. some integers between rankings never appear). To resolve this, i’ll recalibrate the rankings by matching each vector of ranks to the vector of its sorted unique values:</p>
<pre class="r"><code>library(dplyr)
qswurus20 %&gt;%
  select(institution, starts_with(&quot;rk_&quot;)) %&gt;%
  mutate_at(
    vars(starts_with(&quot;rk_&quot;)),
    ~ match(., sort(unique(as.numeric(.))))
  ) %&gt;%
  print() -&gt; qswurus20</code></pre>
<pre><code>## # A tibble: 38 x 7
##    institution rk_academic rk_employer rk_ratio rk_citations
##    &lt;chr&gt;             &lt;int&gt;       &lt;int&gt;    &lt;int&gt;        &lt;int&gt;
##  1 MASSACHUSE…           3           2        6            3
##  2 STANFORD U…           2           3        4            5
##  3 HARVARD UN…           1           1       11            4
##  4 CALIFORNIA…          12          14        1            2
##  5 UNIVERSITY…           9          11       13           12
##  6 PRINCETON …           7           7       18            1
##  7 CORNELL UN…          11          13       22            7
##  8 UNIVERSITY…          14          12        8           17
##  9 YALE UNIVE…           6           4        2           24
## 10 COLUMBIA U…           8           8        7           25
## # … with 28 more rows, and 2 more variables: rk_intl_faculty &lt;int&gt;,
## #   rk_intl_students &lt;int&gt;</code></pre>
<p>This subset of universities is now contiguously ranked along the six dimensions described above. The Kendall correlation <span class="math inline">\(\tau_{ij}\)</span> between two rankings measures their concordance. To calculate it, every pair of universities contributes either <span class="math inline">\(+1\)</span> or <span class="math inline">\(-1\)</span> according as the rankings <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> place that pair in the same order, and the sum is scaled down by the number of pairs <span class="math inline">\({n\choose 2}\)</span> so that the result lies between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. We interpret <span class="math inline">\(\tau_{ij}=1\)</span> as perfect concordance (the rankings are equivalent), <span class="math inline">\(\tau_{ij}=-1\)</span> as perfect discordance (the rankings are reversed), and <span class="math inline">\(\tau_{ij}=0\)</span> as independence (the rankings are unrelated).</p>
<p>The QS rankings are not variations on a theme, like different measures of guideline adherence or positive affect, but they do all seem potentially sensitive to a university’s resources, including funding and prestige. I intuit that the two reputational metrics should be positively correlated, and that the two international ratios should be as well. I also wonder if the faculty–student ratio might be anti-correlated with the number of citations per faculty, separating more research-focused institutions from more teaching-focused ones.</p>
<div id="correlation-heatmap" class="section level3">
<h3>correlation heatmap</h3>
<p>A common way to visualize correlation matrices is the heatmap, so i’ll use that technique first (see below). While the rankings by academic and employer reputations are highly concordant, those by international faculty and student ratios are less so; and, the faculty–student ratio and faculty citation rankings have the weakest concordance of all, but are nevertheless positively correlated.</p>
<pre class="r"><code>c &lt;- cor(select(qswurus20, starts_with(&quot;rk_&quot;)), method = &quot;kendall&quot;)
corrplot::corrplot(c)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/Kendall%20rank%20correlations-1.png" width="672" /></p>
<p>This visualization is useful, but it’s very busy: To compare any pair of rankings, i have to find the cell in the grid corresponding to that pair and refer back to the color scale to assess its meaning. I can’t rely on the nearby cells for context, because they may be stronger or weaker than average and skew my interpretation. For example, the visibly weak associations between the faculty–student ratio and other rankings (the third row or column) happen to be arranged so that the slightly stronger among them, with the two reputational variables, are sandwiched between the <em>even stronger</em> associations between the two reputational rankings and between them and the faculty citations ranking; whereas its weaker associations are sandwiched between more typical, but still comparatively stronger, associations. A different ordering of the variables might “obscure” this pattern and “reveal” others.</p>
<p>The plot is also strictly pairwise: Every correlation between two rankings occupies its own cell—two, in fact, making almost half of the plot duplicative. This means that a subset analysis of, say, three rankings requires focusing on three cells at the corners of a right triangle while ignoring all the surrounding cells. This is not an easy visual task. It would be straightforward to create a new plot for any subset, but then the larger context of the remaining rankings would be lost.</p>
</div>
<div id="correlation-biplot" class="section level3">
<h3>correlation biplot</h3>
<p>MDS of variables offers a natural alternative visualization: the biplot. As with MDS of cases, the point isn’t to overlay the case scores and variable loadings from a singular value decomposition, but to use the scores or loadings alone to endow the cases or variables with a Euclidean geometry they didn’t yet have. To that end, i’ll plot the variables as vectors with tails at the origin and heads at their fabricated coordinates <span class="math inline">\(Y=V\Lambda^{1/2}\)</span>:</p>
<pre class="r"><code>c.eigen &lt;- eigen(c)
c.coord &lt;- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
plot(c.coord, pch = NA, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;)
arrows(0, 0, c.coord[, 1], c.coord[, 2])
text(c.coord, labels = rownames(c))</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/multidimensional%20scaling%20of%20variables-1.png" width="672" /></p>
<p>A more elegant ggplot2-style graphic can be rendered with <a href="https://github.com/corybrunson/ordr">ordr</a>, with a unit circle included for reference:</p>
<pre class="r"><code>library(ordr)
eigen_ord(c) %&gt;%
  as_tbl_ord() %&gt;%
  augment() %&gt;%
  mutate_u(metric = stringr::str_remove(.name, &quot;rk_&quot;)) %&gt;%
  confer_inertia(1) %&gt;%
  negate_to_nonneg_orthant(&quot;u&quot;) -&gt;
  c_eigen
c_eigen %&gt;%
  ggbiplot() +
  theme_minimal() +
  geom_unit_circle() +
  geom_u_vector() +
  geom_u_text_radiate(aes(label = metric)) +
  scale_x_continuous(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .2)) +
  ggtitle(&quot;MDS of Kendall correlations between university rankings&quot;)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>With respect to the pairwise correlations, the biplot is significantly less precise: Though the vectors all have unit length in <span class="math inline">\(\mathbb{R}^r\)</span> (<span class="math inline">\(r\leq m=6\)</span>), their projections onto the first two principal coordinates are much shorter, indicating that much of the geometric configuration requires additional dimensions to represent. Indeed, these coordinates capture only <span class="math inline">\(48.2\%+14.3\%=62.5\%\)</span> of the inertia in the full representation. This means that the angles between the vectors must be interpreted with caution: For example, it looks like the academic and employer reputation rankings are extremely correlated, but the apparent alignment of the vectors could be an artifact of the projection, when in fact they “rise” and “fall” in opposite directions along the remaining dimensions. The correlation heatmap leaves no such ambiguity.</p>
<p>However, the biplot far surpasses the heatmap at parsimony: Each variable is represented by a single vector, and the angle cosines between the variable vectors roughly approximate their correlations. For instance, the rankings based on international student and faculty ratios have correlation around <span class="math inline">\(\cos(\frac{\pi}{4})=\frac{1}{\sqrt{2}}\)</span>, corresponding to either explaining half the “variance” in the other—not technically meaningful in the ranking context but a useful conceptual anchor. Meanwhile, the faculty–student ratio ranking is nearly independent of the faculty citation ranking, contrary to my intuition that these rankings would reflect a <em>reverse</em> association between research- and teaching-oriented institutions. The convenience of recognizing correlations as cosines may be worth the significant risk of error, especially since that error (the residual <span class="math inline">\(37.5\%\)</span> of inertia) can be exactly quantified.</p>
<p>Moreover, the principal coordinates of the variable vectors indicate their loadings onto the first and second principal moments of inertia—the two dimensions that capture the most variation in the data. For example, the first principal coordinate is most aligned with the two reputational rankings, suggesting that a general prestige ranking is the strongest overall component of the several specific rankings. In contrast, the faculty–student ratio and faculty citation rankings load most strongly onto the second principal coordinate, suggesting that the divide between research- and teaching-focused institutions may yet be important to understanding how universities compare along these different metrics. These observations, provisional though they are, would be difficult to discern from the heatmap. More importantly, unlike the secondary patterns visible in the heatmap, these are in no sense artifacts of the layout but arise directly from the (correlational) data.</p>
<p>This last point means that observations made from a biplot can be validated from the MDS coordinates. In particular, we can examine the variables’ loadings onto the third principal coordinate, and we can check whether the reputational rankings are aligned or misaligned along it.</p>
<pre class="r"><code>c_eigen %&gt;%
  tidy(.matrix = &quot;u&quot;) %&gt;%
  select(-.name, -.matrix)</code></pre>
<pre><code>## # A tibble: 6 x 7
##     EV1     EV2     EV3     EV4     EV5      EV6 metric       
##   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        
## 1 0.834 -0.0907 -0.412   0.0430  0.0206 -0.351   academic     
## 2 0.795 -0.0964 -0.477  -0.0416  0.181   0.311   employer     
## 3 0.517  0.771   0.0480  0.331  -0.158   0.0372  ratio        
## 4 0.731 -0.352   0.239  -0.0278 -0.528   0.0685  citations    
## 5 0.631 -0.233   0.521   0.392   0.352  -0.00783 intl_faculty 
## 6 0.603  0.262   0.324  -0.665   0.140  -0.0312  intl_students</code></pre>
<pre class="r"><code>c_eigen %&gt;%
  tidy(.matrix = &quot;coord&quot;)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   .name .values .inertia .prop_var
##   &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 EV1     2.89     2.89     0.482 
## 2 EV2     0.858    0.858    0.143 
## 3 EV3     0.833    0.833    0.139 
## 4 EV4     0.709    0.709    0.118 
## 5 EV5     0.480    0.480    0.0799
## 6 EV6     0.227    0.227    0.0379</code></pre>
<p>Based on the third principal coordinates, the reputational rankings are aligned, as we knew already from the correlation matrix and heatmap. What’s a bit more interesting is that this component seems to separate these two rankings from those having to do with faculty citation rates and the international compositions of the faculty and student body. Based on the decomposition of inertia, this third principal coordinate is nearly as important as the second! It therefore makes sense to plot the two together:</p>
<pre class="r"><code>c_eigen %&gt;%
  ggbiplot(aes(x = 2, y = 3)) +
  theme_minimal() +
  geom_unit_circle() +
  geom_u_vector() +
  geom_u_text_radiate(aes(label = metric)) +
  scale_x_continuous(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .4)) +
  ggtitle(&quot;MDS of Kendall correlations between university rankings&quot;,
          &quot;Second and third principal coordinates&quot;)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The primary antitheses of the reputational rankings, after removing the first principal coordinate, are the two rankings based on international composition—and this axis is largely independent of the axis apparently distinguishing research- from teaching-oriented institutions. From my own limited knowledge, i’d hazard a guess that this reflects two tiers of international representation among students and faculty, one expressed by the most prestigious institutions that recruit highly qualified applicants from all over the world, and the other expressed by institutions that are not especially prestigious but are located in communities or regions with high percentages of international residents.</p>
<p>This is of course no more than idle speculation on my part! But a visualization scheme that encourages hypothesis generation is worth having on hand.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>That is to say, if the variables don’t have meaningful zero values and/or commensurate scales, then they should be centered to zero mean and/or scaled to unit variance.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This leaves us with only 38 universities, so my inferences must be taken with extreme caution!<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>I AM THE DISCRIMINANT</title>
      <link>http://corybrunson.github.io/2019/08/02/lda/</link>
      <pubDate>2019 Aug 02 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>http://corybrunson.github.io/2019/08/02/lda/</guid>
      <description>


<p>For over a year (intermittently, not cumulatively) i’ve been developing an R package, <a href="https://github.com/corybrunson/ordr">ordr</a>, for incorporating ordination techniques into a tidyverse workflow. (Credit where it’s due: Emily Paul, a summer intern from Avon High School, helped extend the package functionality and has since used it in other projects and provided critical feedback on its design.) This week, i finally deployed <a href="https://corybrunson.github.io/ordr/">a pkgdown website</a> and tweeted a meek solicitation for feedback.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Interestingly, the one component that held me back, that i put off time and time again before finally grinding out a solution over the past few weeks, was the component that originally complemented PCA in <a href="https://github.com/vqv/ggbiplot">Vince Vu’s original and inspirational ggbiplot package</a>: Accessor methods for the MASS package implementation of linear discriminant analysis.</p>
<p>A big part of the reason for this procrastination was that <a href="https://github.com/cran/MASS/blob/master/R/lda.R">the source code of <code>MASS::lda()</code></a> looks nothing like the tidy definition of LDA found in textbooks. (I have on my shelf my mom’s copies of <a href="https://books.google.com/books/about/Multivariate_Analysis.html?id=HHNnBDaNsuUC">Tatsuoka’s <em>Multivariate Analysis</em></a> and of <a href="https://books.google.com/books/about/Psychometric_theory.html?id=WE59AAAAMAAJ">Nunnally’s <em>Psychometric Theory</em></a>, but there are several similar introductions online, for example <a href="https://sebastianraschka.com/Articles/2014_python_lda.html">Raschka’s</a>.) In the standard presentation, LDA boils down to an eigendecomposition of the quotient <span class="math inline">\({C_W}^{-1}C_B\)</span> of the between- by the within-group covariance matrices. (This is equivalent, up to a scale factor, to the eigendecomposition of the same quotient of the corresponding scatter matrices <span class="math inline">\(S_\bullet=nC_\bullet\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>)
But rather than build up to a single eigendecomposition, <code>MASS::lda()</code> relies on sequential compositions of a scaling matrix with eigenvector and other matrices that are difficult to follow by reading the (minimally documented) code.</p>
<p>Meanwhile, several discussions of LDA on StackOverflow provide useful insights into the output of <code>MASS::lda()</code>, but the more prolific respondents tend not to use R, and the numerical examples derive from implementations in other languages.
The MASS book, <em>Modern Applied Statistics with S</em> by Venables and Ripley (<a href="https://tinyurl.com/yyhpyu2d">PDF</a>), details their mathematical formula in section 12.1 but does not discuss their implementation.
So, while it’s still fresh in my mind, i want to offer what i hope will be an evergreen dissection of <code>MASS::lda()</code>. In a future post i’ll survey different ways an LDA might be summarized in a biplot and how i tweaked Venables and Ripley’s function to make these options available in ordr.</p>
<div id="tldr" class="section level3">
<h3>tl;dr</h3>
<p><code>MASS::lda()</code> gradually and conditionally composes a variable transformation (a matrix that acts on the columns of the data matrix) to simplify the covariance quotient, then uses singular value decomposition to obtain its eigendecomposition. Rather than returning both the variable transformation and the quotient eigendecomposition, it returns their product, a matrix of discriminant coefficients that transforms the data and/or group centroids to their discriminant coordinates (scores). Consequently, the variable loadings cannot be recovered from the output.</p>
</div>
<div id="class-methods" class="section level3">
<h3>class methods</h3>
<p>For those (like me until recently) unfamiliar with <a href="http://adv-r.had.co.nz/S3.html">the S3 object system</a>, <a href="https://github.com/cran/MASS/blob/master/R/lda.R">the <code>MASS::lda()</code> source code</a> may look strange. Essentially, and henceforth assuming the MASS package has been attached so that <code>MASS::</code> is unnecessary, the generic function <code>lda()</code> performs <em>method dispatch</em> by checking the class(es) of its primary input <code>x</code>, then sending <code>x</code> (and any other inputs) to one of several class-specific methods <code>lda.&lt;class&gt;()</code>, or else to <code>lda.default()</code>. These methods are not exported, so they aren’t visible to the user.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> In the case of <code>lda()</code> every class-specific method transforms its input to a standard form (a data table <code>x</code> and a vector of group assignments <code>grouping</code>, plus any of several other parameters) and passes these to <code>lda.default()</code>. This default method is the workhorse of the implementation, so it’s the only chunk of source code i’ll get into here.</p>
</div>
<div id="example-inputs-and-parameter-settings" class="section level3">
<h3>example inputs and parameter settings</h3>
<p>The code will be executed in sequence, punctuated by printouts and plots of key objects defined along the way. This requires assigning illustrative values to the function arguments. I’ll take as my example the famous diabetes data originally analyzed by <a href="https://link.springer.com/article/10.1007/BF00423145">Reaven and Miller (1979)</a>, available from <a href="https://github.com/friendly/heplots">the heplots package</a>, which consists of a BMI-type measure and the results of four glucose and insulin tests for 145 patients, along with their diagnostic grouping as non-diabetic (“normal”), subclinically (“chemical”) diabetic, and overtly diabetic.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> For our purposes, the only other required parameter is <code>tol</code>, the tolerance at which small values encountered along the matrix algebra are interpreted as zero.</p>
<pre class="r"><code>x &lt;- heplots::Diabetes[, 1:5]
grouping &lt;- heplots::Diabetes[, 6]
tol &lt;- 1e-04</code></pre>
<pre class="r"><code>set.seed(2)
s &lt;- sort(sample(nrow(x), 6))
print(x[s, ])</code></pre>
<pre><code>##     relwt glufast glutest instest sspg
## 24   0.97      90     327     192  124
## 27   1.20      98     365     145  158
## 82   1.11      93     393     490  259
## 102  1.13      92     476     433  226
## 133  0.85     330    1520      13  303
## 134  0.81     123     557     130  152</code></pre>
<pre class="r"><code>print(grouping[s])</code></pre>
<pre><code>## [1] Normal            Normal            Normal            Chemical_Diabetic
## [5] Overt_Diabetic    Overt_Diabetic   
## Levels: Normal Chemical_Diabetic Overt_Diabetic</code></pre>
</div>
<div id="data-parameters-and-summary-information" class="section level3">
<h3>data parameters and summary information</h3>
<p>The first several lines of <code>lda.default()</code> capture summary information about the data table <code>x</code> (and ensure that it is a matrix) and the group assignments <code>grouping</code>: <code>n</code> is the number of cases in <code>x</code>, <code>p</code> is the number of variables, <code>g</code> is <code>grouping</code> coerced to a factor-valued vector, <code>lev</code> is the vector of factor levels (groups), <code>counts</code> is a vector of the number of cases assigned to each group, <code>proportions</code> is a vector of their proportions of <code>n</code>, and <code>ng</code> is the number of groups.</p>
<p>It will help to keep track of some mathematical notation for these concepts in parallel: Call the data matrix <span class="math inline">\(X\in\mathbb{R}^{n\times p}\)</span>, the column vector of group assignments <span class="math inline">\(g\in[q]^{n}\)</span>, and a diagonal matrix <span class="math inline">\(N\in\mathbb{N}^{q\times q}\)</span> of group counts <span class="math inline">\(n_1,\ldots,n_q\)</span>. Note that <code>lev</code> and <code>ng</code> correspond to <span class="math inline">\([q]\)</span> and <span class="math inline">\(q\)</span>, respectively. Additionally let <span class="math inline">\(G=(\delta_{g_i,k})\in\{0,1\}^{n\times q}\)</span> denote the 0,1-matrix with <span class="math inline">\(i,k\)</span>-th entry <span class="math inline">\(1\)</span> when case <span class="math inline">\(i\)</span> is assigned to group <span class="math inline">\(k\)</span>. (I’ll use the indices <span class="math inline">\(1\leq i\leq n\)</span> for the cases, <span class="math inline">\(1\leq j\leq p\)</span> for the variables, and <span class="math inline">\(1\leq k\leq q\)</span> for the groups.)</p>
<pre class="r"><code>if(is.null(dim(x))) stop(&quot;&#39;x&#39; is not a matrix&quot;)
x &lt;- as.matrix(x)
if(any(!is.finite(x)))
  stop(&quot;infinite, NA or NaN values in &#39;x&#39;&quot;)
n &lt;- nrow(x)
p &lt;- ncol(x)
if(n != length(grouping))
  stop(&quot;nrow(x) and length(grouping) are different&quot;)
g &lt;- as.factor(grouping)
lev &lt;- lev1 &lt;- levels(g)
counts &lt;- as.vector(table(g))
# excised handling of `prior`
if(any(counts == 0L)) {
  empty &lt;- lev[counts == 0L]
  warning(sprintf(ngettext(length(empty),
                           &quot;group %s is empty&quot;,
                           &quot;groups %s are empty&quot;),
                  paste(empty, collapse = &quot; &quot;)), domain = NA)
  lev1 &lt;- lev[counts &gt; 0L]
  g &lt;- factor(g, levels = lev1)
  counts &lt;- as.vector(table(g))
}
proportions &lt;- counts/n
ng &lt;- length(proportions)
names(counts) &lt;- lev1 # `names(prior)` excised</code></pre>
<p>The steps are straightforward, with the exception of the handling of prior probabilities (<code>prior</code>), which is beyond our scope and which i’ve excised from the code. (When not provided, <code>prior</code> takes the default value <code>proportions</code>, which are explicitly defined within <code>lda.default()</code> and will be substituted for <code>prior</code> below.) If <code>grouping</code> is a factor that is missing elements of any of its levels, then these elements are removed from the analysis with a warning. I’ll also skip the lines relevant only to cross-validation (<code>CV = TRUE</code>), which include a compatibility check here and a large conditional chunk later on.</p>
<pre class="r"><code># group counts and proportions of the total:
print(cbind(counts, proportions))</code></pre>
<pre><code>##                   counts proportions
## Normal                76   0.5241379
## Chemical_Diabetic     36   0.2482759
## Overt_Diabetic        33   0.2275862</code></pre>
</div>
<div id="variable-standardization-and-the-correlation-matrix" class="section level3">
<h3>variable standardization and the correlation matrix</h3>
<p>Next, we calculate the group centroids <span class="math inline">\(\overline{X}=N^{-1}G^\top X\in\mathbb{R}^{q\times p}\)</span> and the within-group covariance matrix <span class="math inline">\(C_W=\frac{1}{n}{X_0}^\top{X_0}\)</span>, where <span class="math inline">\({X_0}=X-G\overline{X}\)</span> consists of the differences between the cases (rows of <span class="math inline">\(X\)</span>) and their corresponding group centroids. <code>lda.default()</code> stores <span class="math inline">\(\overline{X}\)</span> as <code>group.means</code> and the square roots of the diagonal entries of <span class="math inline">\(C_W\)</span>—that is, the standard deviations—as <code>f1</code>.
The conditional statement requires that at least some variances be nonzero, up to the tolerance threshold <code>tol</code>. Finally, <code>scaling</code> is initialized as a diagonal matrix <span class="math inline">\(S_0\)</span> of inverted variable standard deviations <span class="math inline">\(\frac{1}{\sigma_j}\)</span>.</p>
<pre class="r"><code>## drop attributes to avoid e.g. matrix() methods
group.means &lt;- tapply(c(x), list(rep(g, p), col(x)), mean)
f1 &lt;- sqrt(diag(var(x - group.means[g,  ])))
if(any(f1 &lt; tol)) {
  const &lt;- format((1L:p)[f1 &lt; tol])
  stop(sprintf(ngettext(length(const),
                        &quot;variable %s appears to be constant within groups&quot;,
                        &quot;variables %s appear to be constant within groups&quot;),
               paste(const, collapse = &quot; &quot;)),
       domain = NA)
}
# scale columns to unit variance before checking for collinearity
scaling &lt;- diag(1/f1, , p)</code></pre>
<p>I’ll refer to <span class="math inline">\({X_0}\)</span> as the “within-group centered data”. The group centroids are returned as the named member <code>means</code> of the ‘lda’-class object returned by <code>lda.default()</code>. The standard deviations of and correlations among (some of) the five variables are evident in their pairwise scatterplots, which here and forward fix the aspect ratio at 1:</p>
<pre class="r"><code># group centroids
print(group.means)</code></pre>
<pre><code>##                           1         2         3        4        5
## Normal            0.9372368  91.18421  349.9737 172.6447 114.0000
## Chemical_Diabetic 1.0558333  99.30556  493.9444 288.0000 208.9722
## Overt_Diabetic    0.9839394 217.66667 1043.7576 106.0000 318.8788</code></pre>
<pre class="r"><code># within-group centered data
pairs(x - group.means[g, ], asp = 1, pch = 19, cex = .5)</code></pre>
<p><img src="/post/2019-08-02-lda_files/figure-html/unnamed-chunk-6-1.png" width="768" /></p>
<pre class="r"><code># within-group standard deviations
print(f1)</code></pre>
<pre><code>##       relwt     glufast     glutest     instest        sspg 
##   0.1195937  36.8753901 150.7536138 102.2913665  65.8125752</code></pre>
<pre class="r"><code># inverses of within-group standard deviations
scaling0 &lt;- scaling
print(scaling0)</code></pre>
<pre><code>##          [,1]       [,2]       [,3]        [,4]       [,5]
## [1,] 8.361643 0.00000000 0.00000000 0.000000000 0.00000000
## [2,] 0.000000 0.02711836 0.00000000 0.000000000 0.00000000
## [3,] 0.000000 0.00000000 0.00663334 0.000000000 0.00000000
## [4,] 0.000000 0.00000000 0.00000000 0.009775996 0.00000000
## [5,] 0.000000 0.00000000 0.00000000 0.000000000 0.01519466</code></pre>
<p>The object <code>scaling</code> will be redefined twice as the function proceeds, each time by multiplication on the right, concluding with the member named <code>scaling</code> of the ‘lda’ object. We’ll denote these redefinitions <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> and store the three definitions as <code>scaling0</code>, <code>scaling1</code>, and <code>scaling2</code> for unambiguous illustrations. These matrices act on the columns of <span class="math inline">\({X_0}\)</span>, which induces a two-sided action on <span class="math inline">\(C_W\)</span>: <span class="math inline">\(\frac{1}{n}({X_0}S)^\top({X_0}S)=S^\top C_WS\)</span>. The effect of <span class="math inline">\(S_0\)</span> is to standardize the variables, so that the covariance matrix of the transformed data <span class="math inline">\({X_0}S_0\)</span> is the within-group correlation matrix <span class="math inline">\(R_W\)</span> of <span class="math inline">\({X_0}\)</span>. The effect is also evident from a comparison of the pairwise scatterplots:</p>
<pre class="r"><code># within-group correlation matrix
print(cor(x - group.means[g, ]))</code></pre>
<pre><code>##               relwt     glufast    glutest    instest      sspg
## relwt    1.00000000 -0.09623213 -0.1607280  0.1070926 0.3926726
## glufast -0.09623213  1.00000000  0.9264021 -0.2513436 0.3692349
## glutest -0.16072796  0.92640213  1.0000000 -0.2494804 0.3633879
## instest  0.10709258 -0.25134359 -0.2494804  1.0000000 0.2145095
## sspg     0.39267262  0.36923495  0.3633879  0.2145095 1.0000000</code></pre>
<pre class="r"><code># within-group covariance matrix after standardization
print(cov((x - group.means[g, ]) %*% scaling))</code></pre>
<pre><code>##             [,1]        [,2]       [,3]       [,4]      [,5]
## [1,]  1.00000000 -0.09623213 -0.1607280  0.1070926 0.3926726
## [2,] -0.09623213  1.00000000  0.9264021 -0.2513436 0.3692349
## [3,] -0.16072796  0.92640213  1.0000000 -0.2494804 0.3633879
## [4,]  0.10709258 -0.25134359 -0.2494804  1.0000000 0.2145095
## [5,]  0.39267262  0.36923495  0.3633879  0.2145095 1.0000000</code></pre>
<pre class="r"><code># within-group centered data after standardization
pairs((x - group.means[g, ]) %*% scaling, asp = 1, pch = 19, cex = .5)</code></pre>
<p><img src="/post/2019-08-02-lda_files/figure-html/unnamed-chunk-7-1.png" width="768" /></p>
</div>
<div id="variable-sphering-and-the-mahalanobis-distance" class="section level3">
<h3>variable sphering and the Mahalanobis distance</h3>
<p>The next step in the procedure is the signature transformation of LDA. Whereas <span class="math inline">\(S_0\)</span> removes the scales of the variables, leaving them each with unit variance but preserving their correlations, the role of <span class="math inline">\(S_1\)</span> is to additionally remove these correlations. Viewing the set of cases within each group as a cloud of <span class="math inline">\(n_k\)</span> points in <span class="math inline">\(\mathbb{R}^p\)</span>, <span class="math inline">\(S_0\)</span> stretches or compresses each cloud <em>along its coordinate axes</em>, while <span class="math inline">\(S_1\)</span> will stretch or compress it <em>along its principal components</em>. The resulting aggregate point cloud will have within-group covariance <span class="math inline">\(I_n\)</span> (the identity matrix), indicating unit variance and no correlation among its variables; according to these summary statistics, then, it is rotationally symmetric. Accordingly, the column action of <span class="math inline">\(S_1\)</span> is called a <em>sphering transformation</em>, while the distances among the transformed points are called their <em>Mahalanobis distances</em> after <a href="https://en.wikipedia.org/wiki/Prasanta_Chandra_Mahalanobis">their originator</a>. See <a href="https://stats.stackexchange.com/a/62147/68743">this SO answer by whuber</a> for a helpful illustration of the transformation. Its importance is illustrated in Chapter 11 of <a href="https://www.fbbva.es/microsite/multivariate-statistics/biplots.html">Greenacre’s <em>Biplots in Practice</em></a> (p. 114).</p>
<p><span class="math inline">\(S_1\)</span> is calculated differently for different choices of <code>method</code> (see the documentation <code>?MASS::lda</code>). For simplicity, the code chunk below follows the <code>"mle"</code> method.
As hinted in the previous paragraph, the process of obtaining a sphering transformation is closely related to principal components analysis. One can be calculated via eigendecomposition of the standardized within-group covariance matrix or, equivalently, via singular value decomposition of the standardized within-group centered data <span class="math inline">\(\hat{X}=X_0 S_0\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> <code>lda.default()</code> performs SVD on <span class="math inline">\(\hat{X}\)</span>, scaled by <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span> so that the scatter matrix is <span class="math inline">\(R_W\)</span>:</p>
<pre class="r"><code>fac &lt;- 1/n # excise conditional case `method == &quot;moment&quot;`
X &lt;- sqrt(fac) * (x - group.means[g,  ]) %*% scaling
X.s &lt;- svd(X, nu = 0L)
rank &lt;- sum(X.s$d &gt; tol)
if(rank == 0L) stop(&quot;rank = 0: variables are numerically constant&quot;)
if(rank &lt; p) warning(&quot;variables are collinear&quot;)
scaling &lt;- scaling %*% X.s$v[, 1L:rank] %*% diag(1/X.s$d[1L:rank],,rank)</code></pre>
<p>Taking the SVD to be <span class="math inline">\(\frac{1}{\sqrt{n}}\hat{X}=\hat{U}\hat{D}\hat{V}^\top\)</span>, the sphering matrix is <span class="math inline">\(S_1=S_0\hat{V}{\hat{D}}^{-1}\)</span>.
The alternative calculation is <span class="math inline">\(S_1=S_0\hat{V}{\hat{\Lambda}}^{-1/2}\)</span>, where <span class="math inline">\(\hat{C}=\hat{V}\hat{\Lambda}{\hat{V}}^\top\)</span> is the eigendecomposition of the covariance matrix of <span class="math inline">\(\hat{X}\)</span> so that <span class="math inline">\(\hat{V}\)</span> is the same and <span class="math inline">\(\hat{\Lambda}={\hat{D}}^{1/2}\)</span>. (<span class="math inline">\({\hat{C}}^{-1/2}=\hat{V}{\hat{\Lambda}}^{-1/2}{\hat{V}}^\top\)</span> is called the symmetric square root of <span class="math inline">\(\hat{C}\)</span>.)
Note that the signs of the columns are arbitrary, as in PCA. The difference in absolute values is due to <code>stats::cov()</code> treating <span class="math inline">\(\hat{X}\)</span> as a sample rather than a population, dividing the scatter matrix <span class="math inline">\(\hat{X}^\top \hat{X}\)</span> by <span class="math inline">\(\frac{1}{n-1}\)</span> rather than by <span class="math inline">\(\frac{1}{n}\)</span>.</p>
<pre class="r"><code># standardized within-group centered data
Xhat &lt;- (x - group.means[g,  ]) %*% scaling0
# eigendecomposition of covariance
E &lt;- eigen(cov(Xhat))
# sphering matrix (alternative calculation)
scaling0 %*% E$vectors %*% diag(1/sqrt(E$values))</code></pre>
<pre><code>##              [,1]          [,2]          [,3]         [,4]         [,5]
## [1,] -0.259027622  4.3010514864  5.4856218279 -6.620668927 -2.079154922
## [2,]  0.011804714 -0.0010406454 -0.0019199260 -0.013949375  0.070663381
## [3,]  0.002888749 -0.0004202234 -0.0008316344 -0.002406702 -0.017886747
## [4,] -0.001393182  0.0037523667 -0.0081405500 -0.005925590 -0.000197695
## [5,]  0.003470018  0.0076467225 -0.0008602899  0.018031665  0.001957544</code></pre>
<pre class="r"><code># sphering matrix
scaling1 &lt;- scaling
print(scaling1)</code></pre>
<pre><code>##              [,1]         [,2]          [,3]         [,4]          [,5]
## [1,]  0.259925467  4.315959855 -5.5046361720 -6.643617588  2.0863617201
## [2,] -0.011845632 -0.001044253  0.0019265809 -0.013997727 -0.0709083156
## [3,] -0.002898762 -0.000421680  0.0008345171 -0.002415044  0.0179487460
## [4,]  0.001398012  0.003765373  0.0081687669 -0.005946129  0.0001983803
## [5,] -0.003482045  0.007673228  0.0008632718  0.018094166 -0.0019643291</code></pre>
<p>The sphered within-group centered data now have identity within-group covariance matrix, up to a reasonable tolerance. The sphered coordinates are true to their name, as along each pair of variable coordinates the point cloud exhibits no elliptical tendencies.
While <code>rank</code> is 5 (<span class="math inline">\(=p\)</span>) at this stage, indicating that variation is detected in each of the variables, the final computational step will determine the rank of the LDA itself in terms of the transformed group centroids.</p>
<pre class="r"><code># within-group covariance matrix after sphering
print(cov((x - group.means[g, ]) %*% scaling))</code></pre>
<pre><code>##               [,1]         [,2]          [,3]          [,4]          [,5]
## [1,]  1.006944e+00 3.762724e-16 -5.781689e-16  5.015188e-16  1.113855e-15
## [2,]  3.762724e-16 1.006944e+00  1.892763e-15  4.976970e-16  5.560143e-16
## [3,] -5.781689e-16 1.892763e-15  1.006944e+00 -9.853124e-16 -1.180559e-15
## [4,]  5.015188e-16 4.976970e-16 -9.853124e-16  1.006944e+00  1.095132e-15
## [5,]  1.113855e-15 5.560143e-16 -1.180559e-15  1.095132e-15  1.006944e+00</code></pre>
<pre class="r"><code># within-group centered data after sphering
pairs((x - group.means[g, ]) %*% scaling, asp = 1, pch = 19, cex = .5)</code></pre>
<p><img src="/post/2019-08-02-lda_files/figure-html/unnamed-chunk-10-1.png" width="768" /></p>
<pre class="r"><code># number of variables that contribute to the sphered data
print(rank)</code></pre>
<pre><code>## [1] 5</code></pre>
</div>
<div id="sphering-transformed-group-centroids-and-discriminant-coefficients" class="section level3">
<h3>sphering-transformed group centroids and discriminant coefficients</h3>
<p>The original problem of LDA was to eigendecompose <span class="math inline">\({C_W}^{-1}C_B\)</span>, where <span class="math inline">\(C_B=\frac{1}{q}Y^\top Y\)</span> is the between-groups covariance matrix derived from the centered group centroids <span class="math inline">\(Y=G\overline{X}-\mathbb{1}_{n\times 1}\overline{x}\in\mathbb{R}^{n\times p}\)</span> using the data centroid <span class="math inline">\(\overline{x}=\frac{1}{n}\mathbb{1}_{1\times n}X\in\mathbb{R}^{1\times p}\)</span> (and duplicated according to the group counts).
<code>lda.default()</code> relies on an equivalent calculation <span class="math inline">\(C_B=\frac{1}{q}\overline{Y}^\top\overline{Y}\)</span>, using <span class="math inline">\(\overline{Y}=N(\overline{X}-\mathbb{1}_{q\times 1}\overline{x})\)</span>, which explicitly weights the centered group centroids by the group counts and produces the same scatter matrix as <span class="math inline">\(Y\)</span>.
Under the transformed coordinates <span class="math inline">\({X_0}S_1\)</span>, the matrix quotient to be eigendecomposed becomes <span class="math display">\[\textstyle({S_1}^\top C_WS_1)^{-1}({S_1}^\top C_BS_1)=I_p{S_1}^\top C_BS_1=\frac{1}{q}(YS_1)^\top(YS_1)=\frac{1}{q}(\overline{Y}S_1)^\top(\overline{Y}S_1),\]</span>
the between-groups covariance calculated from the sphering-transformed centered group centroids.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>
This is the first calculation using <span class="math inline">\(C_B\)</span>, which is how <code>lda.default()</code> can wait until here to calculate the data centroid <code>xbar</code>.</p>
<pre class="r"><code>xbar &lt;- colSums(proportions %*% group.means) # sub `proportions` for `prior`
fac &lt;- 1/(ng - 1) # excise conditional case `method == &quot;mle&quot;`
X &lt;- sqrt((n * proportions)*fac) * # sub `proportions` for `prior`
  scale(group.means, center = xbar, scale = FALSE) %*% scaling
X.s &lt;- svd(X, nu = 0L)
rank &lt;- sum(X.s$d &gt; tol * X.s$d[1L])
if(rank == 0L) stop(&quot;group means are numerically identical&quot;)
scaling &lt;- scaling %*% X.s$v[, 1L:rank]
# excise conditional case `is.null(dimnames(x))`
dimnames(scaling) &lt;- list(colnames(x), paste(&quot;LD&quot;, 1L:rank, sep = &quot;&quot;))
dimnames(group.means)[[2L]] &lt;- colnames(x)</code></pre>
<p>The rank now indicates the number of dimensions in the SVD of the sphering-transformed group centroids, which can be no more than <span class="math inline">\(q-1\)</span>.
The final reassignment of <code>scaling</code> gives <span class="math inline">\(S_2\)</span>, the raw discriminant coefficients that express the discriminant coordinates of the centroids (and of the original data) as linear combinations of the centered variable coordinates. In practice, these discriminant coordinates are returned by the <code>predict()</code> method for ‘lda’ objects, <code>MASS:::predict.lda()</code>, and this is demonstrated to conclude the post:</p>
<pre class="r"><code># number of dimensions in the sphering-transformed group centroid SVD
print(rank)</code></pre>
<pre><code>## [1] 2</code></pre>
<pre class="r"><code># discriminant coefficients
scaling2 &lt;- scaling
print(scaling2)</code></pre>
<pre><code>##                   LD1          LD2
## relwt    1.3767523932 -3.823906854
## glufast -0.0340023755  0.037018266
## glutest  0.0127085491 -0.007166541
## instest -0.0001032987 -0.006238295
## sspg     0.0042877747  0.001145987</code></pre>
<pre class="r"><code># discriminant coordinates of centered group centroids
print((group.means - matrix(1, ng, 1) %*% t(xbar)) %*% scaling2)</code></pre>
<pre><code>##                          LD1        LD2
## Normal            -1.7683547  0.4043199
## Chemical_Diabetic  0.3437412 -1.3910995
## Overt_Diabetic     3.6975842  0.5864022</code></pre>
<pre class="r"><code># as recovered using `predict.lda()`
fit &lt;- MASS::lda(group ~ ., heplots::Diabetes)
print(predict(fit, as.data.frame(fit$means))$x)</code></pre>
<pre><code>##                          LD1        LD2
## Normal            -1.7499658  0.4001154
## Chemical_Diabetic  0.3401666 -1.3766336
## Overt_Diabetic     3.6591334  0.5803043</code></pre>
<p>(The slight discrepancy between the manually calculated coordinates and those returned by <code>predict()</code> is left as an exercise for the reader.)</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>If you use ordination and ggplot2, not necessarily together (yet), i’d be grateful for your feedback, too!<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>My exposition here assumes the covariances were calculated from a population rather than a sample perspective, but this introduces discrepancies in a few places.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>To see the source code for an object included in but not exported by a package, use three colons instead of two, e.g. <code>MASS:::lda.default</code>.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>I’ve been searching for a history of the 1970s change from categorizing diabetes as “clinical” versus “overt” to the present-day categories of “type 1” and “type 2”. Recommended reading is very welcome.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>These are equivalent because the matrix factors <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> of an SVD of any matrix <span class="math inline">\(Z\)</span> are the eigenvector matrices of <span class="math inline">\(ZZ^\top\)</span> and of <span class="math inline">\(Z^\top Z\)</span>, respectively.<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>These are not “sphered” centered group centroids because the sphering transformation is specific to the within-group centered data.<a href="#fnref6" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded>


<p>For over a year (intermittently, not cumulatively) i’ve been developing an R package, <a href="https://github.com/corybrunson/ordr">ordr</a>, for incorporating ordination techniques into a tidyverse workflow. (Credit where it’s due: Emily Paul, a summer intern from Avon High School, helped extend the package functionality and has since used it in other projects and provided critical feedback on its design.) This week, i finally deployed <a href="https://corybrunson.github.io/ordr/">a pkgdown website</a> and tweeted a meek solicitation for feedback.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Interestingly, the one component that held me back, that i put off time and time again before finally grinding out a solution over the past few weeks, was the component that originally complemented PCA in <a href="https://github.com/vqv/ggbiplot">Vince Vu’s original and inspirational ggbiplot package</a>: Accessor methods for the MASS package implementation of linear discriminant analysis.</p>
<p>A big part of the reason for this procrastination was that <a href="https://github.com/cran/MASS/blob/master/R/lda.R">the source code of <code>MASS::lda()</code></a> looks nothing like the tidy definition of LDA found in textbooks. (I have on my shelf my mom’s copies of <a href="https://books.google.com/books/about/Multivariate_Analysis.html?id=HHNnBDaNsuUC">Tatsuoka’s <em>Multivariate Analysis</em></a> and of <a href="https://books.google.com/books/about/Psychometric_theory.html?id=WE59AAAAMAAJ">Nunnally’s <em>Psychometric Theory</em></a>, but there are several similar introductions online, for example <a href="https://sebastianraschka.com/Articles/2014_python_lda.html">Raschka’s</a>.) In the standard presentation, LDA boils down to an eigendecomposition of the quotient <span class="math inline">\({C_W}^{-1}C_B\)</span> of the between- by the within-group covariance matrices. (This is equivalent, up to a scale factor, to the eigendecomposition of the same quotient of the corresponding scatter matrices <span class="math inline">\(S_\bullet=nC_\bullet\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>)
But rather than build up to a single eigendecomposition, <code>MASS::lda()</code> relies on sequential compositions of a scaling matrix with eigenvector and other matrices that are difficult to follow by reading the (minimally documented) code.</p>
<p>Meanwhile, several discussions of LDA on StackOverflow provide useful insights into the output of <code>MASS::lda()</code>, but the more prolific respondents tend not to use R, and the numerical examples derive from implementations in other languages.
The MASS book, <em>Modern Applied Statistics with S</em> by Venables and Ripley (<a href="https://tinyurl.com/yyhpyu2d">PDF</a>), details their mathematical formula in section 12.1 but does not discuss their implementation.
So, while it’s still fresh in my mind, i want to offer what i hope will be an evergreen dissection of <code>MASS::lda()</code>. In a future post i’ll survey different ways an LDA might be summarized in a biplot and how i tweaked Venables and Ripley’s function to make these options available in ordr.</p>
<div id="tldr" class="section level3">
<h3>tl;dr</h3>
<p><code>MASS::lda()</code> gradually and conditionally composes a variable transformation (a matrix that acts on the columns of the data matrix) to simplify the covariance quotient, then uses singular value decomposition to obtain its eigendecomposition. Rather than returning both the variable transformation and the quotient eigendecomposition, it returns their product, a matrix of discriminant coefficients that transforms the data and/or group centroids to their discriminant coordinates (scores). Consequently, the variable loadings cannot be recovered from the output.</p>
</div>
<div id="class-methods" class="section level3">
<h3>class methods</h3>
<p>For those (like me until recently) unfamiliar with <a href="http://adv-r.had.co.nz/S3.html">the S3 object system</a>, <a href="https://github.com/cran/MASS/blob/master/R/lda.R">the <code>MASS::lda()</code> source code</a> may look strange. Essentially, and henceforth assuming the MASS package has been attached so that <code>MASS::</code> is unnecessary, the generic function <code>lda()</code> performs <em>method dispatch</em> by checking the class(es) of its primary input <code>x</code>, then sending <code>x</code> (and any other inputs) to one of several class-specific methods <code>lda.&lt;class&gt;()</code>, or else to <code>lda.default()</code>. These methods are not exported, so they aren’t visible to the user.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> In the case of <code>lda()</code> every class-specific method transforms its input to a standard form (a data table <code>x</code> and a vector of group assignments <code>grouping</code>, plus any of several other parameters) and passes these to <code>lda.default()</code>. This default method is the workhorse of the implementation, so it’s the only chunk of source code i’ll get into here.</p>
</div>
<div id="example-inputs-and-parameter-settings" class="section level3">
<h3>example inputs and parameter settings</h3>
<p>The code will be executed in sequence, punctuated by printouts and plots of key objects defined along the way. This requires assigning illustrative values to the function arguments. I’ll take as my example the famous diabetes data originally analyzed by <a href="https://link.springer.com/article/10.1007/BF00423145">Reaven and Miller (1979)</a>, available from <a href="https://github.com/friendly/heplots">the heplots package</a>, which consists of a BMI-type measure and the results of four glucose and insulin tests for 145 patients, along with their diagnostic grouping as non-diabetic (“normal”), subclinically (“chemical”) diabetic, and overtly diabetic.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> For our purposes, the only other required parameter is <code>tol</code>, the tolerance at which small values encountered along the matrix algebra are interpreted as zero.</p>
<pre class="r"><code>x &lt;- heplots::Diabetes[, 1:5]
grouping &lt;- heplots::Diabetes[, 6]
tol &lt;- 1e-04</code></pre>
<pre class="r"><code>set.seed(2)
s &lt;- sort(sample(nrow(x), 6))
print(x[s, ])</code></pre>
<pre><code>##     relwt glufast glutest instest sspg
## 24   0.97      90     327     192  124
## 27   1.20      98     365     145  158
## 82   1.11      93     393     490  259
## 102  1.13      92     476     433  226
## 133  0.85     330    1520      13  303
## 134  0.81     123     557     130  152</code></pre>
<pre class="r"><code>print(grouping[s])</code></pre>
<pre><code>## [1] Normal            Normal            Normal            Chemical_Diabetic
## [5] Overt_Diabetic    Overt_Diabetic   
## Levels: Normal Chemical_Diabetic Overt_Diabetic</code></pre>
</div>
<div id="data-parameters-and-summary-information" class="section level3">
<h3>data parameters and summary information</h3>
<p>The first several lines of <code>lda.default()</code> capture summary information about the data table <code>x</code> (and ensure that it is a matrix) and the group assignments <code>grouping</code>: <code>n</code> is the number of cases in <code>x</code>, <code>p</code> is the number of variables, <code>g</code> is <code>grouping</code> coerced to a factor-valued vector, <code>lev</code> is the vector of factor levels (groups), <code>counts</code> is a vector of the number of cases assigned to each group, <code>proportions</code> is a vector of their proportions of <code>n</code>, and <code>ng</code> is the number of groups.</p>
<p>It will help to keep track of some mathematical notation for these concepts in parallel: Call the data matrix <span class="math inline">\(X\in\mathbb{R}^{n\times p}\)</span>, the column vector of group assignments <span class="math inline">\(g\in[q]^{n}\)</span>, and a diagonal matrix <span class="math inline">\(N\in\mathbb{N}^{q\times q}\)</span> of group counts <span class="math inline">\(n_1,\ldots,n_q\)</span>. Note that <code>lev</code> and <code>ng</code> correspond to <span class="math inline">\([q]\)</span> and <span class="math inline">\(q\)</span>, respectively. Additionally let <span class="math inline">\(G=(\delta_{g_i,k})\in\{0,1\}^{n\times q}\)</span> denote the 0,1-matrix with <span class="math inline">\(i,k\)</span>-th entry <span class="math inline">\(1\)</span> when case <span class="math inline">\(i\)</span> is assigned to group <span class="math inline">\(k\)</span>. (I’ll use the indices <span class="math inline">\(1\leq i\leq n\)</span> for the cases, <span class="math inline">\(1\leq j\leq p\)</span> for the variables, and <span class="math inline">\(1\leq k\leq q\)</span> for the groups.)</p>
<pre class="r"><code>if(is.null(dim(x))) stop(&quot;&#39;x&#39; is not a matrix&quot;)
x &lt;- as.matrix(x)
if(any(!is.finite(x)))
  stop(&quot;infinite, NA or NaN values in &#39;x&#39;&quot;)
n &lt;- nrow(x)
p &lt;- ncol(x)
if(n != length(grouping))
  stop(&quot;nrow(x) and length(grouping) are different&quot;)
g &lt;- as.factor(grouping)
lev &lt;- lev1 &lt;- levels(g)
counts &lt;- as.vector(table(g))
# excised handling of `prior`
if(any(counts == 0L)) {
  empty &lt;- lev[counts == 0L]
  warning(sprintf(ngettext(length(empty),
                           &quot;group %s is empty&quot;,
                           &quot;groups %s are empty&quot;),
                  paste(empty, collapse = &quot; &quot;)), domain = NA)
  lev1 &lt;- lev[counts &gt; 0L]
  g &lt;- factor(g, levels = lev1)
  counts &lt;- as.vector(table(g))
}
proportions &lt;- counts/n
ng &lt;- length(proportions)
names(counts) &lt;- lev1 # `names(prior)` excised</code></pre>
<p>The steps are straightforward, with the exception of the handling of prior probabilities (<code>prior</code>), which is beyond our scope and which i’ve excised from the code. (When not provided, <code>prior</code> takes the default value <code>proportions</code>, which are explicitly defined within <code>lda.default()</code> and will be substituted for <code>prior</code> below.) If <code>grouping</code> is a factor that is missing elements of any of its levels, then these elements are removed from the analysis with a warning. I’ll also skip the lines relevant only to cross-validation (<code>CV = TRUE</code>), which include a compatibility check here and a large conditional chunk later on.</p>
<pre class="r"><code># group counts and proportions of the total:
print(cbind(counts, proportions))</code></pre>
<pre><code>##                   counts proportions
## Normal                76   0.5241379
## Chemical_Diabetic     36   0.2482759
## Overt_Diabetic        33   0.2275862</code></pre>
</div>
<div id="variable-standardization-and-the-correlation-matrix" class="section level3">
<h3>variable standardization and the correlation matrix</h3>
<p>Next, we calculate the group centroids <span class="math inline">\(\overline{X}=N^{-1}G^\top X\in\mathbb{R}^{q\times p}\)</span> and the within-group covariance matrix <span class="math inline">\(C_W=\frac{1}{n}{X_0}^\top{X_0}\)</span>, where <span class="math inline">\({X_0}=X-G\overline{X}\)</span> consists of the differences between the cases (rows of <span class="math inline">\(X\)</span>) and their corresponding group centroids. <code>lda.default()</code> stores <span class="math inline">\(\overline{X}\)</span> as <code>group.means</code> and the square roots of the diagonal entries of <span class="math inline">\(C_W\)</span>—that is, the standard deviations—as <code>f1</code>.
The conditional statement requires that at least some variances be nonzero, up to the tolerance threshold <code>tol</code>. Finally, <code>scaling</code> is initialized as a diagonal matrix <span class="math inline">\(S_0\)</span> of inverted variable standard deviations <span class="math inline">\(\frac{1}{\sigma_j}\)</span>.</p>
<pre class="r"><code>## drop attributes to avoid e.g. matrix() methods
group.means &lt;- tapply(c(x), list(rep(g, p), col(x)), mean)
f1 &lt;- sqrt(diag(var(x - group.means[g,  ])))
if(any(f1 &lt; tol)) {
  const &lt;- format((1L:p)[f1 &lt; tol])
  stop(sprintf(ngettext(length(const),
                        &quot;variable %s appears to be constant within groups&quot;,
                        &quot;variables %s appear to be constant within groups&quot;),
               paste(const, collapse = &quot; &quot;)),
       domain = NA)
}
# scale columns to unit variance before checking for collinearity
scaling &lt;- diag(1/f1, , p)</code></pre>
<p>I’ll refer to <span class="math inline">\({X_0}\)</span> as the “within-group centered data”. The group centroids are returned as the named member <code>means</code> of the ‘lda’-class object returned by <code>lda.default()</code>. The standard deviations of and correlations among (some of) the five variables are evident in their pairwise scatterplots, which here and forward fix the aspect ratio at 1:</p>
<pre class="r"><code># group centroids
print(group.means)</code></pre>
<pre><code>##                           1         2         3        4        5
## Normal            0.9372368  91.18421  349.9737 172.6447 114.0000
## Chemical_Diabetic 1.0558333  99.30556  493.9444 288.0000 208.9722
## Overt_Diabetic    0.9839394 217.66667 1043.7576 106.0000 318.8788</code></pre>
<pre class="r"><code># within-group centered data
pairs(x - group.means[g, ], asp = 1, pch = 19, cex = .5)</code></pre>
<p><img src="/post/2019-08-02-lda_files/figure-html/unnamed-chunk-6-1.png" width="768" /></p>
<pre class="r"><code># within-group standard deviations
print(f1)</code></pre>
<pre><code>##       relwt     glufast     glutest     instest        sspg 
##   0.1195937  36.8753901 150.7536138 102.2913665  65.8125752</code></pre>
<pre class="r"><code># inverses of within-group standard deviations
scaling0 &lt;- scaling
print(scaling0)</code></pre>
<pre><code>##          [,1]       [,2]       [,3]        [,4]       [,5]
## [1,] 8.361643 0.00000000 0.00000000 0.000000000 0.00000000
## [2,] 0.000000 0.02711836 0.00000000 0.000000000 0.00000000
## [3,] 0.000000 0.00000000 0.00663334 0.000000000 0.00000000
## [4,] 0.000000 0.00000000 0.00000000 0.009775996 0.00000000
## [5,] 0.000000 0.00000000 0.00000000 0.000000000 0.01519466</code></pre>
<p>The object <code>scaling</code> will be redefined twice as the function proceeds, each time by multiplication on the right, concluding with the member named <code>scaling</code> of the ‘lda’ object. We’ll denote these redefinitions <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> and store the three definitions as <code>scaling0</code>, <code>scaling1</code>, and <code>scaling2</code> for unambiguous illustrations. These matrices act on the columns of <span class="math inline">\({X_0}\)</span>, which induces a two-sided action on <span class="math inline">\(C_W\)</span>: <span class="math inline">\(\frac{1}{n}({X_0}S)^\top({X_0}S)=S^\top C_WS\)</span>. The effect of <span class="math inline">\(S_0\)</span> is to standardize the variables, so that the covariance matrix of the transformed data <span class="math inline">\({X_0}S_0\)</span> is the within-group correlation matrix <span class="math inline">\(R_W\)</span> of <span class="math inline">\({X_0}\)</span>. The effect is also evident from a comparison of the pairwise scatterplots:</p>
<pre class="r"><code># within-group correlation matrix
print(cor(x - group.means[g, ]))</code></pre>
<pre><code>##               relwt     glufast    glutest    instest      sspg
## relwt    1.00000000 -0.09623213 -0.1607280  0.1070926 0.3926726
## glufast -0.09623213  1.00000000  0.9264021 -0.2513436 0.3692349
## glutest -0.16072796  0.92640213  1.0000000 -0.2494804 0.3633879
## instest  0.10709258 -0.25134359 -0.2494804  1.0000000 0.2145095
## sspg     0.39267262  0.36923495  0.3633879  0.2145095 1.0000000</code></pre>
<pre class="r"><code># within-group covariance matrix after standardization
print(cov((x - group.means[g, ]) %*% scaling))</code></pre>
<pre><code>##             [,1]        [,2]       [,3]       [,4]      [,5]
## [1,]  1.00000000 -0.09623213 -0.1607280  0.1070926 0.3926726
## [2,] -0.09623213  1.00000000  0.9264021 -0.2513436 0.3692349
## [3,] -0.16072796  0.92640213  1.0000000 -0.2494804 0.3633879
## [4,]  0.10709258 -0.25134359 -0.2494804  1.0000000 0.2145095
## [5,]  0.39267262  0.36923495  0.3633879  0.2145095 1.0000000</code></pre>
<pre class="r"><code># within-group centered data after standardization
pairs((x - group.means[g, ]) %*% scaling, asp = 1, pch = 19, cex = .5)</code></pre>
<p><img src="/post/2019-08-02-lda_files/figure-html/unnamed-chunk-7-1.png" width="768" /></p>
</div>
<div id="variable-sphering-and-the-mahalanobis-distance" class="section level3">
<h3>variable sphering and the Mahalanobis distance</h3>
<p>The next step in the procedure is the signature transformation of LDA. Whereas <span class="math inline">\(S_0\)</span> removes the scales of the variables, leaving them each with unit variance but preserving their correlations, the role of <span class="math inline">\(S_1\)</span> is to additionally remove these correlations. Viewing the set of cases within each group as a cloud of <span class="math inline">\(n_k\)</span> points in <span class="math inline">\(\mathbb{R}^p\)</span>, <span class="math inline">\(S_0\)</span> stretches or compresses each cloud <em>along its coordinate axes</em>, while <span class="math inline">\(S_1\)</span> will stretch or compress it <em>along its principal components</em>. The resulting aggregate point cloud will have within-group covariance <span class="math inline">\(I_n\)</span> (the identity matrix), indicating unit variance and no correlation among its variables; according to these summary statistics, then, it is rotationally symmetric. Accordingly, the column action of <span class="math inline">\(S_1\)</span> is called a <em>sphering transformation</em>, while the distances among the transformed points are called their <em>Mahalanobis distances</em> after <a href="https://en.wikipedia.org/wiki/Prasanta_Chandra_Mahalanobis">their originator</a>. See <a href="https://stats.stackexchange.com/a/62147/68743">this SO answer by whuber</a> for a helpful illustration of the transformation. Its importance is illustrated in Chapter 11 of <a href="https://www.fbbva.es/microsite/multivariate-statistics/biplots.html">Greenacre’s <em>Biplots in Practice</em></a> (p. 114).</p>
<p><span class="math inline">\(S_1\)</span> is calculated differently for different choices of <code>method</code> (see the documentation <code>?MASS::lda</code>). For simplicity, the code chunk below follows the <code>"mle"</code> method.
As hinted in the previous paragraph, the process of obtaining a sphering transformation is closely related to principal components analysis. One can be calculated via eigendecomposition of the standardized within-group covariance matrix or, equivalently, via singular value decomposition of the standardized within-group centered data <span class="math inline">\(\hat{X}=X_0 S_0\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> <code>lda.default()</code> performs SVD on <span class="math inline">\(\hat{X}\)</span>, scaled by <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span> so that the scatter matrix is <span class="math inline">\(R_W\)</span>:</p>
<pre class="r"><code>fac &lt;- 1/n # excise conditional case `method == &quot;moment&quot;`
X &lt;- sqrt(fac) * (x - group.means[g,  ]) %*% scaling
X.s &lt;- svd(X, nu = 0L)
rank &lt;- sum(X.s$d &gt; tol)
if(rank == 0L) stop(&quot;rank = 0: variables are numerically constant&quot;)
if(rank &lt; p) warning(&quot;variables are collinear&quot;)
scaling &lt;- scaling %*% X.s$v[, 1L:rank] %*% diag(1/X.s$d[1L:rank],,rank)</code></pre>
<p>Taking the SVD to be <span class="math inline">\(\frac{1}{\sqrt{n}}\hat{X}=\hat{U}\hat{D}\hat{V}^\top\)</span>, the sphering matrix is <span class="math inline">\(S_1=S_0\hat{V}{\hat{D}}^{-1}\)</span>.
The alternative calculation is <span class="math inline">\(S_1=S_0\hat{V}{\hat{\Lambda}}^{-1/2}\)</span>, where <span class="math inline">\(\hat{C}=\hat{V}\hat{\Lambda}{\hat{V}}^\top\)</span> is the eigendecomposition of the covariance matrix of <span class="math inline">\(\hat{X}\)</span> so that <span class="math inline">\(\hat{V}\)</span> is the same and <span class="math inline">\(\hat{\Lambda}={\hat{D}}^{1/2}\)</span>. (<span class="math inline">\({\hat{C}}^{-1/2}=\hat{V}{\hat{\Lambda}}^{-1/2}{\hat{V}}^\top\)</span> is called the symmetric square root of <span class="math inline">\(\hat{C}\)</span>.)
Note that the signs of the columns are arbitrary, as in PCA. The difference in absolute values is due to <code>stats::cov()</code> treating <span class="math inline">\(\hat{X}\)</span> as a sample rather than a population, dividing the scatter matrix <span class="math inline">\(\hat{X}^\top \hat{X}\)</span> by <span class="math inline">\(\frac{1}{n-1}\)</span> rather than by <span class="math inline">\(\frac{1}{n}\)</span>.</p>
<pre class="r"><code># standardized within-group centered data
Xhat &lt;- (x - group.means[g,  ]) %*% scaling0
# eigendecomposition of covariance
E &lt;- eigen(cov(Xhat))
# sphering matrix (alternative calculation)
scaling0 %*% E$vectors %*% diag(1/sqrt(E$values))</code></pre>
<pre><code>##              [,1]          [,2]          [,3]         [,4]         [,5]
## [1,] -0.259027622  4.3010514864  5.4856218279 -6.620668927 -2.079154922
## [2,]  0.011804714 -0.0010406454 -0.0019199260 -0.013949375  0.070663381
## [3,]  0.002888749 -0.0004202234 -0.0008316344 -0.002406702 -0.017886747
## [4,] -0.001393182  0.0037523667 -0.0081405500 -0.005925590 -0.000197695
## [5,]  0.003470018  0.0076467225 -0.0008602899  0.018031665  0.001957544</code></pre>
<pre class="r"><code># sphering matrix
scaling1 &lt;- scaling
print(scaling1)</code></pre>
<pre><code>##              [,1]         [,2]          [,3]         [,4]          [,5]
## [1,]  0.259925467  4.315959855 -5.5046361720 -6.643617588  2.0863617201
## [2,] -0.011845632 -0.001044253  0.0019265809 -0.013997727 -0.0709083156
## [3,] -0.002898762 -0.000421680  0.0008345171 -0.002415044  0.0179487460
## [4,]  0.001398012  0.003765373  0.0081687669 -0.005946129  0.0001983803
## [5,] -0.003482045  0.007673228  0.0008632718  0.018094166 -0.0019643291</code></pre>
<p>The sphered within-group centered data now have identity within-group covariance matrix, up to a reasonable tolerance. The sphered coordinates are true to their name, as along each pair of variable coordinates the point cloud exhibits no elliptical tendencies.
While <code>rank</code> is 5 (<span class="math inline">\(=p\)</span>) at this stage, indicating that variation is detected in each of the variables, the final computational step will determine the rank of the LDA itself in terms of the transformed group centroids.</p>
<pre class="r"><code># within-group covariance matrix after sphering
print(cov((x - group.means[g, ]) %*% scaling))</code></pre>
<pre><code>##               [,1]         [,2]          [,3]          [,4]          [,5]
## [1,]  1.006944e+00 3.762724e-16 -5.781689e-16  5.015188e-16  1.113855e-15
## [2,]  3.762724e-16 1.006944e+00  1.892763e-15  4.976970e-16  5.560143e-16
## [3,] -5.781689e-16 1.892763e-15  1.006944e+00 -9.853124e-16 -1.180559e-15
## [4,]  5.015188e-16 4.976970e-16 -9.853124e-16  1.006944e+00  1.095132e-15
## [5,]  1.113855e-15 5.560143e-16 -1.180559e-15  1.095132e-15  1.006944e+00</code></pre>
<pre class="r"><code># within-group centered data after sphering
pairs((x - group.means[g, ]) %*% scaling, asp = 1, pch = 19, cex = .5)</code></pre>
<p><img src="/post/2019-08-02-lda_files/figure-html/unnamed-chunk-10-1.png" width="768" /></p>
<pre class="r"><code># number of variables that contribute to the sphered data
print(rank)</code></pre>
<pre><code>## [1] 5</code></pre>
</div>
<div id="sphering-transformed-group-centroids-and-discriminant-coefficients" class="section level3">
<h3>sphering-transformed group centroids and discriminant coefficients</h3>
<p>The original problem of LDA was to eigendecompose <span class="math inline">\({C_W}^{-1}C_B\)</span>, where <span class="math inline">\(C_B=\frac{1}{q}Y^\top Y\)</span> is the between-groups covariance matrix derived from the centered group centroids <span class="math inline">\(Y=G\overline{X}-\mathbb{1}_{n\times 1}\overline{x}\in\mathbb{R}^{n\times p}\)</span> using the data centroid <span class="math inline">\(\overline{x}=\frac{1}{n}\mathbb{1}_{1\times n}X\in\mathbb{R}^{1\times p}\)</span> (and duplicated according to the group counts).
<code>lda.default()</code> relies on an equivalent calculation <span class="math inline">\(C_B=\frac{1}{q}\overline{Y}^\top\overline{Y}\)</span>, using <span class="math inline">\(\overline{Y}=N(\overline{X}-\mathbb{1}_{q\times 1}\overline{x})\)</span>, which explicitly weights the centered group centroids by the group counts and produces the same scatter matrix as <span class="math inline">\(Y\)</span>.
Under the transformed coordinates <span class="math inline">\({X_0}S_1\)</span>, the matrix quotient to be eigendecomposed becomes <span class="math display">\[\textstyle({S_1}^\top C_WS_1)^{-1}({S_1}^\top C_BS_1)=I_p{S_1}^\top C_BS_1=\frac{1}{q}(YS_1)^\top(YS_1)=\frac{1}{q}(\overline{Y}S_1)^\top(\overline{Y}S_1),\]</span>
the between-groups covariance calculated from the sphering-transformed centered group centroids.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>
This is the first calculation using <span class="math inline">\(C_B\)</span>, which is how <code>lda.default()</code> can wait until here to calculate the data centroid <code>xbar</code>.</p>
<pre class="r"><code>xbar &lt;- colSums(proportions %*% group.means) # sub `proportions` for `prior`
fac &lt;- 1/(ng - 1) # excise conditional case `method == &quot;mle&quot;`
X &lt;- sqrt((n * proportions)*fac) * # sub `proportions` for `prior`
  scale(group.means, center = xbar, scale = FALSE) %*% scaling
X.s &lt;- svd(X, nu = 0L)
rank &lt;- sum(X.s$d &gt; tol * X.s$d[1L])
if(rank == 0L) stop(&quot;group means are numerically identical&quot;)
scaling &lt;- scaling %*% X.s$v[, 1L:rank]
# excise conditional case `is.null(dimnames(x))`
dimnames(scaling) &lt;- list(colnames(x), paste(&quot;LD&quot;, 1L:rank, sep = &quot;&quot;))
dimnames(group.means)[[2L]] &lt;- colnames(x)</code></pre>
<p>The rank now indicates the number of dimensions in the SVD of the sphering-transformed group centroids, which can be no more than <span class="math inline">\(q-1\)</span>.
The final reassignment of <code>scaling</code> gives <span class="math inline">\(S_2\)</span>, the raw discriminant coefficients that express the discriminant coordinates of the centroids (and of the original data) as linear combinations of the centered variable coordinates. In practice, these discriminant coordinates are returned by the <code>predict()</code> method for ‘lda’ objects, <code>MASS:::predict.lda()</code>, and this is demonstrated to conclude the post:</p>
<pre class="r"><code># number of dimensions in the sphering-transformed group centroid SVD
print(rank)</code></pre>
<pre><code>## [1] 2</code></pre>
<pre class="r"><code># discriminant coefficients
scaling2 &lt;- scaling
print(scaling2)</code></pre>
<pre><code>##                   LD1          LD2
## relwt    1.3767523932 -3.823906854
## glufast -0.0340023755  0.037018266
## glutest  0.0127085491 -0.007166541
## instest -0.0001032987 -0.006238295
## sspg     0.0042877747  0.001145987</code></pre>
<pre class="r"><code># discriminant coordinates of centered group centroids
print((group.means - matrix(1, ng, 1) %*% t(xbar)) %*% scaling2)</code></pre>
<pre><code>##                          LD1        LD2
## Normal            -1.7683547  0.4043199
## Chemical_Diabetic  0.3437412 -1.3910995
## Overt_Diabetic     3.6975842  0.5864022</code></pre>
<pre class="r"><code># as recovered using `predict.lda()`
fit &lt;- MASS::lda(group ~ ., heplots::Diabetes)
print(predict(fit, as.data.frame(fit$means))$x)</code></pre>
<pre><code>##                          LD1        LD2
## Normal            -1.7499658  0.4001154
## Chemical_Diabetic  0.3401666 -1.3766336
## Overt_Diabetic     3.6591334  0.5803043</code></pre>
<p>(The slight discrepancy between the manually calculated coordinates and those returned by <code>predict()</code> is left as an exercise for the reader.)</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>If you use ordination and ggplot2, not necessarily together (yet), i’d be grateful for your feedback, too!<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>My exposition here assumes the covariances were calculated from a population rather than a sample perspective, but this introduces discrepancies in a few places.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>To see the source code for an object included in but not exported by a package, use three colons instead of two, e.g. <code>MASS:::lda.default</code>.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>I’ve been searching for a history of the 1970s change from categorizing diabetes as “clinical” versus “overt” to the present-day categories of “type 1” and “type 2”. Recommended reading is very welcome.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>These are equivalent because the matrix factors <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> of an SVD of any matrix <span class="math inline">\(Z\)</span> are the eigenvector matrices of <span class="math inline">\(ZZ^\top\)</span> and of <span class="math inline">\(Z^\top Z\)</span>, respectively.<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>These are not “sphered” centered group centroids because the sphering transformation is specific to the within-group centered data.<a href="#fnref6" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>About</title>
      <link>http://corybrunson.github.io/about/</link>
      <pubDate>2019 Mar 04 (Mon), 00:00:00 +0000</pubDate>
      
      <guid>http://corybrunson.github.io/about/</guid>
      <description>


<div id="who-i-am-here" class="section level2">
<h2>who i am (here)</h2>
<p>I’m Cory Brunson<sup>he/him/his</sup>, and this is a lightweight space for me to practice expressing my outlook on matters at least peripherally professional. I completed a PhD in mathematics in 2013, and i’m now a postdoctoral fellow training myself in network analysis, data science, and health informatics. I’m an enthusiast for the extended <a href="https://www.tidyverse.org/">tidyverse</a> library, and i’m exploring new techniques and applications of <a href="https://arxiv.org/abs/1710.04019">topological data analysis</a>.</p>
<p>My under-developed <a href="https://jasoncorybrunson.netlify.com/">professional website</a> uses the highly-developed <a href="https://github.com/gcushen/hugo-academic"><code>hugo-academic</code></a> theme.
You can also find me at the <a href="https://health.uconn.edu/quantitative-medicine/">Center for Quantitative Medicine</a> or at the links in the footer.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
</div>
<div id="what-and-how-i-blog" class="section level2">
<h2>what and how i blog</h2>
<p>My schedule and content may evolve, but my goal at the outset is to post fortnightly or so on, or in the vicinity of, four categories: professional development, research progress, methodological technique, and personal curiosity. I want to use this space to express ideas that are messy and incomplete; i’ll strive both to read others as charitably as reasonable and to write with the expectation that others will do the same.</p>
<p>I haven’t incorporated comments, but feel free to send me feedback via email or social media!</p>
</div>
<div id="just-for-fun" class="section level2">
<h2>just for fun</h2>
<ul>
<li>My Erdős number is <a href="https://www.math.vt.edu/people/brown/doc/fibo_number.pdf">3</a>.</li>
<li>My Bacon number is <a href="http://www.imdb.com/title/tt1696494/">3</a>.</li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Thanks to Zack Guo for the <a href="https://github.com/gizak/nofancy"><code>nofancy</code> Hugo theme</a>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded>


<div id="who-i-am-here" class="section level2">
<h2>who i am (here)</h2>
<p>I’m Cory Brunson<sup>he/him/his</sup>, and this is a lightweight space for me to practice expressing my outlook on matters at least peripherally professional. I completed a PhD in mathematics in 2013, and i’m now a postdoctoral fellow training myself in network analysis, data science, and health informatics. I’m an enthusiast for the extended <a href="https://www.tidyverse.org/">tidyverse</a> library, and i’m exploring new techniques and applications of <a href="https://arxiv.org/abs/1710.04019">topological data analysis</a>.</p>
<p>My under-developed <a href="https://jasoncorybrunson.netlify.com/">professional website</a> uses the highly-developed <a href="https://github.com/gcushen/hugo-academic"><code>hugo-academic</code></a> theme.
You can also find me at the <a href="https://health.uconn.edu/quantitative-medicine/">Center for Quantitative Medicine</a> or at the links in the footer.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
</div>
<div id="what-and-how-i-blog" class="section level2">
<h2>what and how i blog</h2>
<p>My schedule and content may evolve, but my goal at the outset is to post fortnightly or so on, or in the vicinity of, four categories: professional development, research progress, methodological technique, and personal curiosity. I want to use this space to express ideas that are messy and incomplete; i’ll strive both to read others as charitably as reasonable and to write with the expectation that others will do the same.</p>
<p>I haven’t incorporated comments, but feel free to send me feedback via email or social media!</p>
</div>
<div id="just-for-fun" class="section level2">
<h2>just for fun</h2>
<ul>
<li>My Erdős number is <a href="https://www.math.vt.edu/people/brown/doc/fibo_number.pdf">3</a>.</li>
<li>My Bacon number is <a href="http://www.imdb.com/title/tt1696494/">3</a>.</li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Thanks to Zack Guo for the <a href="https://github.com/gizak/nofancy"><code>nofancy</code> Hugo theme</a>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
  </channel>
</rss>
