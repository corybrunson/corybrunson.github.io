<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>murmuring in the background</title>
    <link>/</link>
    <description>Recent content on murmuring in the background</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>2020 Apr 17 (Fri), 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>how to calculate aesthetics in a ggplot2 extension</title>
      <link>/2020/04/17/calculate-aesthetics/</link>
      <pubDate>2020 Apr 17 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/17/calculate-aesthetics/</guid>
      <description><div id="how-to-use-computed-variables" class="section level2">
<h2>how to use computed variables</h2>
<p>One of the many subtle features of ggplot2 is the ability to pass variables to aesthetics that are not present in the data but rather are computed internally by a statistical transformation (stat). For users, <a href="https://ggplot2.tidyverse.org/reference/stat.html">the documentation for this feature</a> illustrates the use of <code>stat(&lt;variable&gt;)</code> (previously <code>..&lt;variable&gt;..</code>, since upgraded to <code>after_stat(&lt;variable&gt;)</code>) in an aesthetic specification.</p>
<p>Some stats do this by default. For example, the count stat sends its computed <code>count</code> to both coordinate aesthetics by default. Because it requires <em>exactly</em> one of them to be specified, only the other receives the count:</p>
<pre class="r"><code>print(StatCount$default_aes)</code></pre>
<pre><code>## Aesthetic mapping: 
## * `x`      -&gt; `after_stat(count)`
## * `y`      -&gt; `after_stat(count)`
## * `weight` -&gt; 1</code></pre>
<pre class="r"><code>print(StatCount$required_aes)</code></pre>
<pre><code>## [1] &quot;x|y&quot;</code></pre>
<p>This is how the count stat supports its companion graphical element, the bar geom. This geom needs both the categorical variable from the data and the count variable computed by the stat in order to produce a <em>(frequency) bar plot</em>. The code below, which tallies the cars in the <code>mpg</code> data set by classification, makes some of this implicit control explicit:</p>
<pre class="r"><code>table(mpg$class)</code></pre>
<pre><code>## 
##    2seater    compact    midsize    minivan     pickup subcompact        suv 
##          5         47         41         11         33         35         62</code></pre>
<pre class="r"><code>ggplot(mpg) +
  stat_count(aes(x = class, y = after_stat(count)))</code></pre>
<p><img src="/post/2020-04-03-calculate-aesthetics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>More than calling single variables, <code>after_stat()</code> can also perform and return calculations involving these variables. For example, to produce a <em>relative (frequency) bar plot</em> of the classified cars, the <code>y</code> variable needs not the raw counts but what proportion they make up of the total Note the <code>y</code> axis range in this revised plot:</p>
<pre class="r"><code>ggplot(mpg) +
  stat_count(aes(x = class, y = after_stat(count / sum(count)))) +
  scale_y_continuous(labels = scales::percent)</code></pre>
<p><img src="/post/2020-04-03-calculate-aesthetics_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>More illustrations can be found in the aforelinked documentation, which are reasonably intuitive from a user’s perspective. Though it’s not immediately evident where <code>after_stat()</code> locates these computed variables and what the limits are to its ability to perform calculations on them. The documentation <code>help(after_stat)</code> thoroughly tracks the processing of an aesthetic from start to stat to scale, but again with users in mind.</p>
</div>
<div id="how-to-make-computed-variables" class="section level2">
<h2>how to make computed variables</h2>
<p>Another exceptional feature of ggplot2 is its extensibility. Users with specialized plotting needs can, with limited exposure to the package internals, write <a href="https://exts.ggplot2.tidyverse.org/gallery/">stats and geoms that produce new types of plots</a>. Because they are extensions, rather than standalone packages, they benefit from the grammatical rigor of ggplot2 and often combine well with existing stats and geoms.</p>
<p>From a developer’s perspective, especially someoene like myself with limited low-level programming experience, computed variables can appear mysterious.
Yet, they are perhaps the single easiest feature to include in a ggplot2 extension.</p>
<p>To illustrate, consider this simplified custom stat from <a href="https://ggplot2.tidyverse.org/articles/extending-ggplot2.html">the vignette on extending ggplot2</a>:</p>
<pre class="r"><code># a custom ggproto stat to fit a linear model to data
StatLm &lt;- ggproto(&quot;StatLm&quot;, Stat, 
  required_aes = c(&quot;x&quot;, &quot;y&quot;),
  
  compute_group = function(data, scales, params, n = 100, formula = y ~ x) {
    rng &lt;- range(data$x, na.rm = TRUE)
    grid &lt;- data.frame(x = seq(rng[1], rng[2], length = n))
    
    mod &lt;- lm(formula, data = data)
    grid$y &lt;- predict(mod, newdata = grid)
    
    grid
  }
)
# a corresponding stat layer
stat_lm &lt;- function(mapping = NULL, data = NULL, geom = &quot;line&quot;,
                    position = &quot;identity&quot;, na.rm = FALSE, show.legend = NA, 
                    inherit.aes = TRUE, n = 50, formula = y ~ x, 
                    ...) {
  layer(
    stat = StatLm, data = data, mapping = mapping, geom = geom, 
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(n = n, formula = formula, na.rm = na.rm, ...)
  )
}
# an illustration of the stat
ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm()</code></pre>
<p><img src="/post/2020-04-03-calculate-aesthetics_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The computation step of this linear model stat returns a data frame <code>grid</code> with two columns: a regular sequence of <code>x</code> values spanning the range of engine displacement volumes in the data set, whose number are determined by the parameter <code>n</code>; and the predicted highway speed <code>y</code> at each, according to the internally-fitted model <code>mod</code>. Notice that the data returned by the stat is differently sized than the data passed to it:</p>
<pre class="r"><code>dim(mpg)</code></pre>
<pre><code>## [1] 234  11</code></pre>
<pre class="r"><code>head(mpg)</code></pre>
<pre><code>## # A tibble: 6 x 11
##   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class 
##   &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 
## 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…
## 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…
## 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…
## 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…
## 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…
## 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…</code></pre>
<pre class="r"><code>data &lt;- transform(mpg, x = displ, y = hwy)[, c(&quot;x&quot;, &quot;y&quot;)]
dim(StatLm$compute_group(data))</code></pre>
<pre><code>## [1] 100   2</code></pre>
<pre class="r"><code>head(StatLm$compute_group(data))</code></pre>
<pre><code>##          x        y
## 1 1.600000 30.04871
## 2 1.654545 29.85613
## 3 1.709091 29.66355
## 4 1.763636 29.47098
## 5 1.818182 29.27840
## 6 1.872727 29.08582</code></pre>
<p>The predictions computed by the stat are estimates of conditional means (under a set of assumptions outside the scope of this example), and it’s often useful for a plot to encode the uncertainty of those estimates graphically. First, the uncertainty must be computed by the stat, as below by including a standard error calculation at the <code>predict()</code> step:</p>
<pre class="r"><code># the linear model ggproto stat, with a computed variable for standard error
StatLm &lt;- ggproto(&quot;StatLm&quot;, Stat, 
  required_aes = c(&quot;x&quot;, &quot;y&quot;),
  
  compute_group = function(data, scales, params, n = 100, formula = y ~ x) {
    rng &lt;- range(data$x, na.rm = TRUE)
    grid &lt;- data.frame(x = seq(rng[1], rng[2], length = n))
    
    mod &lt;- lm(formula, data = data)
    pred &lt;- predict(mod, newdata = grid, se.fit = TRUE)
    grid$y &lt;- pred$fit
    grid$yse &lt;- pred$se.fit
    
    grid
  }
)</code></pre>
<p>In addition to <code>x</code> and <code>y</code>, the data frame computed by the stat now includes a <code>yse</code> column, containing the standard errors of the predicted means contained in <code>y</code>:</p>
<pre class="r"><code>head(StatLm$compute_group(data))</code></pre>
<pre><code>##          x        y       yse
## 1 1.600000 30.04871 0.4420916
## 2 1.654545 29.85613 0.4333956
## 3 1.709091 29.66355 0.4247865
## 4 1.763636 29.47098 0.4162698
## 5 1.818182 29.27840 0.4078513
## 6 1.872727 29.08582 0.3995371</code></pre>
<p>Paired with the ribbon geom, this stat can now produce a 95% confidence band for the mean highway speeds predicted for the full range of engine displacement volumes<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>:</p>
<pre class="r"><code>ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm(geom = &quot;ribbon&quot;, alpha = .2,
          aes(ymin = after_stat(y - 2 * yse), ymax = after_stat(y + 2 * yse))) +
  stat_lm()</code></pre>
<p><img src="/post/2020-04-03-calculate-aesthetics_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>As a concluding caveat, i haven’t dug far enough into the ggplot2 source code to know exactly how the expressions fed to <code>after_stat()</code> are evaluated. In principle, if a stat returns the data frame <code>ret</code>, then <code>after_stat(&lt;expression&gt;)</code> is evaluated like <code>with(ret, &lt;expression&gt;)</code>. In particular, objects in the global environment are recognized:</p>
<pre class="r"><code>z &lt;- 3
ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm(geom = &quot;ribbon&quot;, alpha = .2,
          aes(ymin = after_stat(y - z * yse), ymax = after_stat(y + z * yse))) +
  stat_lm()</code></pre>
<p><img src="/post/2020-04-03-calculate-aesthetics_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Finally, to document new computed variables, the natural thing to do is mimic the documentation of those in the main package—for example, <a href="https://github.com/tidyverse/ggplot2/blob/master/R/stat-count.r"><code>help(stat_count)</code></a>. Here is some minimal documentation for the standard error variable:</p>
<pre class="r"><code>#&#39; @section Computed variables:
#&#39; \describe{
#&#39;   \item{yse}{standard error of predicted means}
#&#39; }</code></pre>
<p>In a pinch, the computed variables may be gleaned from the source code for the relevant compute method of the stat—if the internals are concise and tidy enough. Since the count stat performs group-wise tallies, the method to inspect is <code>StatCount$compute_group()</code>:</p>
<pre class="r"><code>print(StatCount$compute_group)</code></pre>
<pre><code>## &lt;ggproto method&gt;
##   &lt;Wrapper function&gt;
##     function (...) 
## f(..., self = self)
## 
##   &lt;Inner function (f)&gt;
##     function (self, data, scales, width = NULL, flipped_aes = FALSE) 
## {
##     data &lt;- flip_data(data, flipped_aes)
##     x &lt;- data$x
##     weight &lt;- data$weight %||% rep(1, length(x))
##     width &lt;- width %||% (resolution(x) * 0.9)
##     count &lt;- as.numeric(tapply(weight, x, sum, na.rm = TRUE))
##     count[is.na(count)] &lt;- 0
##     bars &lt;- new_data_frame(list(count = count, prop = count/sum(abs(count)), 
##         x = sort(unique(x)), width = width, flipped_aes = flipped_aes), 
##         n = length(count))
##     flip_data(bars, flipped_aes)
## }</code></pre>
<p>Indeed, <code>count</code> is not the only variable computed by the stat. The code is specialized, but it’s clear that some additional variables—<code>prop</code>, <code>x</code>, and <code>width</code>—are computed as well. The last two are meant for the paired geom; they <em>could</em> be invoked using <code>after_stat()</code>, but this is not their intended role, and they are not documented among the computed stats. The documentation serves dual purposes: enable intended use, and avert unintended use.</p>
</div>
<div id="conclusion" class="section level2">
<h2>conclusion</h2>
<p>To sum up the topic for ggplot2 extension developers:</p>
<ul>
<li>A <em>computed variable</em> is just a column of the data frame returned by <code>Stat*$compute_*()</code>.</li>
<li>Any expression involving such computed variables can be passed as a <em>calculated aesthetic</em> via <code>aes(&lt;aes&gt; = after_stat(&lt;expr&gt;))</code>.</li>
<li>Users should be able to learn about computed variables in a specific <strong>Computed variables</strong> section of the documentation for such a stat.</li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note that the <code>yse</code> are <em>not</em> standard errors for the estimates; hence, the confidence bands do not represent expected prediction errors. This confusion seems to inflame tempers, if top search results are any indication.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><div id="how-to-use-computed-variables" class="section level2">
<h2>how to use computed variables</h2>
<p>One of the many subtle features of ggplot2 is the ability to pass variables to aesthetics that are not present in the data but rather are computed internally by a statistical transformation (stat). For users, <a href="https://ggplot2.tidyverse.org/reference/stat.html">the documentation for this feature</a> illustrates the use of <code>stat(&lt;variable&gt;)</code> (previously <code>..&lt;variable&gt;..</code>, since upgraded to <code>after_stat(&lt;variable&gt;)</code>) in an aesthetic specification.</p>
<p>Some stats do this by default. For example, the count stat sends its computed <code>count</code> to both coordinate aesthetics by default. Because it requires <em>exactly</em> one of them to be specified, only the other receives the count:</p>
<pre class="r"><code>print(StatCount$default_aes)</code></pre>
<pre><code>## Aesthetic mapping: 
## * `x`      -&gt; `after_stat(count)`
## * `y`      -&gt; `after_stat(count)`
## * `weight` -&gt; 1</code></pre>
<pre class="r"><code>print(StatCount$required_aes)</code></pre>
<pre><code>## [1] &quot;x|y&quot;</code></pre>
<p>This is how the count stat supports its companion graphical element, the bar geom. This geom needs both the categorical variable from the data and the count variable computed by the stat in order to produce a <em>(frequency) bar plot</em>. The code below, which tallies the cars in the <code>mpg</code> data set by classification, makes some of this implicit control explicit:</p>
<pre class="r"><code>table(mpg$class)</code></pre>
<pre><code>## 
##    2seater    compact    midsize    minivan     pickup subcompact        suv 
##          5         47         41         11         33         35         62</code></pre>
<pre class="r"><code>ggplot(mpg) +
  stat_count(aes(x = class, y = after_stat(count)))</code></pre>
<p><img src="/post/2020-04-03-calculate-aesthetics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>More than calling single variables, <code>after_stat()</code> can also perform and return calculations involving these variables. For example, to produce a <em>relative (frequency) bar plot</em> of the classified cars, the <code>y</code> variable needs not the raw counts but what proportion they make up of the total Note the <code>y</code> axis range in this revised plot:</p>
<pre class="r"><code>ggplot(mpg) +
  stat_count(aes(x = class, y = after_stat(count / sum(count)))) +
  scale_y_continuous(labels = scales::percent)</code></pre>
<p><img src="/post/2020-04-03-calculate-aesthetics_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>More illustrations can be found in the aforelinked documentation, which are reasonably intuitive from a user’s perspective. Though it’s not immediately evident where <code>after_stat()</code> locates these computed variables and what the limits are to its ability to perform calculations on them. The documentation <code>help(after_stat)</code> thoroughly tracks the processing of an aesthetic from start to stat to scale, but again with users in mind.</p>
</div>
<div id="how-to-make-computed-variables" class="section level2">
<h2>how to make computed variables</h2>
<p>Another exceptional feature of ggplot2 is its extensibility. Users with specialized plotting needs can, with limited exposure to the package internals, write <a href="https://exts.ggplot2.tidyverse.org/gallery/">stats and geoms that produce new types of plots</a>. Because they are extensions, rather than standalone packages, they benefit from the grammatical rigor of ggplot2 and often combine well with existing stats and geoms.</p>
<p>From a developer’s perspective, especially someoene like myself with limited low-level programming experience, computed variables can appear mysterious.
Yet, they are perhaps the single easiest feature to include in a ggplot2 extension.</p>
<p>To illustrate, consider this simplified custom stat from <a href="https://ggplot2.tidyverse.org/articles/extending-ggplot2.html">the vignette on extending ggplot2</a>:</p>
<pre class="r"><code># a custom ggproto stat to fit a linear model to data
StatLm &lt;- ggproto(&quot;StatLm&quot;, Stat, 
  required_aes = c(&quot;x&quot;, &quot;y&quot;),
  
  compute_group = function(data, scales, params, n = 100, formula = y ~ x) {
    rng &lt;- range(data$x, na.rm = TRUE)
    grid &lt;- data.frame(x = seq(rng[1], rng[2], length = n))
    
    mod &lt;- lm(formula, data = data)
    grid$y &lt;- predict(mod, newdata = grid)
    
    grid
  }
)
# a corresponding stat layer
stat_lm &lt;- function(mapping = NULL, data = NULL, geom = &quot;line&quot;,
                    position = &quot;identity&quot;, na.rm = FALSE, show.legend = NA, 
                    inherit.aes = TRUE, n = 50, formula = y ~ x, 
                    ...) {
  layer(
    stat = StatLm, data = data, mapping = mapping, geom = geom, 
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(n = n, formula = formula, na.rm = na.rm, ...)
  )
}
# an illustration of the stat
ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm()</code></pre>
<p><img src="/post/2020-04-03-calculate-aesthetics_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The computation step of this linear model stat returns a data frame <code>grid</code> with two columns: a regular sequence of <code>x</code> values spanning the range of engine displacement volumes in the data set, whose number are determined by the parameter <code>n</code>; and the predicted highway speed <code>y</code> at each, according to the internally-fitted model <code>mod</code>. Notice that the data returned by the stat is differently sized than the data passed to it:</p>
<pre class="r"><code>dim(mpg)</code></pre>
<pre><code>## [1] 234  11</code></pre>
<pre class="r"><code>head(mpg)</code></pre>
<pre><code>## # A tibble: 6 x 11
##   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class 
##   &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 
## 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…
## 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…
## 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…
## 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…
## 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…
## 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…</code></pre>
<pre class="r"><code>data &lt;- transform(mpg, x = displ, y = hwy)[, c(&quot;x&quot;, &quot;y&quot;)]
dim(StatLm$compute_group(data))</code></pre>
<pre><code>## [1] 100   2</code></pre>
<pre class="r"><code>head(StatLm$compute_group(data))</code></pre>
<pre><code>##          x        y
## 1 1.600000 30.04871
## 2 1.654545 29.85613
## 3 1.709091 29.66355
## 4 1.763636 29.47098
## 5 1.818182 29.27840
## 6 1.872727 29.08582</code></pre>
<p>The predictions computed by the stat are estimates of conditional means (under a set of assumptions outside the scope of this example), and it’s often useful for a plot to encode the uncertainty of those estimates graphically. First, the uncertainty must be computed by the stat, as below by including a standard error calculation at the <code>predict()</code> step:</p>
<pre class="r"><code># the linear model ggproto stat, with a computed variable for standard error
StatLm &lt;- ggproto(&quot;StatLm&quot;, Stat, 
  required_aes = c(&quot;x&quot;, &quot;y&quot;),
  
  compute_group = function(data, scales, params, n = 100, formula = y ~ x) {
    rng &lt;- range(data$x, na.rm = TRUE)
    grid &lt;- data.frame(x = seq(rng[1], rng[2], length = n))
    
    mod &lt;- lm(formula, data = data)
    pred &lt;- predict(mod, newdata = grid, se.fit = TRUE)
    grid$y &lt;- pred$fit
    grid$yse &lt;- pred$se.fit
    
    grid
  }
)</code></pre>
<p>In addition to <code>x</code> and <code>y</code>, the data frame computed by the stat now includes a <code>yse</code> column, containing the standard errors of the predicted means contained in <code>y</code>:</p>
<pre class="r"><code>head(StatLm$compute_group(data))</code></pre>
<pre><code>##          x        y       yse
## 1 1.600000 30.04871 0.4420916
## 2 1.654545 29.85613 0.4333956
## 3 1.709091 29.66355 0.4247865
## 4 1.763636 29.47098 0.4162698
## 5 1.818182 29.27840 0.4078513
## 6 1.872727 29.08582 0.3995371</code></pre>
<p>Paired with the ribbon geom, this stat can now produce a 95% confidence band for the mean highway speeds predicted for the full range of engine displacement volumes<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>:</p>
<pre class="r"><code>ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm(geom = &quot;ribbon&quot;, alpha = .2,
          aes(ymin = after_stat(y - 2 * yse), ymax = after_stat(y + 2 * yse))) +
  stat_lm()</code></pre>
<p><img src="/post/2020-04-03-calculate-aesthetics_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>As a concluding caveat, i haven’t dug far enough into the ggplot2 source code to know exactly how the expressions fed to <code>after_stat()</code> are evaluated. In principle, if a stat returns the data frame <code>ret</code>, then <code>after_stat(&lt;expression&gt;)</code> is evaluated like <code>with(ret, &lt;expression&gt;)</code>. In particular, objects in the global environment are recognized:</p>
<pre class="r"><code>z &lt;- 3
ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm(geom = &quot;ribbon&quot;, alpha = .2,
          aes(ymin = after_stat(y - z * yse), ymax = after_stat(y + z * yse))) +
  stat_lm()</code></pre>
<p><img src="/post/2020-04-03-calculate-aesthetics_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Finally, to document new computed variables, the natural thing to do is mimic the documentation of those in the main package—for example, <a href="https://github.com/tidyverse/ggplot2/blob/master/R/stat-count.r"><code>help(stat_count)</code></a>. Here is some minimal documentation for the standard error variable:</p>
<pre class="r"><code>#&#39; @section Computed variables:
#&#39; \describe{
#&#39;   \item{yse}{standard error of predicted means}
#&#39; }</code></pre>
<p>In a pinch, the computed variables may be gleaned from the source code for the relevant compute method of the stat—if the internals are concise and tidy enough. Since the count stat performs group-wise tallies, the method to inspect is <code>StatCount$compute_group()</code>:</p>
<pre class="r"><code>print(StatCount$compute_group)</code></pre>
<pre><code>## &lt;ggproto method&gt;
##   &lt;Wrapper function&gt;
##     function (...) 
## f(..., self = self)
## 
##   &lt;Inner function (f)&gt;
##     function (self, data, scales, width = NULL, flipped_aes = FALSE) 
## {
##     data &lt;- flip_data(data, flipped_aes)
##     x &lt;- data$x
##     weight &lt;- data$weight %||% rep(1, length(x))
##     width &lt;- width %||% (resolution(x) * 0.9)
##     count &lt;- as.numeric(tapply(weight, x, sum, na.rm = TRUE))
##     count[is.na(count)] &lt;- 0
##     bars &lt;- new_data_frame(list(count = count, prop = count/sum(abs(count)), 
##         x = sort(unique(x)), width = width, flipped_aes = flipped_aes), 
##         n = length(count))
##     flip_data(bars, flipped_aes)
## }</code></pre>
<p>Indeed, <code>count</code> is not the only variable computed by the stat. The code is specialized, but it’s clear that some additional variables—<code>prop</code>, <code>x</code>, and <code>width</code>—are computed as well. The last two are meant for the paired geom; they <em>could</em> be invoked using <code>after_stat()</code>, but this is not their intended role, and they are not documented among the computed stats. The documentation serves dual purposes: enable intended use, and avert unintended use.</p>
</div>
<div id="conclusion" class="section level2">
<h2>conclusion</h2>
<p>To sum up the topic for ggplot2 extension developers:</p>
<ul>
<li>A <em>computed variable</em> is just a column of the data frame returned by <code>Stat*$compute_*()</code>.</li>
<li>Any expression involving such computed variables can be passed as a <em>calculated aesthetic</em> via <code>aes(&lt;aes&gt; = after_stat(&lt;expr&gt;))</code>.</li>
<li>Users should be able to learn about computed variables in a specific <strong>Computed variables</strong> section of the documentation for such a stat.</li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note that the <code>yse</code> are <em>not</em> standard errors for the estimates; hence, the confidence bands do not represent expected prediction errors. This confusion seems to inflame tempers, if top search results are any indication.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>the dimensions of (abstracted) polyamory literature</title>
      <link>/2019/10/25/goodreads-polyamory/</link>
      <pubDate>2019 Oct 25 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/25/goodreads-polyamory/</guid>
      <description><p>I’ve gained immensely from reading the handful of non-fiction books on polyamory i’ve made time to, including Dossie Easton and Janet Hardy’s <em>The Ethical Slut</em> and Franklin Veaux and Eve Rickert’s <em>More Than Two</em>. While i’ve identified as poly since i discovered the term in grad school, i exhibit at least my share of emotional immaturity, and in addition to actual experience building healthy relationships i know i’d benefit from funneling a bit more of this literature into my reading queue. So, with a small group of friends (which has reduced for the time being to someone i’m dating plus myself), i recently started a poly/kink book club.</p>
<p>From the start, i intended to slide fiction, fantasy/scifi, memoir, anthropology, history, and any other genres i could discover onto our shelf, to accrue a well-rounded appreciation for what was available. Though this of course put me to wondering how i could even learn the contours of the poly literature, in order to ensure that i sampled widely from it! Fortunes of timing provided me with three excellent resources:</p>
<ul>
<li><a href="https://www.goodreads.com/user/show/57466005">a Goodreads account</a>;</li>
<li><a href="https://maraaverick.rbind.io/2017/08/goodreads-part-i-rgoodreads/">a blog tutorial</a> by Mara Averick on using R packages to scrape and crunch Goodreads data; and</li>
<li><a href="https://www.goodreads.com/list/tag/polyamory">a book</a> by Julia Silge and David Robinson on doing text mining in tidyverse style.</li>
</ul>
<p>The tutorial and <a href="https://maraaverick.rbind.io/2017/10/goodreads-part-2/">its sequel</a> will get you up to speed; i’ll outline my web-scraping script and focus mostly on the analysis.</p>
<div id="scrape" class="section level2">
<h2>scrape</h2>
<p>It would be, let’s say, impractical to manually search out books with explicitly poly content or themes, and even then i’d likely miss a bunch whose descriptions don’t let on too clearly. Fortunately, Goodreads allows users both to curate thematic lists <em>and</em> to tag their lists with keywords! <a href="https://www.goodreads.com/list/tag/polyamory">Here is the collection of lists tagged “polyamory”</a>, numbering in the dozens. Helpfully, the lists span genres, including fiction, young adult, memoirs, specific configurations like triads, and space opera (natch). Less helpfully, they also range more broadly in topic and theme, for example exotica, sex positivity, and love. On the whole, though, the tag seems like a good candidate for a one-off look.</p>
<p>There are other relevant tags, of course, like <a href="https://www.goodreads.com/list/tag/open-relationships">“open-relationships”</a>. But the others i’ve found turn out to be far less sensitive and often less specific. For example, <a href="https://www.goodreads.com/list/tag/nonmonogamy">“nonmonogamy”</a> turns up two relevant lists but <a href="https://www.goodreads.com/list/tag/non-monogamy">“non monogamy”</a> turns up a third, and <a href="https://www.goodreads.com/list/tag/swinging">“swinging”</a> yields four lists, of which one is a conspicuous false positive. For simplicity, i’ll stick with the “polyamory” tag. This is something i can revisit if the results aren’t facially relevant.</p>
<p>My code (in <a href="../supplementary/">the supplementary folder</a>) scrapes first the list URLs from the meta-list (<code>polyamory_listopia</code>), then the book URLs from each list page (<code>polyamory_lists</code>), and finally metadata from each book page (<code>polyamory_books</code>): title, link, author(s), genre(s), and book description. While the authors and title serve as identifiers and the genres may serve as annotations, the descriptions constitute the raw material on which i’ll perform a text analysis. (The full texts of the books themselves are not so freely available,<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> the titles are unlikely to reliably encode recurring features, and the genres are likely too few to discriminate except between broad categories.)</p>
<p>It’s important to note that there are two tiers of redundancy in the book list, only one of which i eliminate. First, the same Goodreads <em>entry</em><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> may appear in multiple lists; second, the same <em>book</em> may have been erroneously entered into Goodreads multiple times (which may appear in different lists or even the same list). It would require a few hours of manual curation to resolve the latter problem,<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> and i haven’t put in that time here; but the first problem is easily handled using <code>group_by()</code> and <code>summarize()</code>.</p>
</div>
<div id="crunch" class="section level2">
<h2>crunch</h2>
<p>Here is the book list obtained from the scraping script, slightly tidied, using the identifying string of the URL as a unique identifier:</p>
<pre class="r"><code>library(tidyverse)
read_rds(here::here(&quot;supplementary/goodreads-polyamory-booklist.rds&quot;)) %&gt;%
  ungroup() %&gt;%
  select(title = title_page, id = link, lists, genres, description) %&gt;%
  mutate(short_title = str_replace(title, &quot;(: .+$)|( \\(.+$)&quot;, &quot;&quot;)) %&gt;%
  mutate(id = str_replace(id, &quot;/book/show/([0-9]+)[^0-9].*$&quot;, &quot;\\1&quot;)) %&gt;%
  mutate(id = as.integer(id)) %&gt;%
  print() -&gt; polyamory_booklist</code></pre>
<pre><code>## # A tibble: 1,179 x 6
##    title           id lists      genres      description     short_title   
##    &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;         
##  1 100 Love …  1.13e4 Books on … Poetry|Cla… Against the ba… 100 Love Sonn…
##  2 199 Ways …  1.61e7 Sex, Love… &quot;&quot;          199 Ways To Im… 199 Ways To I…
##  3 30th Cent…  3.53e7 Ménage Po… Science Fi… CAPTAIN JENNIF… 30th Century  
##  4 A + E 4ev…  1.24e7 The Most … Sequential… Asher Machnik … A + E 4ever   
##  5 A Bear&#39;s …  2.73e7 Ménage Po… Erotica|Me… Most days, Oli… A Bear&#39;s Jour…
##  6 A Bear&#39;s …  4.06e7 Polyfi Tr… Paranormal… Most days, Oli… A Bear&#39;s Jour…
##  7 A Bear&#39;s …  2.71e7 Ménage Po… Erotica|Me… Charlotte “Cha… A Bear&#39;s Mercy
##  8 A Bear&#39;s …  4.06e7 Polyfi Tr… Menage|M M… Charlotte “Cha… A Bear&#39;s Mercy
##  9 A Bear&#39;s …  2.68e7 Ménage Po… Erotica|Me… Quinn Taylor h… A Bear&#39;s Neme…
## 10 A Bear&#39;s …  4.05e7 Polyfi Tr… Fantasy|Pa… Quinn Taylor h… A Bear&#39;s Neme…
## # … with 1,169 more rows</code></pre>
<p>(Some of the duplicate entries are evident.) My goal here is to represent these 1,179 book entries as points (or vectors) in some low-dimensional space, based on the co-occurrence of words in their descriptions. Ideally, the coordinate dimenisons of this space will correspond to identifiable features that will help characterize the dimensions <em>and</em> allow the dimensions to characterize individual books in turn. If the number of dimensions is low enough, then it will also be possible to visualize the point cloud and coordinate vectors.</p>
<div id="word-counts" class="section level3">
<h3>word counts</h3>
<p>Adapting a workflow from <a href="https://www.tidytextmining.com/">the tidytext book</a>, i first “unnest” the book list—in <a href="https://tidyr.tidyverse.org/reference/unnest.html">the tidyr sense</a>, except using a tidytext method specific to strings of written language. “Stop” words are articles, prepositions, and other words that add little to no value to a text analysis; instances of them in the unnested data are excluded via an <a href="https://dplyr.tidyverse.org/reference/join.html">anti-join</a>. Finally, i count the number of times each word is used in each description as a new variable <span class="math inline">\(n\)</span>.</p>
<pre class="r"><code>library(tidytext)
polyamory_booklist %&gt;%
  mutate(clean_descr = str_replace_all(description, &quot;[^[:alpha:]\\s]&quot;, &quot; &quot;)) %&gt;%
  mutate(clean_descr = str_trim(clean_descr)) %&gt;%
  select(-description) %&gt;%
  unnest_tokens(word, clean_descr) %&gt;%
  anti_join(stop_words, by = &quot;word&quot;) %&gt;%
  count(title, short_title, id, lists, genres, word, name = &quot;n&quot;) %&gt;%
  print() -&gt; polybooks_wordcounts</code></pre>
<pre><code>## # A tibble: 76,194 x 7
##    title       short_title       id lists     genres         word         n
##    &lt;chr&gt;       &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;    &lt;int&gt;
##  1 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… backdrop     1
##  2 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… beloved      1
##  3 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… celebra…     1
##  4 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… de           1
##  5 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… delicate     1
##  6 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… flowers      1
##  7 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… hot          1
##  8 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… isla         1
##  9 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… love         2
## 10 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… matilde      1
## # … with 76,184 more rows</code></pre>
<p>A couple more word counts that will be useful downstream are the number <span class="math inline">\(d\)</span> of book descriptions that use each word and the total number <span class="math inline">\(m\)</span> of uses across the corpus.</p>
<pre class="r"><code>polybooks_wordcounts %&gt;%
  left_join(
    polybooks_wordcounts %&gt;%
      group_by(word) %&gt;%
      summarize(d = n(), m = sum(n)),
    by = c(&quot;word&quot;)
  ) %&gt;%
  filter(m &gt;= 12) %&gt;%
  arrange(desc(d), desc(n)) %&gt;%
  print() -&gt; polybooks_wordusages</code></pre>
<pre><code>## # A tibble: 48,020 x 9
##    title      short_title      id lists  genres     word      n     d     m
##    &lt;chr&gt;      &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 What Love… What Love Is 2.95e7 Best … Nonfictio… love     16   495  1226
##  2 The Four … The Four Lo… 3.06e4 Books… Christian… love     14   495  1226
##  3 The Futur… The Future … 7.21e5 Books… Relations… love     14   495  1226
##  4 Why We Lo… Why We Love  1.32e5 Sex, … Psycholog… love     14   495  1226
##  5 Longing f… Longing for… 1.73e7 Sex, … &quot;&quot;         love     13   495  1226
##  6 The Art o… The Art of … 1.41e4 Books… Psycholog… love     13   495  1226
##  7 Rogue Eve… Rogue Ever … 4.52e7 FFF+ … Romance|C… love     12   495  1226
##  8 Ardently:… Ardently     2.56e7 Books… Classics|… love     11   495  1226
##  9 Love Magi… Love Magic   1.36e7 Sex, … &quot;&quot;         love     10   495  1226
## 10 In Praise… In Praise o… 1.36e7 Books… Philosoph… love      9   495  1226
## # … with 48,010 more rows</code></pre>
<p>2,197 distinct words appear in these descriptions (omitting the stop words).
Happily, the most prevalent (and, it turns out, most frequent) word in these descriptions is “love”. &lt;83</p>
</div>
<div id="relative-frequencies" class="section level3">
<h3>relative frequencies</h3>
<p>Its ubiquity makes “love” unlikely to be an effective token for the purpose of mapping this book collection in coordinate space: If a feature describes everything, then it describes nothing. The traditional usefulness weighting on words is instead the <em>term frequency–inverse document frequency</em>, or <em>tf-idf</em>. This is the product of two quotients: the term frequency <span class="math inline">\(\frac{n}{N}\)</span> for a given book, where <span class="math inline">\(N\)</span> is the number of words in its description; and (the logarithm of) the inverse of the document frequency <span class="math inline">\(\frac{d}{D}\)</span>, where <span class="math inline">\(D\)</span> is the number of books (descriptions) in the corpus. The tidytext package provides a helper function for this step, <code>bind_tf_idf()</code>, which obviates the previous chunk. Additionally i’ve filtered out the less discriminating words (having maximum td-idf at most <span class="math inline">\(0.1\)</span>):</p>
<pre class="r"><code>polybooks_wordcounts %&gt;%
  bind_tf_idf(word, id, n) %&gt;%
  group_by(word) %&gt;% filter(max(tf_idf) &gt; .1) %&gt;% ungroup() %&gt;%
  select(title, short_title, id, lists, genres, word, tf_idf) %&gt;%
  print() -&gt; polybooks_tfidf</code></pre>
<pre><code>## # A tibble: 59,759 x 7
##    title       short_title       id lists     genres         word    tf_idf
##    &lt;chr&gt;       &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;
##  1 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… backdr… 0.189 
##  2 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… beloved 0.136 
##  3 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… celebr… 0.171 
##  4 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… de      0.136 
##  5 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… delica… 0.162 
##  6 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… flowers 0.189 
##  7 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… hot     0.0738
##  8 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… isla    0.212 
##  9 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… love    0.0573
## 10 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… matilde 0.235 
## # … with 59,749 more rows</code></pre>
<p>I’ll use these remaining 9,005 words to make a first pass at gauging the dimensionality of the corpus and visualizing the books and features, using classical PCA. This requires widening the table into a classical data matrix having one row per book, one column per word, and log-transformed tf-idf values (to better mimic normality). The words are capitalized to prevent conflicts with existing column names, and a separate tibble includes only the metadata.</p>
<pre class="r"><code>polybooks_tfidf %&gt;%
  mutate(log_tf_idf = log(tf_idf)) %&gt;%
  select(-tf_idf) %&gt;%
  mutate(word = toupper(word)) %&gt;%
  spread(word, log_tf_idf, fill = 0) -&gt;
  polybooks_tfidf_wide
polybooks_tfidf_wide %&gt;%
  select(title, short_title, id, lists, genres) -&gt;
  polybooks_meta</code></pre>
<p>R implementations of ordination methods tend to produce atrociously unreadable output when sample sizes and variable dimensions grow large, so this is an especially apt place to invoke <a href="https://github.com/corybrunson/ordr">ordr</a>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The ID numbers serve as unique identifiers, just in case duplicate entries for the same book have exactly the same title, and so short titles are bound back in after the PCA:</p>
<pre class="r"><code>library(ordr)
polybooks_tfidf_wide %&gt;%
  select(-title, -short_title, -lists, -genres) %&gt;%
  column_to_rownames(&quot;id&quot;) %&gt;%
  as.matrix() %&gt;%
  prcomp() %&gt;%
  as_tbl_ord() %&gt;%
  augment() %&gt;%
  bind_cols_u(select(polybooks_meta, short_title)) %&gt;%
  print() -&gt; polybooks_pca</code></pre>
<pre><code>## # A tbl_ord of class &#39;prcomp&#39;: (1170 x 1170) x (9005 x 1170)&#39;
## # 1170 coordinates: PC1, PC2, ..., PC1170
## # 
## # U: [ 1170 x 1170 | 2 ]
##     PC1     PC2      PC3 ... |   .name   short_title             
##                              |   &lt;chr&gt;   &lt;chr&gt;                   
## 1 -3.12  0.0527 -0.00351     | 1 11339   100 Love Sonnets        
## 2  2.69 -3.08    3.14    ... | 2 161489… 199 Ways To Improve You…
## 3  1.93  1.14    0.819       | 3 352771… 30th Century            
## 4  1.52 -0.958  -0.255       | 4 124467… A + E 4ever             
## 5  2.71  4.21   -1.04        | 5 272627… A Bear&#39;s Journey        
## # … with 1,165 more rows
## # 
## # V: [ 9005 x 1170 | 2 ]
##         PC1         PC2        PC3 ... |   .name       .center
##                                        |   &lt;chr&gt;         &lt;dbl&gt;
## 1  0.00340   0.0000956   0.00569       | 1 À         -0.0182  
## 2 -0.00138   0.00104    -0.000921  ... | 2 AARON     -0.00836 
## 3 -0.00236  -0.00257     0.00216       | 3 ABANDONED -0.0203  
## 4  0.000251  0.0000375  -0.0000183     | 4 ABBEY     -0.000521
## 5  0.000370  0.00000365  0.000444      | 5 ABBI      -0.00174 
## # … with 9,000 more rows</code></pre>
<p>The ubiquitous scree plot helps gauge the dimensionality of the tf-idf space, though for readability i’m restricting it to the principal components (PCs) that account for at least <span class="math inline">\(0.4\%\)</span> of the total variance each:</p>
<pre class="r"><code>polybooks_pca %&gt;%
  fortify(.matrix = &quot;coord&quot;) %&gt;%
  filter(.prop_var &gt; .004) %&gt;%
  ggplot(aes(x = .name, y = .prop_var)) +
  geom_bar(stat = &quot;identity&quot;) +
  labs(x = &quot;&quot;, y = &quot;Proportion of variance&quot;) +
  theme(axis.text.x = element_text(angle = 90))</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/pca%20scree%20plot-1.png" width="768" /></p>
<p>This is not promising: The first PC accounts for less than one fortieth of the total variation, though the remaining PCs are even less distinctive. A 1-dimensional biplot would make the most sense, but in order to add some annotation i’m extending it to 2 dimensions, with the caveat that the second, vertical dimension should be understood—for any specific slice along PC1—as a more or less arbitrary perspective on a more or less spherical cloud. I’ll highlight and label the convex hull of the projected cloud to help think about how the books are dispersed:</p>
<pre class="r"><code>ggbiplot(polybooks_pca) +
  geom_u_point(alpha = .5) +
  geom_u_point(stat = &quot;chull&quot;, color = &quot;red&quot;) +
  geom_u_label_repel(
    stat = &quot;chull&quot;, aes(label = short_title),
    color = &quot;red&quot;, alpha = .75, size = 3
  ) +
  scale_x_continuous(expand = expand_scale(mult = c(0.4, 0.1)))</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/pca%20biplot-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The biplot exhibits a common pattern, with the bulk of observations clumped together in a corner and an increasingly thin periphery pushing conically outward. This pattern tends to emerge when the underlying variables are better understood as <em>features</em> than as <em>spectra</em>: When two distinctive and mutually repulsive (not necessarily to say mutually exclusive) features describe a data set, PCA will tend to yield a boomerang shape along the first two PCs. A classic example of this is a set of clinical and laboratory test data <a href="https://link.springer.com/article/10.1007/BF00423145">collected by G.M. Reaven and R. Miller</a> for a diabetic cohort, <a href="https://cran.r-project.org/web/packages/candisc/vignettes/diabetes.html">illustrated here by Michael Friendly</a>. When more features are present and remain mutually repulsive, the resulting bouquets tend to project onto PC1 and PC2 as cones.
This is in contrast to settings in which the constituent variables are uncorrelated, in which point clouds, whether high-dimensional or projected onto PCs, tend to be spherical (as discussed in a <a href="../../../../2019/08/02/lda/">previous post</a>).</p>
<p>It might therefore make more sense to get a reading of the books farthest from the clump, i.e. with the most extreme scores along PC1, rather than those on the outskirts of the point cloud on PC1 and PC2 together.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> To that end, i’ll take the books with the top 12 scores along, and the words with the top 12 loadings onto, PC1:</p>
<pre class="r"><code>polybooks_pca %&gt;%
  tidy(.matrix = &quot;both&quot;, include = &quot;all&quot;) %&gt;%
  select(-starts_with(&quot;PC&quot;), PC1, -.center) %&gt;%
  group_by(.matrix) %&gt;%
  arrange(desc(PC1)) %&gt;% 
  top_n(12, PC1) %&gt;%
  mutate(name = ifelse(is.na(short_title), tolower(.name), short_title)) %&gt;%
  ggplot(aes(x = reorder(name, PC1), y = PC1)) +
  facet_wrap(~ .matrix, scales = &quot;free&quot;) +
  coord_flip() +
  geom_bar(stat = &quot;identity&quot;) +
  labs(x = &quot;Score / Loading&quot;)</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/unnamed-chunk-1-1.png" width="768" /></p>
<p>The scores and loadings are likewise not very discriminating, but they are suggestive of the varieties of polyamory or poly-adjacent literature that push up against its boundaries: Many of the titles suggest niche subgenres of fiction, alongside some general advice or lifestyle volumes. Words like “magical”, “vampire”, “murdered”, “ancient”, and, i’d say, even “city” and “begins” are indicative of the former, while “text” and “academy” may be of the latter.
Overall, though, this is not a very informative dimension. Rather than separating one genre from others, it’s detecting unique descriptions from a medley of genres. And this is just what one might expect from the conical shape of the point cloud.</p>
<p>So, it seems likely that discriminating features exist along which these descriptions are organized, though principal components—which by definition capture variation, not distinction—aren’t a good tool for detecting them.
This is still good news for my ultimate goal of identifying these features. In a follow-up post i’ll use a couple of different tactics, one from text mining and the other not yet widely used.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Though making word frequency data publicly available would presumably be straightforward to do and have if anything a positive impact on sales.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>An entry may have multiple editions, but these never introduced redundancies in my workflow.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>I make a habit of posting “combine requests” for the Goodreads Librarians Group whenever i come across such instances)<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Still very much a work in progress.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Though it is interesting to me that <em>Fahrenheit 451</em> appears so unremarkable through this lens.<a href="#fnref5" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><p>I’ve gained immensely from reading the handful of non-fiction books on polyamory i’ve made time to, including Dossie Easton and Janet Hardy’s <em>The Ethical Slut</em> and Franklin Veaux and Eve Rickert’s <em>More Than Two</em>. While i’ve identified as poly since i discovered the term in grad school, i exhibit at least my share of emotional immaturity, and in addition to actual experience building healthy relationships i know i’d benefit from funneling a bit more of this literature into my reading queue. So, with a small group of friends (which has reduced for the time being to someone i’m dating plus myself), i recently started a poly/kink book club.</p>
<p>From the start, i intended to slide fiction, fantasy/scifi, memoir, anthropology, history, and any other genres i could discover onto our shelf, to accrue a well-rounded appreciation for what was available. Though this of course put me to wondering how i could even learn the contours of the poly literature, in order to ensure that i sampled widely from it! Fortunes of timing provided me with three excellent resources:</p>
<ul>
<li><a href="https://www.goodreads.com/user/show/57466005">a Goodreads account</a>;</li>
<li><a href="https://maraaverick.rbind.io/2017/08/goodreads-part-i-rgoodreads/">a blog tutorial</a> by Mara Averick on using R packages to scrape and crunch Goodreads data; and</li>
<li><a href="https://www.goodreads.com/list/tag/polyamory">a book</a> by Julia Silge and David Robinson on doing text mining in tidyverse style.</li>
</ul>
<p>The tutorial and <a href="https://maraaverick.rbind.io/2017/10/goodreads-part-2/">its sequel</a> will get you up to speed; i’ll outline my web-scraping script and focus mostly on the analysis.</p>
<div id="scrape" class="section level2">
<h2>scrape</h2>
<p>It would be, let’s say, impractical to manually search out books with explicitly poly content or themes, and even then i’d likely miss a bunch whose descriptions don’t let on too clearly. Fortunately, Goodreads allows users both to curate thematic lists <em>and</em> to tag their lists with keywords! <a href="https://www.goodreads.com/list/tag/polyamory">Here is the collection of lists tagged “polyamory”</a>, numbering in the dozens. Helpfully, the lists span genres, including fiction, young adult, memoirs, specific configurations like triads, and space opera (natch). Less helpfully, they also range more broadly in topic and theme, for example exotica, sex positivity, and love. On the whole, though, the tag seems like a good candidate for a one-off look.</p>
<p>There are other relevant tags, of course, like <a href="https://www.goodreads.com/list/tag/open-relationships">“open-relationships”</a>. But the others i’ve found turn out to be far less sensitive and often less specific. For example, <a href="https://www.goodreads.com/list/tag/nonmonogamy">“nonmonogamy”</a> turns up two relevant lists but <a href="https://www.goodreads.com/list/tag/non-monogamy">“non monogamy”</a> turns up a third, and <a href="https://www.goodreads.com/list/tag/swinging">“swinging”</a> yields four lists, of which one is a conspicuous false positive. For simplicity, i’ll stick with the “polyamory” tag. This is something i can revisit if the results aren’t facially relevant.</p>
<p>My code (in <a href="../supplementary/">the supplementary folder</a>) scrapes first the list URLs from the meta-list (<code>polyamory_listopia</code>), then the book URLs from each list page (<code>polyamory_lists</code>), and finally metadata from each book page (<code>polyamory_books</code>): title, link, author(s), genre(s), and book description. While the authors and title serve as identifiers and the genres may serve as annotations, the descriptions constitute the raw material on which i’ll perform a text analysis. (The full texts of the books themselves are not so freely available,<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> the titles are unlikely to reliably encode recurring features, and the genres are likely too few to discriminate except between broad categories.)</p>
<p>It’s important to note that there are two tiers of redundancy in the book list, only one of which i eliminate. First, the same Goodreads <em>entry</em><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> may appear in multiple lists; second, the same <em>book</em> may have been erroneously entered into Goodreads multiple times (which may appear in different lists or even the same list). It would require a few hours of manual curation to resolve the latter problem,<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> and i haven’t put in that time here; but the first problem is easily handled using <code>group_by()</code> and <code>summarize()</code>.</p>
</div>
<div id="crunch" class="section level2">
<h2>crunch</h2>
<p>Here is the book list obtained from the scraping script, slightly tidied, using the identifying string of the URL as a unique identifier:</p>
<pre class="r"><code>library(tidyverse)
read_rds(here::here(&quot;supplementary/goodreads-polyamory-booklist.rds&quot;)) %&gt;%
  ungroup() %&gt;%
  select(title = title_page, id = link, lists, genres, description) %&gt;%
  mutate(short_title = str_replace(title, &quot;(: .+$)|( \\(.+$)&quot;, &quot;&quot;)) %&gt;%
  mutate(id = str_replace(id, &quot;/book/show/([0-9]+)[^0-9].*$&quot;, &quot;\\1&quot;)) %&gt;%
  mutate(id = as.integer(id)) %&gt;%
  print() -&gt; polyamory_booklist</code></pre>
<pre><code>## # A tibble: 1,179 x 6
##    title           id lists      genres      description     short_title   
##    &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;         
##  1 100 Love …  1.13e4 Books on … Poetry|Cla… Against the ba… 100 Love Sonn…
##  2 199 Ways …  1.61e7 Sex, Love… &quot;&quot;          199 Ways To Im… 199 Ways To I…
##  3 30th Cent…  3.53e7 Ménage Po… Science Fi… CAPTAIN JENNIF… 30th Century  
##  4 A + E 4ev…  1.24e7 The Most … Sequential… Asher Machnik … A + E 4ever   
##  5 A Bear&#39;s …  2.73e7 Ménage Po… Erotica|Me… Most days, Oli… A Bear&#39;s Jour…
##  6 A Bear&#39;s …  4.06e7 Polyfi Tr… Paranormal… Most days, Oli… A Bear&#39;s Jour…
##  7 A Bear&#39;s …  2.71e7 Ménage Po… Erotica|Me… Charlotte “Cha… A Bear&#39;s Mercy
##  8 A Bear&#39;s …  4.06e7 Polyfi Tr… Menage|M M… Charlotte “Cha… A Bear&#39;s Mercy
##  9 A Bear&#39;s …  2.68e7 Ménage Po… Erotica|Me… Quinn Taylor h… A Bear&#39;s Neme…
## 10 A Bear&#39;s …  4.05e7 Polyfi Tr… Fantasy|Pa… Quinn Taylor h… A Bear&#39;s Neme…
## # … with 1,169 more rows</code></pre>
<p>(Some of the duplicate entries are evident.) My goal here is to represent these 1,179 book entries as points (or vectors) in some low-dimensional space, based on the co-occurrence of words in their descriptions. Ideally, the coordinate dimenisons of this space will correspond to identifiable features that will help characterize the dimensions <em>and</em> allow the dimensions to characterize individual books in turn. If the number of dimensions is low enough, then it will also be possible to visualize the point cloud and coordinate vectors.</p>
<div id="word-counts" class="section level3">
<h3>word counts</h3>
<p>Adapting a workflow from <a href="https://www.tidytextmining.com/">the tidytext book</a>, i first “unnest” the book list—in <a href="https://tidyr.tidyverse.org/reference/unnest.html">the tidyr sense</a>, except using a tidytext method specific to strings of written language. “Stop” words are articles, prepositions, and other words that add little to no value to a text analysis; instances of them in the unnested data are excluded via an <a href="https://dplyr.tidyverse.org/reference/join.html">anti-join</a>. Finally, i count the number of times each word is used in each description as a new variable <span class="math inline">\(n\)</span>.</p>
<pre class="r"><code>library(tidytext)
polyamory_booklist %&gt;%
  mutate(clean_descr = str_replace_all(description, &quot;[^[:alpha:]\\s]&quot;, &quot; &quot;)) %&gt;%
  mutate(clean_descr = str_trim(clean_descr)) %&gt;%
  select(-description) %&gt;%
  unnest_tokens(word, clean_descr) %&gt;%
  anti_join(stop_words, by = &quot;word&quot;) %&gt;%
  count(title, short_title, id, lists, genres, word, name = &quot;n&quot;) %&gt;%
  print() -&gt; polybooks_wordcounts</code></pre>
<pre><code>## # A tibble: 76,194 x 7
##    title       short_title       id lists     genres         word         n
##    &lt;chr&gt;       &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;    &lt;int&gt;
##  1 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… backdrop     1
##  2 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… beloved      1
##  3 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… celebra…     1
##  4 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… de           1
##  5 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… delicate     1
##  6 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… flowers      1
##  7 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… hot          1
##  8 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… isla         1
##  9 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… love         2
## 10 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… matilde      1
## # … with 76,184 more rows</code></pre>
<p>A couple more word counts that will be useful downstream are the number <span class="math inline">\(d\)</span> of book descriptions that use each word and the total number <span class="math inline">\(m\)</span> of uses across the corpus.</p>
<pre class="r"><code>polybooks_wordcounts %&gt;%
  left_join(
    polybooks_wordcounts %&gt;%
      group_by(word) %&gt;%
      summarize(d = n(), m = sum(n)),
    by = c(&quot;word&quot;)
  ) %&gt;%
  filter(m &gt;= 12) %&gt;%
  arrange(desc(d), desc(n)) %&gt;%
  print() -&gt; polybooks_wordusages</code></pre>
<pre><code>## # A tibble: 48,020 x 9
##    title      short_title      id lists  genres     word      n     d     m
##    &lt;chr&gt;      &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 What Love… What Love Is 2.95e7 Best … Nonfictio… love     16   495  1226
##  2 The Four … The Four Lo… 3.06e4 Books… Christian… love     14   495  1226
##  3 The Futur… The Future … 7.21e5 Books… Relations… love     14   495  1226
##  4 Why We Lo… Why We Love  1.32e5 Sex, … Psycholog… love     14   495  1226
##  5 Longing f… Longing for… 1.73e7 Sex, … &quot;&quot;         love     13   495  1226
##  6 The Art o… The Art of … 1.41e4 Books… Psycholog… love     13   495  1226
##  7 Rogue Eve… Rogue Ever … 4.52e7 FFF+ … Romance|C… love     12   495  1226
##  8 Ardently:… Ardently     2.56e7 Books… Classics|… love     11   495  1226
##  9 Love Magi… Love Magic   1.36e7 Sex, … &quot;&quot;         love     10   495  1226
## 10 In Praise… In Praise o… 1.36e7 Books… Philosoph… love      9   495  1226
## # … with 48,010 more rows</code></pre>
<p>2,197 distinct words appear in these descriptions (omitting the stop words).
Happily, the most prevalent (and, it turns out, most frequent) word in these descriptions is “love”. &lt;83</p>
</div>
<div id="relative-frequencies" class="section level3">
<h3>relative frequencies</h3>
<p>Its ubiquity makes “love” unlikely to be an effective token for the purpose of mapping this book collection in coordinate space: If a feature describes everything, then it describes nothing. The traditional usefulness weighting on words is instead the <em>term frequency–inverse document frequency</em>, or <em>tf-idf</em>. This is the product of two quotients: the term frequency <span class="math inline">\(\frac{n}{N}\)</span> for a given book, where <span class="math inline">\(N\)</span> is the number of words in its description; and (the logarithm of) the inverse of the document frequency <span class="math inline">\(\frac{d}{D}\)</span>, where <span class="math inline">\(D\)</span> is the number of books (descriptions) in the corpus. The tidytext package provides a helper function for this step, <code>bind_tf_idf()</code>, which obviates the previous chunk. Additionally i’ve filtered out the less discriminating words (having maximum td-idf at most <span class="math inline">\(0.1\)</span>):</p>
<pre class="r"><code>polybooks_wordcounts %&gt;%
  bind_tf_idf(word, id, n) %&gt;%
  group_by(word) %&gt;% filter(max(tf_idf) &gt; .1) %&gt;% ungroup() %&gt;%
  select(title, short_title, id, lists, genres, word, tf_idf) %&gt;%
  print() -&gt; polybooks_tfidf</code></pre>
<pre><code>## # A tibble: 59,759 x 7
##    title       short_title       id lists     genres         word    tf_idf
##    &lt;chr&gt;       &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;
##  1 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… backdr… 0.189 
##  2 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… beloved 0.136 
##  3 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… celebr… 0.171 
##  4 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… de      0.136 
##  5 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… delica… 0.162 
##  6 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… flowers 0.189 
##  7 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… hot     0.0738
##  8 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… isla    0.212 
##  9 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… love    0.0573
## 10 100 Love S… 100 Love Sonn… 11339 Books on… Poetry|Classi… matilde 0.235 
## # … with 59,749 more rows</code></pre>
<p>I’ll use these remaining 9,005 words to make a first pass at gauging the dimensionality of the corpus and visualizing the books and features, using classical PCA. This requires widening the table into a classical data matrix having one row per book, one column per word, and log-transformed tf-idf values (to better mimic normality). The words are capitalized to prevent conflicts with existing column names, and a separate tibble includes only the metadata.</p>
<pre class="r"><code>polybooks_tfidf %&gt;%
  mutate(log_tf_idf = log(tf_idf)) %&gt;%
  select(-tf_idf) %&gt;%
  mutate(word = toupper(word)) %&gt;%
  spread(word, log_tf_idf, fill = 0) -&gt;
  polybooks_tfidf_wide
polybooks_tfidf_wide %&gt;%
  select(title, short_title, id, lists, genres) -&gt;
  polybooks_meta</code></pre>
<p>R implementations of ordination methods tend to produce atrociously unreadable output when sample sizes and variable dimensions grow large, so this is an especially apt place to invoke <a href="https://github.com/corybrunson/ordr">ordr</a>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The ID numbers serve as unique identifiers, just in case duplicate entries for the same book have exactly the same title, and so short titles are bound back in after the PCA:</p>
<pre class="r"><code>library(ordr)
polybooks_tfidf_wide %&gt;%
  select(-title, -short_title, -lists, -genres) %&gt;%
  column_to_rownames(&quot;id&quot;) %&gt;%
  as.matrix() %&gt;%
  prcomp() %&gt;%
  as_tbl_ord() %&gt;%
  augment() %&gt;%
  bind_cols_u(select(polybooks_meta, short_title)) %&gt;%
  print() -&gt; polybooks_pca</code></pre>
<pre><code>## # A tbl_ord of class &#39;prcomp&#39;: (1170 x 1170) x (9005 x 1170)&#39;
## # 1170 coordinates: PC1, PC2, ..., PC1170
## # 
## # U: [ 1170 x 1170 | 2 ]
##     PC1     PC2      PC3 ... |   .name   short_title             
##                              |   &lt;chr&gt;   &lt;chr&gt;                   
## 1 -3.12  0.0527 -0.00351     | 1 11339   100 Love Sonnets        
## 2  2.69 -3.08    3.14    ... | 2 161489… 199 Ways To Improve You…
## 3  1.93  1.14    0.819       | 3 352771… 30th Century            
## 4  1.52 -0.958  -0.255       | 4 124467… A + E 4ever             
## 5  2.71  4.21   -1.04        | 5 272627… A Bear&#39;s Journey        
## # … with 1,165 more rows
## # 
## # V: [ 9005 x 1170 | 2 ]
##         PC1         PC2        PC3 ... |   .name       .center
##                                        |   &lt;chr&gt;         &lt;dbl&gt;
## 1  0.00340   0.0000956   0.00569       | 1 À         -0.0182  
## 2 -0.00138   0.00104    -0.000921  ... | 2 AARON     -0.00836 
## 3 -0.00236  -0.00257     0.00216       | 3 ABANDONED -0.0203  
## 4  0.000251  0.0000375  -0.0000183     | 4 ABBEY     -0.000521
## 5  0.000370  0.00000365  0.000444      | 5 ABBI      -0.00174 
## # … with 9,000 more rows</code></pre>
<p>The ubiquitous scree plot helps gauge the dimensionality of the tf-idf space, though for readability i’m restricting it to the principal components (PCs) that account for at least <span class="math inline">\(0.4\%\)</span> of the total variance each:</p>
<pre class="r"><code>polybooks_pca %&gt;%
  fortify(.matrix = &quot;coord&quot;) %&gt;%
  filter(.prop_var &gt; .004) %&gt;%
  ggplot(aes(x = .name, y = .prop_var)) +
  geom_bar(stat = &quot;identity&quot;) +
  labs(x = &quot;&quot;, y = &quot;Proportion of variance&quot;) +
  theme(axis.text.x = element_text(angle = 90))</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/pca%20scree%20plot-1.png" width="768" /></p>
<p>This is not promising: The first PC accounts for less than one fortieth of the total variation, though the remaining PCs are even less distinctive. A 1-dimensional biplot would make the most sense, but in order to add some annotation i’m extending it to 2 dimensions, with the caveat that the second, vertical dimension should be understood—for any specific slice along PC1—as a more or less arbitrary perspective on a more or less spherical cloud. I’ll highlight and label the convex hull of the projected cloud to help think about how the books are dispersed:</p>
<pre class="r"><code>ggbiplot(polybooks_pca) +
  geom_u_point(alpha = .5) +
  geom_u_point(stat = &quot;chull&quot;, color = &quot;red&quot;) +
  geom_u_label_repel(
    stat = &quot;chull&quot;, aes(label = short_title),
    color = &quot;red&quot;, alpha = .75, size = 3
  ) +
  scale_x_continuous(expand = expand_scale(mult = c(0.4, 0.1)))</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/pca%20biplot-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The biplot exhibits a common pattern, with the bulk of observations clumped together in a corner and an increasingly thin periphery pushing conically outward. This pattern tends to emerge when the underlying variables are better understood as <em>features</em> than as <em>spectra</em>: When two distinctive and mutually repulsive (not necessarily to say mutually exclusive) features describe a data set, PCA will tend to yield a boomerang shape along the first two PCs. A classic example of this is a set of clinical and laboratory test data <a href="https://link.springer.com/article/10.1007/BF00423145">collected by G.M. Reaven and R. Miller</a> for a diabetic cohort, <a href="https://cran.r-project.org/web/packages/candisc/vignettes/diabetes.html">illustrated here by Michael Friendly</a>. When more features are present and remain mutually repulsive, the resulting bouquets tend to project onto PC1 and PC2 as cones.
This is in contrast to settings in which the constituent variables are uncorrelated, in which point clouds, whether high-dimensional or projected onto PCs, tend to be spherical (as discussed in a <a href="../../../../2019/08/02/lda/">previous post</a>).</p>
<p>It might therefore make more sense to get a reading of the books farthest from the clump, i.e. with the most extreme scores along PC1, rather than those on the outskirts of the point cloud on PC1 and PC2 together.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> To that end, i’ll take the books with the top 12 scores along, and the words with the top 12 loadings onto, PC1:</p>
<pre class="r"><code>polybooks_pca %&gt;%
  tidy(.matrix = &quot;both&quot;, include = &quot;all&quot;) %&gt;%
  select(-starts_with(&quot;PC&quot;), PC1, -.center) %&gt;%
  group_by(.matrix) %&gt;%
  arrange(desc(PC1)) %&gt;% 
  top_n(12, PC1) %&gt;%
  mutate(name = ifelse(is.na(short_title), tolower(.name), short_title)) %&gt;%
  ggplot(aes(x = reorder(name, PC1), y = PC1)) +
  facet_wrap(~ .matrix, scales = &quot;free&quot;) +
  coord_flip() +
  geom_bar(stat = &quot;identity&quot;) +
  labs(x = &quot;Score / Loading&quot;)</code></pre>
<p><img src="/post/2019-10-25-goodreads-polyamory_files/figure-html/unnamed-chunk-1-1.png" width="768" /></p>
<p>The scores and loadings are likewise not very discriminating, but they are suggestive of the varieties of polyamory or poly-adjacent literature that push up against its boundaries: Many of the titles suggest niche subgenres of fiction, alongside some general advice or lifestyle volumes. Words like “magical”, “vampire”, “murdered”, “ancient”, and, i’d say, even “city” and “begins” are indicative of the former, while “text” and “academy” may be of the latter.
Overall, though, this is not a very informative dimension. Rather than separating one genre from others, it’s detecting unique descriptions from a medley of genres. And this is just what one might expect from the conical shape of the point cloud.</p>
<p>So, it seems likely that discriminating features exist along which these descriptions are organized, though principal components—which by definition capture variation, not distinction—aren’t a good tool for detecting them.
This is still good news for my ultimate goal of identifying these features. In a follow-up post i’ll use a couple of different tactics, one from text mining and the other not yet widely used.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Though making word frequency data publicly available would presumably be straightforward to do and have if anything a positive impact on sales.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>An entry may have multiple editions, but these never introduced redundancies in my workflow.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>I make a habit of posting “combine requests” for the Goodreads Librarians Group whenever i come across such instances)<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Still very much a work in progress.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Though it is interesting to me that <em>Fahrenheit 451</em> appears so unremarkable through this lens.<a href="#fnref5" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>my reaction to Hidden Figures</title>
      <link>/2019/10/11/hidden-figures/</link>
      <pubDate>2019 Oct 11 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/11/hidden-figures/</guid>
      <description><p><em>I’m a bit overwhelmed and a bit sick this month, and i didn’t want to miss two fortnights in a row. I wrote up this reaction to seeing the film <em>Hidden Figures</em> a couple of years ago on Facebook, and several of my friends liked it then, so i’m sharing it here. It’s slightly edited. –Cory</em></p>
<p><em>Hidden Figures</em> was probably the best civil rights–focused film i’ve seen in a while—not to disparage <em>Selma</em> or <em>The Butler</em>, but my personal taste is for greater subtlety. The movie is most impressive to me, though, as a mathematician biopic.</p>
<p>In large part, this is because they did the math right, or as right as i would hope a movie to. Young Goble didn’t, in an early establishing scene, just solve an equation on the chalkboard; she explained, clearly, what she had done and why it was a reasonable approach.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> After asserting her way into a closed-door (read: white men–only) board meeting, she calculated a re-entry trajectory on demand and, without boring into every detail, provided enough insight into the key steps to give her reasonably intelligent audience a sense of how she was doing it. Most entertainingly for me (though i don’t know how historically accurately), she had the profound realization, not just on screen but out loud, that the (at the time) ancient curiosity of Euler’s method could fuse (parabolic) launch/re-entry and (elliptical) orbital trajectories into a complete, cohesive course. I don’t know that i ever would have thought to wonder whether i’d ever see (one step along) the momentous transition of applied mathematics from analytic to numerical dominance enacted as entertainment.</p>
<p>To my mind, however, its principal achievement was in rejecting the now-entrenched Hollywood stereotype of the ostracized, neuroatypical, and/or disabled mathematical genius.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> This is not to suggest that ostracized, neuroatypical, and/or disabled mathematicians don’t warrant at least their share of screen time, or that it’s a shame that their biopics came first. What’s problematic about the trend until now is that these traits have been presented as integral to the characters’ mathematical interest or ability, which reinforces the myth that mathematical talent is aberrant (one that happily appears to be fading) and does a disservice to the achievements these people did in the face of exceptional challenges.</p>
<p>Three brief examples: Leading this trend was <em>A Beautiful Mind</em>, which depicts Nash’s schizophrenic hallucinations (which i’ve since learned were contrived, as his hallucinations were neither visual nor central to his illness) inspiring him to disengage from his work for a night out, at which he has an epiphany that leads to his famous equilibrium theorem. While i opted not to see <em>The Imitation Game</em>, i understand that the title encapsulates the parallel—absent from the biography but again contrived by the filmmakers—between Turing’s reverse-engineering of the Enigma machine and his faltering attempts, from the autism spectrum, to decipher and emulate other people’s behavior. And the central mathematical conflict in <em>The Man Who Knew Infinity</em>—between Ramanujan’s reverence for the elegance of his own (sometimes false) assertions and his mentor Hardy’s demand for rigorous proofs—is framed as a proxy for the volatile conflict between Hardy’s stigmatized atheism and his student’s stigmatized religion.</p>
<p>While Goble faces serious social challenges, they are not conceived as somehow of a piece with her mathematical talent and work. Individual prejudice impedes her access to resources. Workplace segregation interferes with her performance. Structural discrimination prevents her colleagues from advancing their careers. These obstacles are also externally imposed; Goble herself is mature, competent, and graceful, and the film sees her through an emotional trajectory—making time for her children, falling in love, navigating an exciting and stressful work environment—that is refreshingly unrelated to her genius. And it’s not like Goble’s typicality was essential to this. None of Nash’s, Turing’s, Ramanujan’s, and Hardy’s stories called for the gimmicking they received.</p>
<p>To the extent that future biopics about mathematicians eschew such gimmicks, they’ll owe some credit to <em>Hidden Figures</em>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>She solved a quartic polynomial, presented as a product of quadratics, by factoring each factor into two binomials.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This is, of course, based on my own impressions and expectations.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><p><em>I’m a bit overwhelmed and a bit sick this month, and i didn’t want to miss two fortnights in a row. I wrote up this reaction to seeing the film <em>Hidden Figures</em> a couple of years ago on Facebook, and several of my friends liked it then, so i’m sharing it here. It’s slightly edited. –Cory</em></p>
<p><em>Hidden Figures</em> was probably the best civil rights–focused film i’ve seen in a while—not to disparage <em>Selma</em> or <em>The Butler</em>, but my personal taste is for greater subtlety. The movie is most impressive to me, though, as a mathematician biopic.</p>
<p>In large part, this is because they did the math right, or as right as i would hope a movie to. Young Goble didn’t, in an early establishing scene, just solve an equation on the chalkboard; she explained, clearly, what she had done and why it was a reasonable approach.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> After asserting her way into a closed-door (read: white men–only) board meeting, she calculated a re-entry trajectory on demand and, without boring into every detail, provided enough insight into the key steps to give her reasonably intelligent audience a sense of how she was doing it. Most entertainingly for me (though i don’t know how historically accurately), she had the profound realization, not just on screen but out loud, that the (at the time) ancient curiosity of Euler’s method could fuse (parabolic) launch/re-entry and (elliptical) orbital trajectories into a complete, cohesive course. I don’t know that i ever would have thought to wonder whether i’d ever see (one step along) the momentous transition of applied mathematics from analytic to numerical dominance enacted as entertainment.</p>
<p>To my mind, however, its principal achievement was in rejecting the now-entrenched Hollywood stereotype of the ostracized, neuroatypical, and/or disabled mathematical genius.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> This is not to suggest that ostracized, neuroatypical, and/or disabled mathematicians don’t warrant at least their share of screen time, or that it’s a shame that their biopics came first. What’s problematic about the trend until now is that these traits have been presented as integral to the characters’ mathematical interest or ability, which reinforces the myth that mathematical talent is aberrant (one that happily appears to be fading) and does a disservice to the achievements these people did in the face of exceptional challenges.</p>
<p>Three brief examples: Leading this trend was <em>A Beautiful Mind</em>, which depicts Nash’s schizophrenic hallucinations (which i’ve since learned were contrived, as his hallucinations were neither visual nor central to his illness) inspiring him to disengage from his work for a night out, at which he has an epiphany that leads to his famous equilibrium theorem. While i opted not to see <em>The Imitation Game</em>, i understand that the title encapsulates the parallel—absent from the biography but again contrived by the filmmakers—between Turing’s reverse-engineering of the Enigma machine and his faltering attempts, from the autism spectrum, to decipher and emulate other people’s behavior. And the central mathematical conflict in <em>The Man Who Knew Infinity</em>—between Ramanujan’s reverence for the elegance of his own (sometimes false) assertions and his mentor Hardy’s demand for rigorous proofs—is framed as a proxy for the volatile conflict between Hardy’s stigmatized atheism and his student’s stigmatized religion.</p>
<p>While Goble faces serious social challenges, they are not conceived as somehow of a piece with her mathematical talent and work. Individual prejudice impedes her access to resources. Workplace segregation interferes with her performance. Structural discrimination prevents her colleagues from advancing their careers. These obstacles are also externally imposed; Goble herself is mature, competent, and graceful, and the film sees her through an emotional trajectory—making time for her children, falling in love, navigating an exciting and stressful work environment—that is refreshingly unrelated to her genius. And it’s not like Goble’s typicality was essential to this. None of Nash’s, Turing’s, Ramanujan’s, and Hardy’s stories called for the gimmicking they received.</p>
<p>To the extent that future biopics about mathematicians eschew such gimmicks, they’ll owe some credit to <em>Hidden Figures</em>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>She solved a quartic polynomial, presented as a product of quadratics, by factoring each factor into two binomials.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This is, of course, based on my own impressions and expectations.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>defining and taxonomizing alluvial diagrams</title>
      <link>/2019/09/13/flow-taxonomy/</link>
      <pubDate>2019 Sep 13 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/13/flow-taxonomy/</guid>
      <description><div id="background" class="section level2">
<h2>Background</h2>
<p>I first encountered alluvial diagrams, so-called, in a widely-shared <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0008694">paper</a> by Martin Rosvall and Carl T. Bergstrom. They were investigating the shifting boundaries between distinct scientific fields (“modules”), as reconstructed from sequential years of journal citation data (“states”), and proposed a specialized flow diagram “to highlight the significant changes, fusions, and fissions that the modules undergo between each pair of successive states”. At the time, i was unfamiliar with Sankey diagrams and only passingly familiar with flow diagrams—mostly by way of my computational biologist colleagues and their literature-derived signal transduction networks—but i could imagine myriad uses for this type of visualization and eventually went searching for an implementation in R.</p>
<p>This led me to the <a href="https://github.com/mbojan/alluvial">alluvial</a> package, which Michał Bojanowski was actively developing. As i got more comfortable with and excited about the then-ascendant tidyverse, i took it upon myself to put together <a href="https://github.com/corybrunson/ggalluvial">a ggplot2 extension</a>, which has since become considerably widely used. I later learned that previous developers had released similar extensions, specifically “parallel sets plots” in Thomas Lin Pedersen’s <a href="https://ggforce.data-imaginist.com">ggforce</a> and in Heike Hofmann and Marie Vendettouli’s <a href="https://github.com/heike/ggparallel">ggparallel</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
With continual exposure to these diverse implementations, i began to notice some subtle but important distinctions between others’ and my design principles. It also became clear that there was no consensus distinction between <em>alluvial</em> diagrams/plots and the more well-established genres of <em>Sankey</em> diagrams and <em>parallel sets</em> plots—indeed, no consensus on whether a distinction existed! Based on <a href="https://github.com/corybrunson/ggalluvial/issues">the ggalluvial issues page</a> and <a href="https://stackoverflow.com/search?q=ggalluvial">an occasional query on Stack Overflow</a>, the lack of generally accepted terms surrounding these sorts of diagrams remains a source of confusion.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> My goal in this post is to propose some vocabulary and a taxonomy to <del>alluviate</del> alleviate this confusion, or at least serve as a point of reference for others to propose improvements!</p>
</div>
<div id="a-proposed-taxonomy-for-width-encoded-diagrams" class="section level2">
<h2>A proposed taxonomy for width-encoded diagrams</h2>
<p>Since the terms “diagram” and “plot” (along with “chart”) are sometimes used interchangeably and sometimes fiercely contested, i’ll adopt a convention here and invite suggestions to improve it: <strong>Diagrams</strong> visualize information, <strong>charts</strong> are diagrams whose information is stored as data, and <strong>plots</strong> are charts that are uniquely determined from data by a fixed set of plotting rules.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
In these terms, ggalluvial and the other R packages discussed here unambiguously produce <em>plots</em>.</p>
<p>Here are how i think these various diagrams—flow, Sankey, parallel sets, and alluvial—are related:</p>
<ol style="list-style-type: decimal">
<li><em>Flow diagrams encode directed flows.</em> Flow diagrams may use ribbons, arrows, or other graphical elements to represent flows—that is, directed processes. For example, directed network diagrams of resource transmission between nodes are flow diagrams. Note that flows are not necessarily between nodes: Many Sankey diagrams include incoming or outgoing arrows representing flows from or to elements outside the diagram.</li>
<li><em>Sankey diagrams are flow diagrams with flow weights encoded as ribbon widths.</em> Flow diagrams may be unweighted: Signal transduction networks, for example, usually are. While Sankey diagrams are extremely flexible, their defining characteristic is that weighted flows are represented by ribbons whose widths indicate the weights or “volumes” of flows through them.</li>
<li><a href="https://datascience.blog.wzb.eu/2016/09/27/parallel-coordinate-plots-for-discrete-and-categorical-data-in-r-a-comparison/">Parallel coordinates plots</a> depict cases in a data set by their coordinates along several continuous dimensions, i.e. their values at several continuous variables, arrayed along a discrete axis. <em>Parallel sets plots are analogous to parallel coordinates plots with discrete-valued classificatory dimensions in place of continuous-valued coordinate dimensions.</em> This requires that the plotting rules both determine the order of the classes along each dimension and preserve their relative sizes. In practice, this means that ribbon widths indicate either the absolute weights of the cases or their proportions of the total weight.</li>
<li><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em> This yields a plot with a meaningful continuous axis perpendicular to the discrete axis: The height of a stack at any class is the cumulative weight of the preceding classes, and the stacked sets at different dimensions can be directly compared as stacked bar plots. (Alluvial plots also tend to use splines rather than segments to delineate ribbons, though i think this is far less important than the rules that position the sets and ribbon boundaries.)</li>
</ol>
<div id="examples" class="section level3">
<h3>Examples</h3>
<p>I’m not rendering any plots in this post, so i’ll illustrate these distinctions by pointing to several examples of each, with an emphasis on examples that are “mislabeled” according to my taxonomy:</p>
<ol style="list-style-type: decimal">
<li>Flow diagrams that are <em>not</em> Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:Typical_Signal_Schedule_and_Traffic_Flow_Diagram,_North-South_across_Market_(1929).png">Typical Signal Schedule and Traffic Flow Diagram</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:VA_Business_Line_Finance_and_Accounting.jpg">VA Business Line Finance and Accounting</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F2/">Process flow diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:JIE_Sankey_V5_Fig1.png">The Thermal Efficiency of Steam Engines</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_of_US_Consumer_Expenditure_in_2012.jpg">Sankey Diagram of US Consumer Expenditure in 2012</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Earth_heat_balance_Sankey_diagram.svg">Earth heat balance Sankey diagram</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F3/">Sankey diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Parallel sets plots that are <em>not</em> alluvial plots:
<ul>
<li><a href="https://www.jasondavies.com/parallel-sets/">Titanic Survivors</a>, Jason Davies</li>
<li><a href="https://xeno.graphics/stacked-area-alluvial-diagram/">Stacked area alluvial diagram</a>, Xenographics (note that the vertical axis applies only to the area plot)</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_-_Income_Statement.jpg">Sankey Diagram - Income Statement</a>, Wikimedia Commons</li>
<li><a href="https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0008694.g003">Mapping change in science</a>, <em>PLoS ONE</em></li>
<li><a href="https://www.researchgate.net/figure/Figure-S1-Sankey-diagram-on-global-green-virtual-water-flows-Sankey_fig5_303306020">Sankey diagram on global green virtual water flows</a>, Serrano, Guan, Duarte, and Paavola (2016)</li>
</ul></li>
<li>Alluvial plots:
<ul>
<li><a href="https://www.theinformationlab.co.uk/2018/03/09/build-sankey-diagram-tableau-without-data-prep-beforehand/">Superstore’s Super Sankey</a>, The Information Lab</li>
<li><a href="https://www.theguardian.com/politics/2016/may/06/holyrood-elections-see-rise-of-team-ruth-and-demise-of-labour-vision">How Scotland’s political geography changed, seat by seat</a>, <em>The Guardian</em></li>
<li><a href="https://www.researchgate.net/figure/Alluvial-diagram-for-mapping-changes-in-the-Global-network-The-top-ten-communities_fig6_267734552">Alluvial diagram for mapping changes in the Global network</a>, Lu and Brelsford (2014)</li>
</ul></li>
</ol>
</div>
<div id="how-my-proposal-stacks-up" class="section level3">
<h3>How my proposal stacks up</h3>
<p>Without exhaustively surveying the Internet and technical literature, it’s worthwhile to benchmark my distinctions against those made by some popular chart catalogues, which are much more representative of usage patterns. Xenographics lists several at the end of <a href="https://xeno.graphics/articles/on-graphonyms-the-importance-of-chart-type-names/">their discussion of graphonyms</a>, and of these three make clear distinctions between some of the types described above:</p>
<ul>
<li><a href="https://datavizproject.com/">The DataViz Project</a> describes Sankey diagrams as i did above, and distinguishes alluvial plots from parallel sets plots only in terms of the orientation of the axes (which is horizontal versus vertical) and of the shapes of the connecting ribbons.</li>
<li><a href="https://datavizcatalogue.com">The Data Visualization Catalogue</a> distinguishes parallel sets plots from Sankey diagrams as not using arrows and binning the flows (ribbons) at regular intervals. (They don’t have an entry on alluvial plots.)</li>
<li><a href="http://visualizationuniverse.com/charts/">The Visualization Universe</a> has entries for Sankey diagrams and alluvial plots, but their descriptions are excerpts (or, at least, subsets) of those of the DataViz Project.</li>
</ul>
<p>Importantly, in my taxonomy, by and large, <strong>alluvial plots are not Sankey diagrams, nor even flow diagrams</strong>. This seems appropriate on reflection, since even the original alluvial diagrams did not represent the transmission of material or information between nodes but changes in classification over time.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> This is not to say that alluvial plots cannot represent flow data—several popular examples do—but that the plot elements are not <em>specific</em> to flow data; and that, as a result, the directedness of flow data may not be conveyed well in an alluvial plot.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> I seem to be in agreement with the popular catalogues here.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>A more subtle pattern of usage reflected in my proposal is that parallel sets plots may resize flows from dimension to dimension to reflect changes in proportion that do not necessarily correspond to chances in amount. This must be done carefully in such plots, but it would be anathema to a Sankey diagram.
I’ll also endorse Elijah Meeks’ point, <a href="https://medium.com/@Elijah_Meeks/alluvial-charts-and-their-discontents-10a77d55216b">from this Medium post</a>, that Sankey diagrams can include cycles, whereas parallel sets and alluvial plots would not be able to encode such patterns.</p>
<p>I’m also making a distinction between alluvial and parallel sets plots that the catalogues—and, in my experience, other developers—don’t make.
Indeed, i haven’t seen a discussion anywhere else of whether a parallel sets or alluvial plot puts repeated categories in the same order, or whether the parallel sets are stacked so that the perpendicular axis measures their cumulative size, or whether either of these features is relevant to the choice of which type of plot is better-suited to a given purpose.
Having spent hours on the problem of whether different orderings might benefit a plot, and having been prompted several times to implement gaps between the sets, i’ve come to take the strong position that this distinction matters and that the terminology should reflect it.
While good definitions describe usage rather than prescribe it, i think it’s worth making an argument for this particular technical distinction while the terminology has not yet, ahem, sedemented.</p>
</div>
</div>
<div id="a-prescription-for-distinguishing-alluvial-and-parallel-sets-plots" class="section level2">
<h2>A prescription for distinguishing alluvial and parallel sets plots</h2>
<p>To reiterate:</p>
<blockquote>
<p><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em></p>
</blockquote>
<div id="caveat" class="section level3">
<h3>Caveat</h3>
<p>Neither Rosvall and Bergstrom, who popularized alluvial diagrams, nor Bojanowski, on whose package i based ggalluvial, included a cumulative weight axis perpendicular to the dimensions axis. In fact, what originally prompted me to omit the gaps between strata in ggalluvial was that i didn’t know how to get rid of the vertical axis! Like every great idea i believe i’ve had, i arrived at this one via gradient descent.</p>
<p>That’s still not to say i was first: Hofmann and Vendettouli wrote ggparallel to stack the sets in each dimension and retain a vertical axis in their plots. There may well be other such implementations, but i’m most familiar with the R ecosystem.</p>
</div>
<div id="utility" class="section level3">
<h3>Utility</h3>
<p>First, i claim that the features that distinguish alluvial from parallel sets plots—consistent ordering of sets and a cumulative weight axis—have practical importance.
In particular, they have importance <em>beyond</em> the ability to visually distinguish the sets and compare their weights along each dimension, as can be done from any parallel sets plot.
The use cases i’ve surveyed reveal three distinct settings in which alluvial plots are superior to other parallel sets plots:<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p><strong>Repeated categorical measures data:</strong>
Several users have used alluvial plots to represent data consisting of partitions of cases into the same (or overlapping) classification schemes at different times, in particular before and after some intervention or other significant event.
See scholarly examples in <a href="https://www.sciencedirect.com/science/article/pii/S0378429018317337">Baudron, Ndoli, Habarurema, and Silva (2019)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0195925518302026">Kissinger and Reznik (2019)</a>, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/gec3.12441">Muenchow, Schäfer, and Krüger (2019)</a>, <a href="https://www.jneurosci.org/content/39/28/5534.abstract">Chong et al (2019)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/ejhf.1547">Schlotter et al (2019)</a>, and tweeted examples by <a href="https://twitter.com/KenSteif/status/1006542071375761408">KenSteif</a>, <a href="https://twitter.com/frau_dr_barber/status/1130167116164927488">frau_dr_barber</a>, <a href="https://twitter.com/ericpgreen/status/1133840554968666112">ericpgreen</a>, and <a href="https://twitter.com/5amStats/status/1135153961227509762">5amStats</a> (starboard image).
For these diagrams to communicate the data efficiently, it is essential that the classes be consistently ordered.
Some implementations of parallel sets plots default to this behavior, as <a href="https://matthewdharris.com/2017/11/11/a-brief-diversion-into-static-alluvial-sankey-diagrams-in-r/">showcased by Matt Harris</a>; but others may automatically sort the sets by size, and still others allow arbitrary orderings—which may be useful for interactive exploration, as with Meeks’ implementation, but inappropriate for static renderings.</p>
<p><strong>Multipartite network data:</strong>
A handful of users have used alluvial plots to represent multipartite graphs, which necessarily satisfy the constraint that the total degree of the nodes in each part is the same. In particular, genomic analyses produce one-to-one connections among stages in transcription processes (lncRNA, miRNA, and mRNA), which have been encoded into alluvial plots by <a href="https://peerj.com/articles/6091/">Zheng et al (2018)</a>, <a href="https://cancerci.biomedcentral.com/articles/10.1186/s12935-019-0817-y">Long et al (2019a)</a>, and <a href="https://www.frontiersin.org/articles/10.3389/fonc.2019.00649/full">Long et al (2019b)</a>. See <a href="https://www.frontiersin.org/articles/10.3389/fimmu.2019.00660/full">Vazquez Bernat et al (2019)</a> for a similar usage, and <a href="https://twitter.com/MyriamCTraub/status/1169236685160402946">MyriamCTraub</a> and <a href="https://twitter.com/BenMoretti/status/1100378930865827840">BenMoretti</a> on Twitter.
A similar principle is at work in <a href="https://watanabesmith.rbind.io/post/ranked-black-mirror/">Watanabe Smith’s illustration of ranked-choice voting</a>, especially with respect to those occasions when a voter lost influence by only ranking a few of the options (which segues into the next setting).
While these users excluded the cumulative weight axis, through the fixed heights of the stacked sets the plots communicate that the total degree at each stage is the same.</p>
<p><strong>Censored data:</strong>
Finally, something i think alluvial plots do exceptionally better than general parallel sets plots is accentuate censoredness in data. Check out scholarly articles by <a href="https://www.sciencedirect.com/science/article/pii/S1075996418301021">Seekatz et al (2018)</a> and <a href="https://journals.lww.com/ccmjournal/Fulltext/2019/01000/Evaluating_Delivery_of_Low_Tidal_Volume.8.aspx">Sjoding, Gong, Hass, and Iwashyna (2019)</a> and <a href="https://mdneuzerling.com/post/my-data-science-job-hunt/">David Neuzerling’s reflections on the job hunt</a>, which use alluvial plots to depict changes in subjects’ status across several time points or stages with a specific set (or blank space where it would be) for subjects who became unavailable later in the study. The cumulative weight axis and gridlines allow the reader to immediately discern the reduction in sample size at each step.
(Though they use network data, <a href="https://www.nature.com/articles/srep06773">Lu and Brelsford (2014)</a> make similar use of this property to visualize non-connections between sets together with connections.)</p>
<p>That’s the substance of my argument for the alluvial–parallel sets distinction, but i’ll finish with a bit of fluorish.</p>
</div>
<div id="connotativity" class="section level3">
<h3>Connotativity</h3>
<p>The special features of alluvial plots are connoted by their peculiar terminology: I’ve decided to call the rectangles representing the parallel sets “strata” to suggest that they are more stable than the crisscrossing alluvia, and indeed this stability (with respect to their order in the plot) is what users expect when many of the categorical dimensions classify subjects into the same categories. The term also suggests, as does “alluvia”, that the various sets into which the subjects are partitioned at each (usually horizontal) position along the dimension axis are themselves (vertically) positioned in accordance with gravity. That is, they have “settled” one atop another with no defiantly empty space in between.</p>
<p>Thus i deposit my case.</p>
</div>
</div>
<div id="coda" class="section level2">
<h2>Coda</h2>
<p>I am by no means an expert in data visualization! While i feel strongly that my taxonomy makes the best of the present scrambling of terms, i am quite open to countersuggestions and especially to use cases that undercut it. If you come across them, please do send them my way.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I only just discovered Yawei Ge and Hofmann’s <a href="https://yaweige.github.io/ggpcp/">ggpcp</a> package for general parallel coordinate plots, under active development!<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>That’s not to say that useful distinctions haven’t been made somewhere, e.g. the technical literature on data visualization, but i feel confident in claiming that they have had limited effects on practice!<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>In ggplot2, these rules are the stat, geom, coord, and scale layers. One of the great contributions of ggplot2, in my view, was to make them explicit to lay users like myself.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>It’s unfortunate that i named the ribbons between adjacent axes <em>flows</em> in ggalluvial, but i maintain that it was preferable to calling them <em>fans</em>.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>A similar misfit is a simplicial complex represented by a network diagram: The type of diagram is designed for a different type of data (pairwise-relational with possible directedness and multiplicity) and fails to convey essential data elements (higher-dimensional simplices).<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>A contrary example is RAWGraphs, which <a href="https://rawgraphs.io/learning/how-to-make-an-alluvial-diagram/">describes alluvial plots</a> as “a specific kind of Sankey diagrams”.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>I found most of these examples by searching for “ggalluvial” in Google Scholar or Twitter.<a href="#fnref7" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><div id="background" class="section level2">
<h2>Background</h2>
<p>I first encountered alluvial diagrams, so-called, in a widely-shared <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0008694">paper</a> by Martin Rosvall and Carl T. Bergstrom. They were investigating the shifting boundaries between distinct scientific fields (“modules”), as reconstructed from sequential years of journal citation data (“states”), and proposed a specialized flow diagram “to highlight the significant changes, fusions, and fissions that the modules undergo between each pair of successive states”. At the time, i was unfamiliar with Sankey diagrams and only passingly familiar with flow diagrams—mostly by way of my computational biologist colleagues and their literature-derived signal transduction networks—but i could imagine myriad uses for this type of visualization and eventually went searching for an implementation in R.</p>
<p>This led me to the <a href="https://github.com/mbojan/alluvial">alluvial</a> package, which Michał Bojanowski was actively developing. As i got more comfortable with and excited about the then-ascendant tidyverse, i took it upon myself to put together <a href="https://github.com/corybrunson/ggalluvial">a ggplot2 extension</a>, which has since become considerably widely used. I later learned that previous developers had released similar extensions, specifically “parallel sets plots” in Thomas Lin Pedersen’s <a href="https://ggforce.data-imaginist.com">ggforce</a> and in Heike Hofmann and Marie Vendettouli’s <a href="https://github.com/heike/ggparallel">ggparallel</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
With continual exposure to these diverse implementations, i began to notice some subtle but important distinctions between others’ and my design principles. It also became clear that there was no consensus distinction between <em>alluvial</em> diagrams/plots and the more well-established genres of <em>Sankey</em> diagrams and <em>parallel sets</em> plots—indeed, no consensus on whether a distinction existed! Based on <a href="https://github.com/corybrunson/ggalluvial/issues">the ggalluvial issues page</a> and <a href="https://stackoverflow.com/search?q=ggalluvial">an occasional query on Stack Overflow</a>, the lack of generally accepted terms surrounding these sorts of diagrams remains a source of confusion.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> My goal in this post is to propose some vocabulary and a taxonomy to <del>alluviate</del> alleviate this confusion, or at least serve as a point of reference for others to propose improvements!</p>
</div>
<div id="a-proposed-taxonomy-for-width-encoded-diagrams" class="section level2">
<h2>A proposed taxonomy for width-encoded diagrams</h2>
<p>Since the terms “diagram” and “plot” (along with “chart”) are sometimes used interchangeably and sometimes fiercely contested, i’ll adopt a convention here and invite suggestions to improve it: <strong>Diagrams</strong> visualize information, <strong>charts</strong> are diagrams whose information is stored as data, and <strong>plots</strong> are charts that are uniquely determined from data by a fixed set of plotting rules.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
In these terms, ggalluvial and the other R packages discussed here unambiguously produce <em>plots</em>.</p>
<p>Here are how i think these various diagrams—flow, Sankey, parallel sets, and alluvial—are related:</p>
<ol style="list-style-type: decimal">
<li><em>Flow diagrams encode directed flows.</em> Flow diagrams may use ribbons, arrows, or other graphical elements to represent flows—that is, directed processes. For example, directed network diagrams of resource transmission between nodes are flow diagrams. Note that flows are not necessarily between nodes: Many Sankey diagrams include incoming or outgoing arrows representing flows from or to elements outside the diagram.</li>
<li><em>Sankey diagrams are flow diagrams with flow weights encoded as ribbon widths.</em> Flow diagrams may be unweighted: Signal transduction networks, for example, usually are. While Sankey diagrams are extremely flexible, their defining characteristic is that weighted flows are represented by ribbons whose widths indicate the weights or “volumes” of flows through them.</li>
<li><a href="https://datascience.blog.wzb.eu/2016/09/27/parallel-coordinate-plots-for-discrete-and-categorical-data-in-r-a-comparison/">Parallel coordinates plots</a> depict cases in a data set by their coordinates along several continuous dimensions, i.e. their values at several continuous variables, arrayed along a discrete axis. <em>Parallel sets plots are analogous to parallel coordinates plots with discrete-valued classificatory dimensions in place of continuous-valued coordinate dimensions.</em> This requires that the plotting rules both determine the order of the classes along each dimension and preserve their relative sizes. In practice, this means that ribbon widths indicate either the absolute weights of the cases or their proportions of the total weight.</li>
<li><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em> This yields a plot with a meaningful continuous axis perpendicular to the discrete axis: The height of a stack at any class is the cumulative weight of the preceding classes, and the stacked sets at different dimensions can be directly compared as stacked bar plots. (Alluvial plots also tend to use splines rather than segments to delineate ribbons, though i think this is far less important than the rules that position the sets and ribbon boundaries.)</li>
</ol>
<div id="examples" class="section level3">
<h3>Examples</h3>
<p>I’m not rendering any plots in this post, so i’ll illustrate these distinctions by pointing to several examples of each, with an emphasis on examples that are “mislabeled” according to my taxonomy:</p>
<ol style="list-style-type: decimal">
<li>Flow diagrams that are <em>not</em> Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:Typical_Signal_Schedule_and_Traffic_Flow_Diagram,_North-South_across_Market_(1929).png">Typical Signal Schedule and Traffic Flow Diagram</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:VA_Business_Line_Finance_and_Accounting.jpg">VA Business Line Finance and Accounting</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F2/">Process flow diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Sankey diagrams:
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:JIE_Sankey_V5_Fig1.png">The Thermal Efficiency of Steam Engines</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_of_US_Consumer_Expenditure_in_2012.jpg">Sankey Diagram of US Consumer Expenditure in 2012</a>, Wikimedia Commons</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Earth_heat_balance_Sankey_diagram.svg">Earth heat balance Sankey diagram</a>, Wikimedia Commons</li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438221/figure/F3/">Sankey diagram</a>, Alonso et al (2017)</li>
</ul></li>
<li>Parallel sets plots that are <em>not</em> alluvial plots:
<ul>
<li><a href="https://www.jasondavies.com/parallel-sets/">Titanic Survivors</a>, Jason Davies</li>
<li><a href="https://xeno.graphics/stacked-area-alluvial-diagram/">Stacked area alluvial diagram</a>, Xenographics (note that the vertical axis applies only to the area plot)</li>
<li><a href="https://commons.wikimedia.org/wiki/File:Sankey_Diagram_-_Income_Statement.jpg">Sankey Diagram - Income Statement</a>, Wikimedia Commons</li>
<li><a href="https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0008694.g003">Mapping change in science</a>, <em>PLoS ONE</em></li>
<li><a href="https://www.researchgate.net/figure/Figure-S1-Sankey-diagram-on-global-green-virtual-water-flows-Sankey_fig5_303306020">Sankey diagram on global green virtual water flows</a>, Serrano, Guan, Duarte, and Paavola (2016)</li>
</ul></li>
<li>Alluvial plots:
<ul>
<li><a href="https://www.theinformationlab.co.uk/2018/03/09/build-sankey-diagram-tableau-without-data-prep-beforehand/">Superstore’s Super Sankey</a>, The Information Lab</li>
<li><a href="https://www.theguardian.com/politics/2016/may/06/holyrood-elections-see-rise-of-team-ruth-and-demise-of-labour-vision">How Scotland’s political geography changed, seat by seat</a>, <em>The Guardian</em></li>
<li><a href="https://www.researchgate.net/figure/Alluvial-diagram-for-mapping-changes-in-the-Global-network-The-top-ten-communities_fig6_267734552">Alluvial diagram for mapping changes in the Global network</a>, Lu and Brelsford (2014)</li>
</ul></li>
</ol>
</div>
<div id="how-my-proposal-stacks-up" class="section level3">
<h3>How my proposal stacks up</h3>
<p>Without exhaustively surveying the Internet and technical literature, it’s worthwhile to benchmark my distinctions against those made by some popular chart catalogues, which are much more representative of usage patterns. Xenographics lists several at the end of <a href="https://xeno.graphics/articles/on-graphonyms-the-importance-of-chart-type-names/">their discussion of graphonyms</a>, and of these three make clear distinctions between some of the types described above:</p>
<ul>
<li><a href="https://datavizproject.com/">The DataViz Project</a> describes Sankey diagrams as i did above, and distinguishes alluvial plots from parallel sets plots only in terms of the orientation of the axes (which is horizontal versus vertical) and of the shapes of the connecting ribbons.</li>
<li><a href="https://datavizcatalogue.com">The Data Visualization Catalogue</a> distinguishes parallel sets plots from Sankey diagrams as not using arrows and binning the flows (ribbons) at regular intervals. (They don’t have an entry on alluvial plots.)</li>
<li><a href="http://visualizationuniverse.com/charts/">The Visualization Universe</a> has entries for Sankey diagrams and alluvial plots, but their descriptions are excerpts (or, at least, subsets) of those of the DataViz Project.</li>
</ul>
<p>Importantly, in my taxonomy, by and large, <strong>alluvial plots are not Sankey diagrams, nor even flow diagrams</strong>. This seems appropriate on reflection, since even the original alluvial diagrams did not represent the transmission of material or information between nodes but changes in classification over time.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> This is not to say that alluvial plots cannot represent flow data—several popular examples do—but that the plot elements are not <em>specific</em> to flow data; and that, as a result, the directedness of flow data may not be conveyed well in an alluvial plot.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> I seem to be in agreement with the popular catalogues here.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>A more subtle pattern of usage reflected in my proposal is that parallel sets plots may resize flows from dimension to dimension to reflect changes in proportion that do not necessarily correspond to chances in amount. This must be done carefully in such plots, but it would be anathema to a Sankey diagram.
I’ll also endorse Elijah Meeks’ point, <a href="https://medium.com/@Elijah_Meeks/alluvial-charts-and-their-discontents-10a77d55216b">from this Medium post</a>, that Sankey diagrams can include cycles, whereas parallel sets and alluvial plots would not be able to encode such patterns.</p>
<p>I’m also making a distinction between alluvial and parallel sets plots that the catalogues—and, in my experience, other developers—don’t make.
Indeed, i haven’t seen a discussion anywhere else of whether a parallel sets or alluvial plot puts repeated categories in the same order, or whether the parallel sets are stacked so that the perpendicular axis measures their cumulative size, or whether either of these features is relevant to the choice of which type of plot is better-suited to a given purpose.
Having spent hours on the problem of whether different orderings might benefit a plot, and having been prompted several times to implement gaps between the sets, i’ve come to take the strong position that this distinction matters and that the terminology should reflect it.
While good definitions describe usage rather than prescribe it, i think it’s worth making an argument for this particular technical distinction while the terminology has not yet, ahem, sedemented.</p>
</div>
</div>
<div id="a-prescription-for-distinguishing-alluvial-and-parallel-sets-plots" class="section level2">
<h2>A prescription for distinguishing alluvial and parallel sets plots</h2>
<p>To reiterate:</p>
<blockquote>
<p><em>Alluvial plots are parallel sets plots in which classes are ordered consistently across dimensions and stacked without gaps at each dimension.</em></p>
</blockquote>
<div id="caveat" class="section level3">
<h3>Caveat</h3>
<p>Neither Rosvall and Bergstrom, who popularized alluvial diagrams, nor Bojanowski, on whose package i based ggalluvial, included a cumulative weight axis perpendicular to the dimensions axis. In fact, what originally prompted me to omit the gaps between strata in ggalluvial was that i didn’t know how to get rid of the vertical axis! Like every great idea i believe i’ve had, i arrived at this one via gradient descent.</p>
<p>That’s still not to say i was first: Hofmann and Vendettouli wrote ggparallel to stack the sets in each dimension and retain a vertical axis in their plots. There may well be other such implementations, but i’m most familiar with the R ecosystem.</p>
</div>
<div id="utility" class="section level3">
<h3>Utility</h3>
<p>First, i claim that the features that distinguish alluvial from parallel sets plots—consistent ordering of sets and a cumulative weight axis—have practical importance.
In particular, they have importance <em>beyond</em> the ability to visually distinguish the sets and compare their weights along each dimension, as can be done from any parallel sets plot.
The use cases i’ve surveyed reveal three distinct settings in which alluvial plots are superior to other parallel sets plots:<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p><strong>Repeated categorical measures data:</strong>
Several users have used alluvial plots to represent data consisting of partitions of cases into the same (or overlapping) classification schemes at different times, in particular before and after some intervention or other significant event.
See scholarly examples in <a href="https://www.sciencedirect.com/science/article/pii/S0378429018317337">Baudron, Ndoli, Habarurema, and Silva (2019)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0195925518302026">Kissinger and Reznik (2019)</a>, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/gec3.12441">Muenchow, Schäfer, and Krüger (2019)</a>, <a href="https://www.jneurosci.org/content/39/28/5534.abstract">Chong et al (2019)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/ejhf.1547">Schlotter et al (2019)</a>, and tweeted examples by <a href="https://twitter.com/KenSteif/status/1006542071375761408">KenSteif</a>, <a href="https://twitter.com/frau_dr_barber/status/1130167116164927488">frau_dr_barber</a>, <a href="https://twitter.com/ericpgreen/status/1133840554968666112">ericpgreen</a>, and <a href="https://twitter.com/5amStats/status/1135153961227509762">5amStats</a> (starboard image).
For these diagrams to communicate the data efficiently, it is essential that the classes be consistently ordered.
Some implementations of parallel sets plots default to this behavior, as <a href="https://matthewdharris.com/2017/11/11/a-brief-diversion-into-static-alluvial-sankey-diagrams-in-r/">showcased by Matt Harris</a>; but others may automatically sort the sets by size, and still others allow arbitrary orderings—which may be useful for interactive exploration, as with Meeks’ implementation, but inappropriate for static renderings.</p>
<p><strong>Multipartite network data:</strong>
A handful of users have used alluvial plots to represent multipartite graphs, which necessarily satisfy the constraint that the total degree of the nodes in each part is the same. In particular, genomic analyses produce one-to-one connections among stages in transcription processes (lncRNA, miRNA, and mRNA), which have been encoded into alluvial plots by <a href="https://peerj.com/articles/6091/">Zheng et al (2018)</a>, <a href="https://cancerci.biomedcentral.com/articles/10.1186/s12935-019-0817-y">Long et al (2019a)</a>, and <a href="https://www.frontiersin.org/articles/10.3389/fonc.2019.00649/full">Long et al (2019b)</a>. See <a href="https://www.frontiersin.org/articles/10.3389/fimmu.2019.00660/full">Vazquez Bernat et al (2019)</a> for a similar usage, and <a href="https://twitter.com/MyriamCTraub/status/1169236685160402946">MyriamCTraub</a> and <a href="https://twitter.com/BenMoretti/status/1100378930865827840">BenMoretti</a> on Twitter.
A similar principle is at work in <a href="https://watanabesmith.rbind.io/post/ranked-black-mirror/">Watanabe Smith’s illustration of ranked-choice voting</a>, especially with respect to those occasions when a voter lost influence by only ranking a few of the options (which segues into the next setting).
While these users excluded the cumulative weight axis, through the fixed heights of the stacked sets the plots communicate that the total degree at each stage is the same.</p>
<p><strong>Censored data:</strong>
Finally, something i think alluvial plots do exceptionally better than general parallel sets plots is accentuate censoredness in data. Check out scholarly articles by <a href="https://www.sciencedirect.com/science/article/pii/S1075996418301021">Seekatz et al (2018)</a> and <a href="https://journals.lww.com/ccmjournal/Fulltext/2019/01000/Evaluating_Delivery_of_Low_Tidal_Volume.8.aspx">Sjoding, Gong, Hass, and Iwashyna (2019)</a> and <a href="https://mdneuzerling.com/post/my-data-science-job-hunt/">David Neuzerling’s reflections on the job hunt</a>, which use alluvial plots to depict changes in subjects’ status across several time points or stages with a specific set (or blank space where it would be) for subjects who became unavailable later in the study. The cumulative weight axis and gridlines allow the reader to immediately discern the reduction in sample size at each step.
(Though they use network data, <a href="https://www.nature.com/articles/srep06773">Lu and Brelsford (2014)</a> make similar use of this property to visualize non-connections between sets together with connections.)</p>
<p>That’s the substance of my argument for the alluvial–parallel sets distinction, but i’ll finish with a bit of fluorish.</p>
</div>
<div id="connotativity" class="section level3">
<h3>Connotativity</h3>
<p>The special features of alluvial plots are connoted by their peculiar terminology: I’ve decided to call the rectangles representing the parallel sets “strata” to suggest that they are more stable than the crisscrossing alluvia, and indeed this stability (with respect to their order in the plot) is what users expect when many of the categorical dimensions classify subjects into the same categories. The term also suggests, as does “alluvia”, that the various sets into which the subjects are partitioned at each (usually horizontal) position along the dimension axis are themselves (vertically) positioned in accordance with gravity. That is, they have “settled” one atop another with no defiantly empty space in between.</p>
<p>Thus i deposit my case.</p>
</div>
</div>
<div id="coda" class="section level2">
<h2>Coda</h2>
<p>I am by no means an expert in data visualization! While i feel strongly that my taxonomy makes the best of the present scrambling of terms, i am quite open to countersuggestions and especially to use cases that undercut it. If you come across them, please do send them my way.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I only just discovered Yawei Ge and Hofmann’s <a href="https://yaweige.github.io/ggpcp/">ggpcp</a> package for general parallel coordinate plots, under active development!<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>That’s not to say that useful distinctions haven’t been made somewhere, e.g. the technical literature on data visualization, but i feel confident in claiming that they have had limited effects on practice!<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>In ggplot2, these rules are the stat, geom, coord, and scale layers. One of the great contributions of ggplot2, in my view, was to make them explicit to lay users like myself.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>It’s unfortunate that i named the ribbons between adjacent axes <em>flows</em> in ggalluvial, but i maintain that it was preferable to calling them <em>fans</em>.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>A similar misfit is a simplicial complex represented by a network diagram: The type of diagram is designed for a different type of data (pairwise-relational with possible directedness and multiplicity) and fails to convey essential data elements (higher-dimensional simplices).<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>A contrary example is RAWGraphs, which <a href="https://rawgraphs.io/learning/how-to-make-an-alluvial-diagram/">describes alluvial plots</a> as “a specific kind of Sankey diagrams”.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>I found most of these examples by searching for “ggalluvial” in Google Scholar or Twitter.<a href="#fnref7" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>comparable pairwise and multivariate associations from presence-absence data</title>
      <link>/2019/08/30/presence-absence/</link>
      <pubDate>2019 Aug 30 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/30/presence-absence/</guid>
      <description><p>A project i’ve had in the works for years, and “almost done” <a href="https://www.siam.org/Conferences/CM/Conference/ns18">since last summer</a>, is a sensitivity and robustness analysis of several techniques used in studies of “comorbidity networks” (also “disease graphs”, “disease maps”, etc.) that have appeared over the past 20 years. As i begin the abstract:</p>
<blockquote>
<p>Comorbidity network analysis (CNA) is an increasingly popular approach in systems medicine, in which mathematical graphs encode epidemiological correlations (links) between diseases (nodes) inferred from their occurrence in an underlying patient population.</p>
</blockquote>
<p>An essential part of this project is a comparison of pairwise versus multivariate network construction. A <strong>pairwise</strong> (or, more generally, “motif-wise”) construction involves aggregating the network from links determined from some measure of association between pairs (or among motifs) of coded disorders. While many such measures are simply defined in terms of data, others, most notably correlation coefficients, are assumed to have latent values that must be estimated from data. In some studies, these estimates control for patient-level covariates such as age, sex, and ethnic group. A <strong>multivariate</strong> construction involves controlling these estimates for <em>other disorders</em>, whose various associations are also being estimated. (Multivariate constructions may but needn’t also control for patient-level covariates.)</p>
<p>To understand the problem of identifying suitable multivariate models, it’s important to know that the underlying data are binary, having the structure of presence–absence data.
<em>Presence–absence data</em> constitute a case–condition matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> whose each entry <span class="math inline">\(x_{ij}\in\{0,1\}\)</span> is one if case <span class="math inline">\(i\)</span> satisfies condition <span class="math inline">\(j\)</span> and zero if not. The term “presence–absence” derives from ecology, where the cases and conditions are sites and species and <span class="math inline">\(x_{ij}=1\)</span> indicates that species <span class="math inline">\(j\)</span> was observed at site <span class="math inline">\(i\)</span>.</p>
<p>A variety of binary association measures emerged in earlier ecology studies—check out <a href="https://www.researchgate.net/profile/Zdenek_Hubalek/publication/229695992_Coefficients_of_Association_and_Similarity_Based_on_Binary_Presence-Absence_Data_An_Evaluation/links/5a2e5ef445851552ae7f1ddc/Coefficients-of-Association-and-Similarity-Based-on-Binary-Presence-Absence-Data-An-Evaluation.pdf">the survey, taxonomy, and comparison by Hubálek from 1982</a>—and a handful, including the correlation coefficient <span class="math inline">\(\phi\)</span> (A<sub>30</sub>) attributed independently to Yule and to Pearson and Heron and Forbes’ coefficient of association (A<sub>40</sub>), made their way into comorbidity network analysis, together with the odds ratio more widely used in medical research. By and large, these statistics do not generalize to summaries of 3 or more variables; those that do tend to be correlation coefficients.</p>
<div id="multivariate-techniques-for-network-construction" class="section level3">
<h3>multivariate techniques for network construction</h3>
<p>I adopted two multivariate approaches to compare to the pairwise approach: One, which uses partial correlation coefficients, has been introduced in psychology and was the subject of <a href="https://arxiv.org/abs/1607.01367">a 2017 tutorial by Epskamp and Fried</a>. If <span class="math inline">\(m\)</span> variables <span class="math inline">\(y_i\mid 1\leq i\leq m\)</span> have standard deviations <span class="math inline">\(\sigma_i\)</span>, then the <em>full partial correlation</em> of <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is the standardized regression coefficient <span class="math inline">\(\displaystyle r&#39;_{ij}=\frac{\sigma_j}{\sigma_i}\beta_{ij}\)</span> from the linear model predicting <span class="math inline">\(y_i\)</span> from all other variables—or, equivalently, <span class="math inline">\(\displaystyle r&#39;_{ji}=\frac{\sigma_i}{\sigma_j}\beta_{ji}\)</span> from the model predicting <span class="math inline">\(y_j\)</span>. The <em>partial correlation network</em> <span class="math inline">\(G&#39;\)</span> is aggregated from the <span class="math inline">\(r&#39;_{ij}\)</span>. Since these correlations are controlled for the effects of other variables, <span class="math inline">\(G&#39;\)</span> should include far fewer <a href="https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/">transitive correlations</a> than a pairwise network.</p>
<p>Conveniently, partial correlations can be calculated from whatever pairwise correlations one begins with.</p>
<p>I also wanted to borrow from the current ecology literature. On my reading, ecology has always been at the cutting edge of multivariate statistical analysis, and lately there have been proposed several correlation and network models to overcome the well-documented limitations of pairwise techniques. These proposals have been motivated by a desire to capture a variety of interactions among species, geography, and environment. The reviews i’ve found aren’t recent enough to have surveyed the most interesting examples, which include <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.2007.0030-1299.16173.x">Ulrich &amp; Gotelli (2007)</a>, <a href="https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-0173.1">Ovaskainen, Hottola, &amp; Siitonen (2010)</a>, <a href="https://link.springer.com/article/10.1007%2Fs12080-015-0281-9">Cazelles, Araújo, Mouquet, &amp; Gravel (2016)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.01892">Morueta-Holme et al (2015)</a>.</p>
<p>Mostly because the authors included a tutorial for their method, implemented in R, in their supporting information, i adopted the <em>joint distribution model</em> (JDM) <a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12180">proposed by Pollock and colleagues</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Though the model was developed to handle both variable interactions (“endogenous” effects) and case-level covariates (“exogenous” effects), for the coming illustration i’m only concerned with endogenous information.</p>
<p>This simplified JDM assumes that each row of a <span class="math inline">\(0,1\)</span>-matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> is obtained from a latent multivariate normal distribution with center <span class="math inline">\(\vec\mu=[\,\mu_1\,\cdots\,\mu_m\,]\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. The <span class="math inline">\(\mu_j\)</span> encode the prevalences of the conditions while <span class="math inline">\(\Sigma\)</span> encodes their correlations. If <span class="math inline">\(Z\sim N(\vec\mu,\Sigma)\)</span>, then the rows of <span class="math inline">\(X\)</span> are assumed to have been generated from samples <span class="math inline">\(\vec z_i=[\,z_1\,\cdots\,z_m\,]\)</span> as <span class="math display">\[x_{ij}=\begin{cases} 0 &amp; z_{ij}\leq 0 \\ 1 &amp; z_{ij}&gt;0 \end{cases}\text.\]</span> The model is hierarchical with respect to the meta-parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> and is fit using Bayesian methods. I’ll denote the correlation matrix estimated this way as <span class="math inline">\(\hat{\mathrm{P}}=(\hat\rho_{ij})\)</span>, since Pollock and colleagues designate the underlying correlation matrix <span class="math inline">\(\mathrm{P}\)</span> (capital <span class="math inline">\(\rho\)</span>).</p>
</div>
<div id="a-comparable-pairwise-construction" class="section level3">
<h3>a comparable pairwise construction</h3>
<p>Since partial correlations can be calculated from any sample correlation data, a grand comparison of these three approaches now requires only a (pairwise) correlation coefficient that meaningfully compares to <span class="math inline">\(\hat\rho_{ij}\)</span>. The JDM relies on a latent multivariate normal, so my natural choice—once i’d found it—was a correlation coefficient based on a latent <em>bivariate</em> normal: the <em>tetrachoric correlation coefficient</em>, often denoted <span class="math inline">\(r_t\)</span>.</p>
<p>The setup for <span class="math inline">\(r_t\)</span> is a distribution <span class="math inline">\(N(\mu,\Sigma)\)</span> with means <span class="math inline">\(\vec\mu=[\,\mu_1,\mu_2\,]\)</span> and covariance <span class="math display">\[\Sigma=\left(\begin{array}{cc} \!{\sigma_1}^2\! &amp; \!\rho\sigma_1\sigma_2\! \\ \!\rho\sigma_1\sigma_2\! &amp; \!{\sigma_2}^2\! \end{array}\right)\text.\]</span>
The upshot is that, as in the JDM, the value of the statistic is the maximum-likelihood estimate of the latent correlation coefficient <span class="math inline">\(\rho\)</span>. Several estimation techniques are discussion in <a href="https://www.researchgate.net/publication/313196484_Polychoric_and_polyserial_correlations">Drasgow’s entry for the <em>Encyclopedia of Statistical Sciences</em></a>.</p>
<p>Implementations of all three statistics are available but require a bit of lead-in, so i’ll come back to this topic—and a grand comparison—in a future post.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Pollock and colleagues call this a “joint species distribution model”, but since i’m applying the method outside ecology, and since the structure of the model clarifies which distributions its name refers to, i’m leaving out the domain-specific qualifier.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><p>A project i’ve had in the works for years, and “almost done” <a href="https://www.siam.org/Conferences/CM/Conference/ns18">since last summer</a>, is a sensitivity and robustness analysis of several techniques used in studies of “comorbidity networks” (also “disease graphs”, “disease maps”, etc.) that have appeared over the past 20 years. As i begin the abstract:</p>
<blockquote>
<p>Comorbidity network analysis (CNA) is an increasingly popular approach in systems medicine, in which mathematical graphs encode epidemiological correlations (links) between diseases (nodes) inferred from their occurrence in an underlying patient population.</p>
</blockquote>
<p>An essential part of this project is a comparison of pairwise versus multivariate network construction. A <strong>pairwise</strong> (or, more generally, “motif-wise”) construction involves aggregating the network from links determined from some measure of association between pairs (or among motifs) of coded disorders. While many such measures are simply defined in terms of data, others, most notably correlation coefficients, are assumed to have latent values that must be estimated from data. In some studies, these estimates control for patient-level covariates such as age, sex, and ethnic group. A <strong>multivariate</strong> construction involves controlling these estimates for <em>other disorders</em>, whose various associations are also being estimated. (Multivariate constructions may but needn’t also control for patient-level covariates.)</p>
<p>To understand the problem of identifying suitable multivariate models, it’s important to know that the underlying data are binary, having the structure of presence–absence data.
<em>Presence–absence data</em> constitute a case–condition matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> whose each entry <span class="math inline">\(x_{ij}\in\{0,1\}\)</span> is one if case <span class="math inline">\(i\)</span> satisfies condition <span class="math inline">\(j\)</span> and zero if not. The term “presence–absence” derives from ecology, where the cases and conditions are sites and species and <span class="math inline">\(x_{ij}=1\)</span> indicates that species <span class="math inline">\(j\)</span> was observed at site <span class="math inline">\(i\)</span>.</p>
<p>A variety of binary association measures emerged in earlier ecology studies—check out <a href="https://www.researchgate.net/profile/Zdenek_Hubalek/publication/229695992_Coefficients_of_Association_and_Similarity_Based_on_Binary_Presence-Absence_Data_An_Evaluation/links/5a2e5ef445851552ae7f1ddc/Coefficients-of-Association-and-Similarity-Based-on-Binary-Presence-Absence-Data-An-Evaluation.pdf">the survey, taxonomy, and comparison by Hubálek from 1982</a>—and a handful, including the correlation coefficient <span class="math inline">\(\phi\)</span> (A<sub>30</sub>) attributed independently to Yule and to Pearson and Heron and Forbes’ coefficient of association (A<sub>40</sub>), made their way into comorbidity network analysis, together with the odds ratio more widely used in medical research. By and large, these statistics do not generalize to summaries of 3 or more variables; those that do tend to be correlation coefficients.</p>
<div id="multivariate-techniques-for-network-construction" class="section level3">
<h3>multivariate techniques for network construction</h3>
<p>I adopted two multivariate approaches to compare to the pairwise approach: One, which uses partial correlation coefficients, has been introduced in psychology and was the subject of <a href="https://arxiv.org/abs/1607.01367">a 2017 tutorial by Epskamp and Fried</a>. If <span class="math inline">\(m\)</span> variables <span class="math inline">\(y_i\mid 1\leq i\leq m\)</span> have standard deviations <span class="math inline">\(\sigma_i\)</span>, then the <em>full partial correlation</em> of <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is the standardized regression coefficient <span class="math inline">\(\displaystyle r&#39;_{ij}=\frac{\sigma_j}{\sigma_i}\beta_{ij}\)</span> from the linear model predicting <span class="math inline">\(y_i\)</span> from all other variables—or, equivalently, <span class="math inline">\(\displaystyle r&#39;_{ji}=\frac{\sigma_i}{\sigma_j}\beta_{ji}\)</span> from the model predicting <span class="math inline">\(y_j\)</span>. The <em>partial correlation network</em> <span class="math inline">\(G&#39;\)</span> is aggregated from the <span class="math inline">\(r&#39;_{ij}\)</span>. Since these correlations are controlled for the effects of other variables, <span class="math inline">\(G&#39;\)</span> should include far fewer <a href="https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/">transitive correlations</a> than a pairwise network.</p>
<p>Conveniently, partial correlations can be calculated from whatever pairwise correlations one begins with.</p>
<p>I also wanted to borrow from the current ecology literature. On my reading, ecology has always been at the cutting edge of multivariate statistical analysis, and lately there have been proposed several correlation and network models to overcome the well-documented limitations of pairwise techniques. These proposals have been motivated by a desire to capture a variety of interactions among species, geography, and environment. The reviews i’ve found aren’t recent enough to have surveyed the most interesting examples, which include <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.2007.0030-1299.16173.x">Ulrich &amp; Gotelli (2007)</a>, <a href="https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-0173.1">Ovaskainen, Hottola, &amp; Siitonen (2010)</a>, <a href="https://link.springer.com/article/10.1007%2Fs12080-015-0281-9">Cazelles, Araújo, Mouquet, &amp; Gravel (2016)</a>, and <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.01892">Morueta-Holme et al (2015)</a>.</p>
<p>Mostly because the authors included a tutorial for their method, implemented in R, in their supporting information, i adopted the <em>joint distribution model</em> (JDM) <a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12180">proposed by Pollock and colleagues</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Though the model was developed to handle both variable interactions (“endogenous” effects) and case-level covariates (“exogenous” effects), for the coming illustration i’m only concerned with endogenous information.</p>
<p>This simplified JDM assumes that each row of a <span class="math inline">\(0,1\)</span>-matrix <span class="math inline">\(X\in\{0,1\}^{n\times m}\)</span> is obtained from a latent multivariate normal distribution with center <span class="math inline">\(\vec\mu=[\,\mu_1\,\cdots\,\mu_m\,]\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. The <span class="math inline">\(\mu_j\)</span> encode the prevalences of the conditions while <span class="math inline">\(\Sigma\)</span> encodes their correlations. If <span class="math inline">\(Z\sim N(\vec\mu,\Sigma)\)</span>, then the rows of <span class="math inline">\(X\)</span> are assumed to have been generated from samples <span class="math inline">\(\vec z_i=[\,z_1\,\cdots\,z_m\,]\)</span> as <span class="math display">\[x_{ij}=\begin{cases} 0 &amp; z_{ij}\leq 0 \\ 1 &amp; z_{ij}&gt;0 \end{cases}\text.\]</span> The model is hierarchical with respect to the meta-parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> and is fit using Bayesian methods. I’ll denote the correlation matrix estimated this way as <span class="math inline">\(\hat{\mathrm{P}}=(\hat\rho_{ij})\)</span>, since Pollock and colleagues designate the underlying correlation matrix <span class="math inline">\(\mathrm{P}\)</span> (capital <span class="math inline">\(\rho\)</span>).</p>
</div>
<div id="a-comparable-pairwise-construction" class="section level3">
<h3>a comparable pairwise construction</h3>
<p>Since partial correlations can be calculated from any sample correlation data, a grand comparison of these three approaches now requires only a (pairwise) correlation coefficient that meaningfully compares to <span class="math inline">\(\hat\rho_{ij}\)</span>. The JDM relies on a latent multivariate normal, so my natural choice—once i’d found it—was a correlation coefficient based on a latent <em>bivariate</em> normal: the <em>tetrachoric correlation coefficient</em>, often denoted <span class="math inline">\(r_t\)</span>.</p>
<p>The setup for <span class="math inline">\(r_t\)</span> is a distribution <span class="math inline">\(N(\mu,\Sigma)\)</span> with means <span class="math inline">\(\vec\mu=[\,\mu_1,\mu_2\,]\)</span> and covariance <span class="math display">\[\Sigma=\left(\begin{array}{cc} \!{\sigma_1}^2\! &amp; \!\rho\sigma_1\sigma_2\! \\ \!\rho\sigma_1\sigma_2\! &amp; \!{\sigma_2}^2\! \end{array}\right)\text.\]</span>
The upshot is that, as in the JDM, the value of the statistic is the maximum-likelihood estimate of the latent correlation coefficient <span class="math inline">\(\rho\)</span>. Several estimation techniques are discussion in <a href="https://www.researchgate.net/publication/313196484_Polychoric_and_polyserial_correlations">Drasgow’s entry for the <em>Encyclopedia of Statistical Sciences</em></a>.</p>
<p>Implementations of all three statistics are available but require a bit of lead-in, so i’ll come back to this topic—and a grand comparison—in a future post.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Pollock and colleagues call this a “joint species distribution model”, but since i’m applying the method outside ecology, and since the structure of the model clarifies which distributions its name refers to, i’m leaving out the domain-specific qualifier.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>multidimensional scaling of variables, and rank correlations</title>
      <link>/2019/08/16/rank-correlations/</link>
      <pubDate>2019 Aug 16 (Fri), 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/16/rank-correlations/</guid>
      <description><p>A fundamental idea in biplot methodology is the <em>conference of inertia</em>, a phrase i picked up from <a href="https://stats.stackexchange.com/a/141755/68743">an SO answer by ttnphns</a> and quickly <a href="https://github.com/corybrunson/ordr/blob/master/R/ord-conference.r">incorporated into ordr</a>. The basic idea arises from the central properties of a biplot, illustrated here for principal components analysis: A case–variable data matrix <span class="math inline">\(X\in\mathbb{R}^{n\times m}\)</span> of ratio variables<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> is singular-value decomposed as <span class="math inline">\(X=UDV^\top\)</span>, for example the <code>mtcars</code> data set:</p>
<pre class="r"><code>x &lt;- scale(mtcars, center = TRUE, scale = TRUE)
s &lt;- svd(x)
r &lt;- length(s$d)</code></pre>
<p>Under this convention, <span class="math inline">\(U\in\mathbb{R}^{n\times r}\)</span> and <span class="math inline">\(V\in\mathbb{R}^{m\times r}\)</span> arise from eigendecompositions of <span class="math inline">\(XX^\top\)</span> and of <span class="math inline">\(X^\top X\)</span>, respectively, and <span class="math inline">\(D\in\mathbb{R}^{r\times r}\)</span> is the diagonal matrix of the square roots of their (common) eigenvalues. The matrix factors may be biplotted in three conventional ways:</p>
<ul>
<li>with <em>principal</em> case coordinates <span class="math inline">\(UD\)</span> and <em>standardized</em> variable coordinates <span class="math inline">\(V\)</span>;</li>
<li>with standardized case coordinates <span class="math inline">\(U\)</span> and principal variable coordinates <span class="math inline">\(VD\)</span>;</li>
<li>with <em>symmetric</em> case and variable coordinates <span class="math inline">\(UD^{1/2}\)</span> and <span class="math inline">\(VD^{1/2}\)</span>.</li>
</ul>
<p>Because both sets of eigenvectors <span class="math inline">\(U=\left[\,u_1\,\cdots\,u_r\,\right]\)</span> and <span class="math inline">\(V=\left[\,v_1\,\cdots\,v_r\,\right]\)</span> are orthonormal, <span class="math inline">\(U^\top U=I_r=V^\top V\)</span> and the total inertia (variance) in each matrix is <span class="math inline">\(\sum_{j=1}^{r}{ {v_j}^2 }=r=\sum_{j=1}^{r}{ {v_j}^2 }\)</span>. Meanwhile, <span class="math inline">\(D\)</span> contains all of the inertia of <span class="math inline">\(X\)</span>:</p>
<pre class="r"><code># inertia of the (scaled) data
sum(x^2)</code></pre>
<pre><code>## [1] 341</code></pre>
<pre class="r"><code># inertia of the case and variable factors
sum(s$u^2)</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code>sum(s$v^2)</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code># inertia of the diagonal factor
sum(s$d^2)</code></pre>
<pre><code>## [1] 341</code></pre>
<p>This inertia can then be <em>conferred</em> unto the standardized case or variable coordinates, transforming one or the other into principal coordinates (the first two options above) or both halfway there (the symmetric option). Each of these options confers the inertia in such a way that the sums of the exponents of <span class="math inline">\(D\)</span> in the transformed sets of case (<span class="math inline">\(F=UD^p\)</span>) and variable (<span class="math inline">\(G=VD^q\)</span>) coordinates is <span class="math inline">\(p+q=1\)</span>, which ensures the <em>inner product property</em> <span class="math inline">\(FG^\top=X\)</span> between them. This recovers any entry <span class="math inline">\(x_{ij}\)</span> of <span class="math inline">\(X\)</span> as the inner product <span class="math inline">\(f_i\cdot g_i\)</span> of its case and variable coordinates <span class="math inline">\(f_i=[\,f_{i,1}\,\cdots\,f_{i,r}\,]\)</span> and <span class="math inline">\(g_i=[\,g_{i,1}\,\cdots\,g_{i,r}\,]\)</span>.</p>
<p>By conferring the inertia entirely to the cases or to the variables, we preserve (or best approximate) the geometric configurations of the cases or of the variables. In PCA, the geometry of the cases is usually construed as the distances between them. Here their pairwise distances <span class="math inline">\(\sqrt{(f_{j,1}-f_{i,1})^2+(f_{j,2}-f_{i,2})^2}\)</span> in the first two PCA dimensions are plotted against their “true” distances <span class="math inline">\(\left(\sum_{k=1}^{m}{(x_{j,k}-x_{i,k})^2}\right)^{1/2}\)</span> in the variable space:</p>
<pre class="r"><code># distances between cases
x.dist &lt;- dist(x)
# distances between cases (principal coordinates)
s.dist &lt;- dist(s$u[, 1:2] %*% diag(s$d[1:2]))
# scatterplot
plot(
  x = as.vector(x.dist), y = as.vector(s.dist),
  asp = 1, pch = 19, cex = .5,
  xlab = &quot;Case distances along variable coordinates&quot;,
  ylab = &quot;Case distances in two principal coordinates&quot;
)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/case%20geometry-1.png" width="528" /></p>
<p>Meanwhile, the geometry of the variables is usually understood through their covariances or correlations. Writing <span class="math inline">\(X=[\,y_1\,\cdots\,y_m\,]\)</span> as an array of column variables, the covariance between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is proportional to their inner product <span class="math display">\[\textstyle \operatorname{cov}(y_i,y_j)=\frac{1}{n}y_i\cdot y_j=\frac{1}{n}\lVert y_i\rVert\lVert y_j\rVert\cos\theta_{ij}\text,\]</span> so that the cosine of the angle <span class="math inline">\(\theta_{ij}\)</span> between them equals their correlation:
<span class="math display">\[\cos\theta_{ij}=\frac{\operatorname{cov}(y_i,y_j)}{\sqrt{\operatorname{cov}(y_i,y_i)\operatorname{cov}(y_j,y_j)}/n}=\frac{\operatorname{cov}(y_i,y_j)}{\sigma_i\sigma_j}=r_{ij}\]</span>
Here the cosines <span class="math inline">\(\frac{g_i}{\lVert g_i\rVert}\cdot\frac{g_j}{\lVert g_j\rVert}\)</span> between the variable vectors in the first two PCA dimensions are plotted against their correlations <span class="math inline">\(r_{ij}\)</span> across the original cases:</p>
<pre class="r"><code># correlations between variables
x.cor &lt;- cor(x)
# magnitudes of variable vectors
s.len &lt;- apply(s$v[, 1:2] %*% diag(s$d[1:2]), 1, norm, &quot;2&quot;)
# cosines between variables (principal coordinates)
s.cor &lt;- (s$v[, 1:2] / s.len) %*% diag(s$d[1:2]^2) %*% t(s$v[, 1:2] / s.len)
# scatterplot
plot(
  x = as.vector(x.cor[lower.tri(x.cor)]),
  y = as.vector(s.cor[lower.tri(s.cor)]),
  asp = 1, pch = 19, cex = .5,
  xlab = &quot;Variable correlations among cases&quot;,
  ylab = &quot;Cosines between variable vectors in two principal coordinates&quot;
)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/variable%20geometry-1.png" width="528" /></p>
<div id="multidimensional-scaling-of-variables" class="section level2">
<h2>multidimensional scaling of variables</h2>
<p>The faithful approximation of inter-case distances by principal coordinates is the idea behind <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling">(classical)</a> <em>multidimensional scaling</em> (MDS), which can be applied to a data set of distances <span class="math inline">\(\delta_{ij},\ 1\leq i,j\leq n\)</span> in the absence of coordinates. This technique is based on the eigendecomposition of a doubly-centered matrix of squared distances, which produces matrix <span class="math inline">\(U\Lambda^{1/2}\)</span> whose first <span class="math inline">\(r\)</span> coordinates—for any <span class="math inline">\(r\leq n\)</span>—recover a best approximation of the inter-case distances in terms of the sum of squared errors, i.e. the variance of <span class="math inline">\((U\Lambda^{1/2})(U\Lambda^{1/2})^\top-\Delta=U\Lambda U^\top-\Delta\)</span>, where <span class="math inline">\(\Delta=(\delta_{ij})\in\mathbb{R}^{n\times n}\)</span>. In practice, the goal is usually to position points representing the <span class="math inline">\(n\)</span> cases in a 2-dimensional scatterplot so that their distances <span class="math inline">\(\sqrt{(x_j-x_i)^2+(y_j-y_i)^2}\)</span> approximate their original distances <span class="math inline">\(\delta_{ij}\)</span>, as in this example using road distances between U.S. cities to approximate their geographic arrangement:</p>
<pre class="r"><code>d &lt;- as.matrix(UScitiesD)
cent &lt;- diag(1, nrow(d)) - matrix(1/nrow(d), nrow(d), nrow(d))
d.cent &lt;- -.5 * cent %*% (d^2) %*% cent
d.mds &lt;- svd(d.cent)
d.coord &lt;- d.mds$u[, 1:2] %*% diag(sqrt(d.mds$d[1:2]))
plot(d.coord, pch = NA, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;)
text(d.coord, labels = rownames(d))</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/multidimensional%20scaling-1.png" width="816" /></p>
<p>The faithful approximation of inter-variable covariances by the inner products of their principal coordinate vectors suggests a complementary technique that i haven’t found explicitly discussed in my own background reading. Suppose we have data that consist not of distances between cases but of covariances <span class="math inline">\(\operatorname{cov}(y_i,y_j),\ 1\leq i,j\leq m\)</span> between variables. Again the data are coordinate-free, so PCA cannot be applied. Were the data to have derived from a <em>centered</em> case–variable matrix <span class="math inline">\(X\)</span>, then the covariance matrix <span class="math inline">\(C=(\operatorname{cov}(y_i,y_j))\)</span> would have been obtained as <span class="math inline">\(C=\frac{1}{n}X^\top X\)</span>, which is (up to scalar) the matrix whose eigenvectors would be given by <span class="math inline">\(V\)</span> in the SVD <span class="math inline">\(X=UDV^\top\)</span>. Therefore, we can fabricate coordinates for the <span class="math inline">\(m\)</span> variables that approximate what we know of their geometry—in this case, thinking of the variables as unknown vectors, whose magnitudes and pairwise angles are encoded in <span class="math inline">\(C\)</span>—via an eigendecomposition <span class="math inline">\(C=V\Lambda V^\top\)</span>: Take <span class="math inline">\(Y=V\Lambda^{1/2}\in\mathbb{R}^{m\times r}\)</span>, so that <span class="math inline">\(Y^\top Y\approx C\)</span>.</p>
<p>I’ll validate this line of reasoning by taking the <code>mtcars</code> data set for a spin:</p>
<pre class="r"><code># covariances and standard deviations
c &lt;- cov(mtcars)
s &lt;- diag(sqrt(diag(c)))
# centered data
x &lt;- as.matrix(scale(mtcars, center = TRUE, scale = FALSE))
# eigendecomposition of covariance matrix
c.eigen &lt;- eigen(c)
# artificial coordinates
c.coord &lt;- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
# validate covariance recovery (up to sign)
all.equal(
  as.vector(c.coord %*% t(c.coord)),
  as.vector(c),
  tolerance = 1e-12
)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Thus, whereas <em>MDS of cases</em> is used to represent distances, <em>MDS of variables</em> can be used to represent covariances.
A use case for this technique is a situation in which covariance data exist without variable values. This may of course be the case because original data has become unavailable.</p>
<p>A more interesting setting that gives rise to this situation is the analysis of multiple rankings of the same set of objects in terms of their <em>concordance</em>. Rankings’ concordance is often measured using rank correlations such as Kendall’s <span class="math inline">\(\tau\)</span>, which may be <em>general correlation coefficients</em> in <a href="https://en.wikipedia.org/wiki/Rank_correlation#General_correlation_coefficient">the sense proposed by Kendall</a> but are not associated with an underlying (Euclidean) geometry. Nevertheless, we can use MDS to represent these rankings as unit vectors in Euclidean space whose pairwise cosines approximate their rank correlations!</p>
</div>
<div id="example-rankings-of-universities" class="section level2">
<h2>example: rankings of universities</h2>
<p>A real-world example is provided by the <a href="https://www.topuniversities.com/qs-world-university-rankings/methodology">Quacquarelli Symonds Top University Rankings</a>, which include rankings of hundreds of world universities on six “metrics”: academic reputation, employer reputation, faculty–student ratio, citations per faculty, international faculty ratio, and international student ratio. QS weight these rankings differently in their overall assessment, but our analysis will compare the rankings to each other, so these weights are irrelevant. I restricted the data from the year 2020 to universities in the United States for which integer rankings (i.e. not “400+” placeholders) were available in all four years:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<pre class="r"><code>qswurus20 &lt;- readRDS(here::here(&quot;supplementary/qswurus20.rds&quot;))
head(qswurus20)</code></pre>
<pre><code>##   year                                  institution size focus res age
## 1 2020  MASSACHUSETTS INSTITUTE OF TECHNOLOGY (MIT)    M    CO  VH   5
## 2 2020                          STANFORD UNIVERSITY    L    FC  VH   5
## 3 2020                           HARVARD UNIVERSITY    L    FC  VH   5
## 4 2020 CALIFORNIA INSTITUTE OF TECHNOLOGY (CALTECH)    S    CO  VH   5
## 5 2020                        UNIVERSITY OF CHICAGO    L    FC  VH   5
## 6 2020                         PRINCETON UNIVERSITY    M    CO  VH   5
##   status rk_academic rk_employer rk_ratio rk_citations rk_intl_faculty
## 1      B           5           4       15            7              43
## 2      B           4           5       12           13              62
## 3      B           1           1       40            8             186
## 4      B          23          74        4            4              72
## 5      B          13          37       54           60             249
## 6      B          10          19      192            3             272
##   rk_intl_students
## 1               87
## 2              196
## 3              221
## 4              121
## 5              143
## 6              197</code></pre>
<p>Since the integer rankings were subsetted from the full international data set, they are not contiguous (i.e. some integers between rankings never appear). To resolve this, i’ll recalibrate the rankings by matching each vector of ranks to the vector of its sorted unique values:</p>
<pre class="r"><code>library(dplyr)
qswurus20 %&gt;%
  select(institution, starts_with(&quot;rk_&quot;)) %&gt;%
  mutate_at(
    vars(starts_with(&quot;rk_&quot;)),
    ~ match(., sort(unique(as.numeric(.))))
  ) %&gt;%
  print() -&gt; qswurus20</code></pre>
<pre><code>## # A tibble: 38 x 7
##    institution rk_academic rk_employer rk_ratio rk_citations
##    &lt;chr&gt;             &lt;int&gt;       &lt;int&gt;    &lt;int&gt;        &lt;int&gt;
##  1 MASSACHUSE…           3           2        6            3
##  2 STANFORD U…           2           3        4            5
##  3 HARVARD UN…           1           1       11            4
##  4 CALIFORNIA…          12          14        1            2
##  5 UNIVERSITY…           9          11       13           12
##  6 PRINCETON …           7           7       18            1
##  7 CORNELL UN…          11          13       22            7
##  8 UNIVERSITY…          14          12        8           17
##  9 YALE UNIVE…           6           4        2           24
## 10 COLUMBIA U…           8           8        7           25
## # … with 28 more rows, and 2 more variables: rk_intl_faculty &lt;int&gt;,
## #   rk_intl_students &lt;int&gt;</code></pre>
<p>This subset of universities is now contiguously ranked along the six dimensions described above. The Kendall correlation <span class="math inline">\(\tau_{ij}\)</span> between two rankings measures their concordance. To calculate it, every pair of universities contributes either <span class="math inline">\(+1\)</span> or <span class="math inline">\(-1\)</span> according as the rankings <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> place that pair in the same order, and the sum is scaled down by the number of pairs <span class="math inline">\({n\choose 2}\)</span> so that the result lies between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. We interpret <span class="math inline">\(\tau_{ij}=1\)</span> as perfect concordance (the rankings are equivalent), <span class="math inline">\(\tau_{ij}=-1\)</span> as perfect discordance (the rankings are reversed), and <span class="math inline">\(\tau_{ij}=0\)</span> as independence (the rankings are unrelated).</p>
<p>The QS rankings are not variations on a theme, like different measures of guideline adherence or positive affect, but they do all seem potentially sensitive to a university’s resources, including funding and prestige. I intuit that the two reputational metrics should be positively correlated, and that the two international ratios should be as well. I also wonder if the faculty–student ratio might be anti-correlated with the number of citations per faculty, separating more research-focused institutions from more teaching-focused ones.</p>
<div id="correlation-heatmap" class="section level3">
<h3>correlation heatmap</h3>
<p>A common way to visualize correlation matrices is the heatmap, so i’ll use that technique first (see below). While the rankings by academic and employer reputations are highly concordant, those by international faculty and student ratios are less so; and, the faculty–student ratio and faculty citation rankings have the weakest concordance of all, but are nevertheless positively correlated.</p>
<pre class="r"><code>c &lt;- cor(select(qswurus20, starts_with(&quot;rk_&quot;)), method = &quot;kendall&quot;)
corrplot::corrplot(c)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/Kendall%20rank%20correlations-1.png" width="672" /></p>
<p>This visualization is useful, but it’s very busy: To compare any pair of rankings, i have to find the cell in the grid corresponding to that pair and refer back to the color scale to assess its meaning. I can’t rely on the nearby cells for context, because they may be stronger or weaker than average and skew my interpretation. For example, the visibly weak associations between the faculty–student ratio and other rankings (the third row or column) happen to be arranged so that the slightly stronger among them, with the two reputational variables, are sandwiched between the <em>even stronger</em> associations between the two reputational rankings and between them and the faculty citations ranking; whereas its weaker associations are sandwiched between more typical, but still comparatively stronger, associations. A different ordering of the variables might “obscure” this pattern and “reveal” others.</p>
<p>The plot is also strictly pairwise: Every correlation between two rankings occupies its own cell—two, in fact, making almost half of the plot duplicative. This means that a subset analysis of, say, three rankings requires focusing on three cells at the corners of a right triangle while ignoring all the surrounding cells. This is not an easy visual task. It would be straightforward to create a new plot for any subset, but then the larger context of the remaining rankings would be lost.</p>
</div>
<div id="correlation-biplot" class="section level3">
<h3>correlation biplot</h3>
<p>MDS of variables offers a natural alternative visualization: the biplot. As with MDS of cases, the point isn’t to overlay the case scores and variable loadings from a singular value decomposition, but to use the scores or loadings alone to endow the cases or variables with a Euclidean geometry they didn’t yet have. To that end, i’ll plot the variables as vectors with tails at the origin and heads at their fabricated coordinates <span class="math inline">\(Y=V\Lambda^{1/2}\)</span>:</p>
<pre class="r"><code>c.eigen &lt;- eigen(c)
c.coord &lt;- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
plot(c.coord, pch = NA, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;)
arrows(0, 0, c.coord[, 1], c.coord[, 2])
text(c.coord, labels = rownames(c))</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/multidimensional%20scaling%20of%20variables-1.png" width="672" /></p>
<p>A more elegant ggplot2-style graphic can be rendered with <a href="https://github.com/corybrunson/ordr">ordr</a>, with a unit circle included for reference:</p>
<pre class="r"><code>library(ordr)
eigen_ord(c) %&gt;%
  as_tbl_ord() %&gt;%
  augment() %&gt;%
  mutate_u(metric = stringr::str_remove(.name, &quot;rk_&quot;)) %&gt;%
  confer_inertia(1) %&gt;%
  negate_to_nonneg_orthant(&quot;u&quot;) -&gt;
  c_eigen
c_eigen %&gt;%
  ggbiplot() +
  theme_minimal() +
  geom_unit_circle() +
  geom_u_vector() +
  geom_u_text_radiate(aes(label = metric)) +
  scale_x_continuous(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .2)) +
  ggtitle(&quot;MDS of Kendall correlations between university rankings&quot;)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>With respect to the pairwise correlations, the biplot is significantly less precise: Though the vectors all have unit length in <span class="math inline">\(\mathbb{R}^r\)</span> (<span class="math inline">\(r\leq m=6\)</span>), their projections onto the first two principal coordinates are much shorter, indicating that much of the geometric configuration requires additional dimensions to represent. Indeed, these coordinates capture only <span class="math inline">\(48.2\%+14.3\%=62.5\%\)</span> of the inertia in the full representation. This means that the angles between the vectors must be interpreted with caution: For example, it looks like the academic and employer reputation rankings are extremely correlated, but the apparent alignment of the vectors could be an artifact of the projection, when in fact they “rise” and “fall” in opposite directions along the remaining dimensions. The correlation heatmap leaves no such ambiguity.</p>
<p>However, the biplot far surpasses the heatmap at parsimony: Each variable is represented by a single vector, and the angle cosines between the variable vectors roughly approximate their correlations. For instance, the rankings based on international student and faculty ratios have correlation around <span class="math inline">\(\cos(\frac{\pi}{4})=\frac{1}{\sqrt{2}}\)</span>, corresponding to either explaining half the “variance” in the other—not technically meaningful in the ranking context but a useful conceptual anchor. Meanwhile, the faculty–student ratio ranking is nearly independent of the faculty citation ranking, contrary to my intuition that these rankings would reflect a <em>reverse</em> association between research- and teaching-oriented institutions. The convenience of recognizing correlations as cosines may be worth the significant risk of error, especially since that error (the residual <span class="math inline">\(37.5\%\)</span> of inertia) can be exactly quantified.</p>
<p>Moreover, the principal coordinates of the variable vectors indicate their loadings onto the first and second principal moments of inertia—the two dimensions that capture the most variation in the data. For example, the first principal coordinate is most aligned with the two reputational rankings, suggesting that a general prestige ranking is the strongest overall component of the several specific rankings. In contrast, the faculty–student ratio and faculty citation rankings load most strongly onto the second principal coordinate, suggesting that the divide between research- and teaching-focused institutions may yet be important to understanding how universities compare along these different metrics. These observations, provisional though they are, would be difficult to discern from the heatmap. More importantly, unlike the secondary patterns visible in the heatmap, these are in no sense artifacts of the layout but arise directly from the (correlational) data.</p>
<p>This last point means that observations made from a biplot can be validated from the MDS coordinates. In particular, we can examine the variables’ loadings onto the third principal coordinate, and we can check whether the reputational rankings are aligned or misaligned along it.</p>
<pre class="r"><code>c_eigen %&gt;%
  tidy(.matrix = &quot;u&quot;) %&gt;%
  select(-.name, -.matrix)</code></pre>
<pre><code>## # A tibble: 6 x 7
##     EV1     EV2     EV3     EV4     EV5      EV6 metric       
##   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        
## 1 0.834 -0.0907 -0.412   0.0430  0.0206 -0.351   academic     
## 2 0.795 -0.0964 -0.477  -0.0416  0.181   0.311   employer     
## 3 0.517  0.771   0.0480  0.331  -0.158   0.0372  ratio        
## 4 0.731 -0.352   0.239  -0.0278 -0.528   0.0685  citations    
## 5 0.631 -0.233   0.521   0.392   0.352  -0.00783 intl_faculty 
## 6 0.603  0.262   0.324  -0.665   0.140  -0.0312  intl_students</code></pre>
<pre class="r"><code>c_eigen %&gt;%
  tidy(.matrix = &quot;coord&quot;)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   .name .values .inertia .prop_var
##   &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 EV1     2.89     2.89     0.482 
## 2 EV2     0.858    0.858    0.143 
## 3 EV3     0.833    0.833    0.139 
## 4 EV4     0.709    0.709    0.118 
## 5 EV5     0.480    0.480    0.0799
## 6 EV6     0.227    0.227    0.0379</code></pre>
<p>Based on the third principal coordinates, the reputational rankings are aligned, as we knew already from the correlation matrix and heatmap. What’s a bit more interesting is that this component seems to separate these two rankings from those having to do with faculty citation rates and the international compositions of the faculty and student body. Based on the decomposition of inertia, this third principal coordinate is nearly as important as the second! It therefore makes sense to plot the two together:</p>
<pre class="r"><code>c_eigen %&gt;%
  ggbiplot(aes(x = 2, y = 3)) +
  theme_minimal() +
  geom_unit_circle() +
  geom_u_vector() +
  geom_u_text_radiate(aes(label = metric)) +
  scale_x_continuous(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .4)) +
  ggtitle(&quot;MDS of Kendall correlations between university rankings&quot;,
          &quot;Second and third principal coordinates&quot;)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The primary antitheses of the reputational rankings, after removing the first principal coordinate, are the two rankings based on international composition—and this axis is largely independent of the axis apparently distinguishing research- from teaching-oriented institutions. From my own limited knowledge, i’d hazard a guess that this reflects two tiers of international representation among students and faculty, one expressed by the most prestigious institutions that recruit highly qualified applicants from all over the world, and the other expressed by institutions that are not especially prestigious but are located in communities or regions with high percentages of international residents.</p>
<p>This is of course no more than idle speculation on my part! But a visualization scheme that encourages hypothesis generation is worth having on hand.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>That is to say, if the variables don’t have meaningful zero values and/or commensurate scales, then they should be centered to zero mean and/or scaled to unit variance.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This leaves us with only 38 universities, so my inferences must be taken with extreme caution!<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
</description>
      <content:encoded><p>A fundamental idea in biplot methodology is the <em>conference of inertia</em>, a phrase i picked up from <a href="https://stats.stackexchange.com/a/141755/68743">an SO answer by ttnphns</a> and quickly <a href="https://github.com/corybrunson/ordr/blob/master/R/ord-conference.r">incorporated into ordr</a>. The basic idea arises from the central properties of a biplot, illustrated here for principal components analysis: A case–variable data matrix <span class="math inline">\(X\in\mathbb{R}^{n\times m}\)</span> of ratio variables<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> is singular-value decomposed as <span class="math inline">\(X=UDV^\top\)</span>, for example the <code>mtcars</code> data set:</p>
<pre class="r"><code>x &lt;- scale(mtcars, center = TRUE, scale = TRUE)
s &lt;- svd(x)
r &lt;- length(s$d)</code></pre>
<p>Under this convention, <span class="math inline">\(U\in\mathbb{R}^{n\times r}\)</span> and <span class="math inline">\(V\in\mathbb{R}^{m\times r}\)</span> arise from eigendecompositions of <span class="math inline">\(XX^\top\)</span> and of <span class="math inline">\(X^\top X\)</span>, respectively, and <span class="math inline">\(D\in\mathbb{R}^{r\times r}\)</span> is the diagonal matrix of the square roots of their (common) eigenvalues. The matrix factors may be biplotted in three conventional ways:</p>
<ul>
<li>with <em>principal</em> case coordinates <span class="math inline">\(UD\)</span> and <em>standardized</em> variable coordinates <span class="math inline">\(V\)</span>;</li>
<li>with standardized case coordinates <span class="math inline">\(U\)</span> and principal variable coordinates <span class="math inline">\(VD\)</span>;</li>
<li>with <em>symmetric</em> case and variable coordinates <span class="math inline">\(UD^{1/2}\)</span> and <span class="math inline">\(VD^{1/2}\)</span>.</li>
</ul>
<p>Because both sets of eigenvectors <span class="math inline">\(U=\left[\,u_1\,\cdots\,u_r\,\right]\)</span> and <span class="math inline">\(V=\left[\,v_1\,\cdots\,v_r\,\right]\)</span> are orthonormal, <span class="math inline">\(U^\top U=I_r=V^\top V\)</span> and the total inertia (variance) in each matrix is <span class="math inline">\(\sum_{j=1}^{r}{ {v_j}^2 }=r=\sum_{j=1}^{r}{ {v_j}^2 }\)</span>. Meanwhile, <span class="math inline">\(D\)</span> contains all of the inertia of <span class="math inline">\(X\)</span>:</p>
<pre class="r"><code># inertia of the (scaled) data
sum(x^2)</code></pre>
<pre><code>## [1] 341</code></pre>
<pre class="r"><code># inertia of the case and variable factors
sum(s$u^2)</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code>sum(s$v^2)</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code># inertia of the diagonal factor
sum(s$d^2)</code></pre>
<pre><code>## [1] 341</code></pre>
<p>This inertia can then be <em>conferred</em> unto the standardized case or variable coordinates, transforming one or the other into principal coordinates (the first two options above) or both halfway there (the symmetric option). Each of these options confers the inertia in such a way that the sums of the exponents of <span class="math inline">\(D\)</span> in the transformed sets of case (<span class="math inline">\(F=UD^p\)</span>) and variable (<span class="math inline">\(G=VD^q\)</span>) coordinates is <span class="math inline">\(p+q=1\)</span>, which ensures the <em>inner product property</em> <span class="math inline">\(FG^\top=X\)</span> between them. This recovers any entry <span class="math inline">\(x_{ij}\)</span> of <span class="math inline">\(X\)</span> as the inner product <span class="math inline">\(f_i\cdot g_i\)</span> of its case and variable coordinates <span class="math inline">\(f_i=[\,f_{i,1}\,\cdots\,f_{i,r}\,]\)</span> and <span class="math inline">\(g_i=[\,g_{i,1}\,\cdots\,g_{i,r}\,]\)</span>.</p>
<p>By conferring the inertia entirely to the cases or to the variables, we preserve (or best approximate) the geometric configurations of the cases or of the variables. In PCA, the geometry of the cases is usually construed as the distances between them. Here their pairwise distances <span class="math inline">\(\sqrt{(f_{j,1}-f_{i,1})^2+(f_{j,2}-f_{i,2})^2}\)</span> in the first two PCA dimensions are plotted against their “true” distances <span class="math inline">\(\left(\sum_{k=1}^{m}{(x_{j,k}-x_{i,k})^2}\right)^{1/2}\)</span> in the variable space:</p>
<pre class="r"><code># distances between cases
x.dist &lt;- dist(x)
# distances between cases (principal coordinates)
s.dist &lt;- dist(s$u[, 1:2] %*% diag(s$d[1:2]))
# scatterplot
plot(
  x = as.vector(x.dist), y = as.vector(s.dist),
  asp = 1, pch = 19, cex = .5,
  xlab = &quot;Case distances along variable coordinates&quot;,
  ylab = &quot;Case distances in two principal coordinates&quot;
)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/case%20geometry-1.png" width="528" /></p>
<p>Meanwhile, the geometry of the variables is usually understood through their covariances or correlations. Writing <span class="math inline">\(X=[\,y_1\,\cdots\,y_m\,]\)</span> as an array of column variables, the covariance between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is proportional to their inner product <span class="math display">\[\textstyle \operatorname{cov}(y_i,y_j)=\frac{1}{n}y_i\cdot y_j=\frac{1}{n}\lVert y_i\rVert\lVert y_j\rVert\cos\theta_{ij}\text,\]</span> so that the cosine of the angle <span class="math inline">\(\theta_{ij}\)</span> between them equals their correlation:
<span class="math display">\[\cos\theta_{ij}=\frac{\operatorname{cov}(y_i,y_j)}{\sqrt{\operatorname{cov}(y_i,y_i)\operatorname{cov}(y_j,y_j)}/n}=\frac{\operatorname{cov}(y_i,y_j)}{\sigma_i\sigma_j}=r_{ij}\]</span>
Here the cosines <span class="math inline">\(\frac{g_i}{\lVert g_i\rVert}\cdot\frac{g_j}{\lVert g_j\rVert}\)</span> between the variable vectors in the first two PCA dimensions are plotted against their correlations <span class="math inline">\(r_{ij}\)</span> across the original cases:</p>
<pre class="r"><code># correlations between variables
x.cor &lt;- cor(x)
# magnitudes of variable vectors
s.len &lt;- apply(s$v[, 1:2] %*% diag(s$d[1:2]), 1, norm, &quot;2&quot;)
# cosines between variables (principal coordinates)
s.cor &lt;- (s$v[, 1:2] / s.len) %*% diag(s$d[1:2]^2) %*% t(s$v[, 1:2] / s.len)
# scatterplot
plot(
  x = as.vector(x.cor[lower.tri(x.cor)]),
  y = as.vector(s.cor[lower.tri(s.cor)]),
  asp = 1, pch = 19, cex = .5,
  xlab = &quot;Variable correlations among cases&quot;,
  ylab = &quot;Cosines between variable vectors in two principal coordinates&quot;
)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/variable%20geometry-1.png" width="528" /></p>
<div id="multidimensional-scaling-of-variables" class="section level2">
<h2>multidimensional scaling of variables</h2>
<p>The faithful approximation of inter-case distances by principal coordinates is the idea behind <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling">(classical)</a> <em>multidimensional scaling</em> (MDS), which can be applied to a data set of distances <span class="math inline">\(\delta_{ij},\ 1\leq i,j\leq n\)</span> in the absence of coordinates. This technique is based on the eigendecomposition of a doubly-centered matrix of squared distances, which produces matrix <span class="math inline">\(U\Lambda^{1/2}\)</span> whose first <span class="math inline">\(r\)</span> coordinates—for any <span class="math inline">\(r\leq n\)</span>—recover a best approximation of the inter-case distances in terms of the sum of squared errors, i.e. the variance of <span class="math inline">\((U\Lambda^{1/2})(U\Lambda^{1/2})^\top-\Delta=U\Lambda U^\top-\Delta\)</span>, where <span class="math inline">\(\Delta=(\delta_{ij})\in\mathbb{R}^{n\times n}\)</span>. In practice, the goal is usually to position points representing the <span class="math inline">\(n\)</span> cases in a 2-dimensional scatterplot so that their distances <span class="math inline">\(\sqrt{(x_j-x_i)^2+(y_j-y_i)^2}\)</span> approximate their original distances <span class="math inline">\(\delta_{ij}\)</span>, as in this example using road distances between U.S. cities to approximate their geographic arrangement:</p>
<pre class="r"><code>d &lt;- as.matrix(UScitiesD)
cent &lt;- diag(1, nrow(d)) - matrix(1/nrow(d), nrow(d), nrow(d))
d.cent &lt;- -.5 * cent %*% (d^2) %*% cent
d.mds &lt;- svd(d.cent)
d.coord &lt;- d.mds$u[, 1:2] %*% diag(sqrt(d.mds$d[1:2]))
plot(d.coord, pch = NA, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;)
text(d.coord, labels = rownames(d))</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/multidimensional%20scaling-1.png" width="816" /></p>
<p>The faithful approximation of inter-variable covariances by the inner products of their principal coordinate vectors suggests a complementary technique that i haven’t found explicitly discussed in my own background reading. Suppose we have data that consist not of distances between cases but of covariances <span class="math inline">\(\operatorname{cov}(y_i,y_j),\ 1\leq i,j\leq m\)</span> between variables. Again the data are coordinate-free, so PCA cannot be applied. Were the data to have derived from a <em>centered</em> case–variable matrix <span class="math inline">\(X\)</span>, then the covariance matrix <span class="math inline">\(C=(\operatorname{cov}(y_i,y_j))\)</span> would have been obtained as <span class="math inline">\(C=\frac{1}{n}X^\top X\)</span>, which is (up to scalar) the matrix whose eigenvectors would be given by <span class="math inline">\(V\)</span> in the SVD <span class="math inline">\(X=UDV^\top\)</span>. Therefore, we can fabricate coordinates for the <span class="math inline">\(m\)</span> variables that approximate what we know of their geometry—in this case, thinking of the variables as unknown vectors, whose magnitudes and pairwise angles are encoded in <span class="math inline">\(C\)</span>—via an eigendecomposition <span class="math inline">\(C=V\Lambda V^\top\)</span>: Take <span class="math inline">\(Y=V\Lambda^{1/2}\in\mathbb{R}^{m\times r}\)</span>, so that <span class="math inline">\(Y^\top Y\approx C\)</span>.</p>
<p>I’ll validate this line of reasoning by taking the <code>mtcars</code> data set for a spin:</p>
<pre class="r"><code># covariances and standard deviations
c &lt;- cov(mtcars)
s &lt;- diag(sqrt(diag(c)))
# centered data
x &lt;- as.matrix(scale(mtcars, center = TRUE, scale = FALSE))
# eigendecomposition of covariance matrix
c.eigen &lt;- eigen(c)
# artificial coordinates
c.coord &lt;- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
# validate covariance recovery (up to sign)
all.equal(
  as.vector(c.coord %*% t(c.coord)),
  as.vector(c),
  tolerance = 1e-12
)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Thus, whereas <em>MDS of cases</em> is used to represent distances, <em>MDS of variables</em> can be used to represent covariances.
A use case for this technique is a situation in which covariance data exist without variable values. This may of course be the case because original data has become unavailable.</p>
<p>A more interesting setting that gives rise to this situation is the analysis of multiple rankings of the same set of objects in terms of their <em>concordance</em>. Rankings’ concordance is often measured using rank correlations such as Kendall’s <span class="math inline">\(\tau\)</span>, which may be <em>general correlation coefficients</em> in <a href="https://en.wikipedia.org/wiki/Rank_correlation#General_correlation_coefficient">the sense proposed by Kendall</a> but are not associated with an underlying (Euclidean) geometry. Nevertheless, we can use MDS to represent these rankings as unit vectors in Euclidean space whose pairwise cosines approximate their rank correlations!</p>
</div>
<div id="example-rankings-of-universities" class="section level2">
<h2>example: rankings of universities</h2>
<p>A real-world example is provided by the <a href="https://www.topuniversities.com/qs-world-university-rankings/methodology">Quacquarelli Symonds Top University Rankings</a>, which include rankings of hundreds of world universities on six “metrics”: academic reputation, employer reputation, faculty–student ratio, citations per faculty, international faculty ratio, and international student ratio. QS weight these rankings differently in their overall assessment, but our analysis will compare the rankings to each other, so these weights are irrelevant. I restricted the data from the year 2020 to universities in the United States for which integer rankings (i.e. not “400+” placeholders) were available in all four years:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<pre class="r"><code>qswurus20 &lt;- readRDS(here::here(&quot;supplementary/qswurus20.rds&quot;))
head(qswurus20)</code></pre>
<pre><code>##   year                                  institution size focus res age
## 1 2020  MASSACHUSETTS INSTITUTE OF TECHNOLOGY (MIT)    M    CO  VH   5
## 2 2020                          STANFORD UNIVERSITY    L    FC  VH   5
## 3 2020                           HARVARD UNIVERSITY    L    FC  VH   5
## 4 2020 CALIFORNIA INSTITUTE OF TECHNOLOGY (CALTECH)    S    CO  VH   5
## 5 2020                        UNIVERSITY OF CHICAGO    L    FC  VH   5
## 6 2020                         PRINCETON UNIVERSITY    M    CO  VH   5
##   status rk_academic rk_employer rk_ratio rk_citations rk_intl_faculty
## 1      B           5           4       15            7              43
## 2      B           4           5       12           13              62
## 3      B           1           1       40            8             186
## 4      B          23          74        4            4              72
## 5      B          13          37       54           60             249
## 6      B          10          19      192            3             272
##   rk_intl_students
## 1               87
## 2              196
## 3              221
## 4              121
## 5              143
## 6              197</code></pre>
<p>Since the integer rankings were subsetted from the full international data set, they are not contiguous (i.e. some integers between rankings never appear). To resolve this, i’ll recalibrate the rankings by matching each vector of ranks to the vector of its sorted unique values:</p>
<pre class="r"><code>library(dplyr)
qswurus20 %&gt;%
  select(institution, starts_with(&quot;rk_&quot;)) %&gt;%
  mutate_at(
    vars(starts_with(&quot;rk_&quot;)),
    ~ match(., sort(unique(as.numeric(.))))
  ) %&gt;%
  print() -&gt; qswurus20</code></pre>
<pre><code>## # A tibble: 38 x 7
##    institution rk_academic rk_employer rk_ratio rk_citations
##    &lt;chr&gt;             &lt;int&gt;       &lt;int&gt;    &lt;int&gt;        &lt;int&gt;
##  1 MASSACHUSE…           3           2        6            3
##  2 STANFORD U…           2           3        4            5
##  3 HARVARD UN…           1           1       11            4
##  4 CALIFORNIA…          12          14        1            2
##  5 UNIVERSITY…           9          11       13           12
##  6 PRINCETON …           7           7       18            1
##  7 CORNELL UN…          11          13       22            7
##  8 UNIVERSITY…          14          12        8           17
##  9 YALE UNIVE…           6           4        2           24
## 10 COLUMBIA U…           8           8        7           25
## # … with 28 more rows, and 2 more variables: rk_intl_faculty &lt;int&gt;,
## #   rk_intl_students &lt;int&gt;</code></pre>
<p>This subset of universities is now contiguously ranked along the six dimensions described above. The Kendall correlation <span class="math inline">\(\tau_{ij}\)</span> between two rankings measures their concordance. To calculate it, every pair of universities contributes either <span class="math inline">\(+1\)</span> or <span class="math inline">\(-1\)</span> according as the rankings <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> place that pair in the same order, and the sum is scaled down by the number of pairs <span class="math inline">\({n\choose 2}\)</span> so that the result lies between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. We interpret <span class="math inline">\(\tau_{ij}=1\)</span> as perfect concordance (the rankings are equivalent), <span class="math inline">\(\tau_{ij}=-1\)</span> as perfect discordance (the rankings are reversed), and <span class="math inline">\(\tau_{ij}=0\)</span> as independence (the rankings are unrelated).</p>
<p>The QS rankings are not variations on a theme, like different measures of guideline adherence or positive affect, but they do all seem potentially sensitive to a university’s resources, including funding and prestige. I intuit that the two reputational metrics should be positively correlated, and that the two international ratios should be as well. I also wonder if the faculty–student ratio might be anti-correlated with the number of citations per faculty, separating more research-focused institutions from more teaching-focused ones.</p>
<div id="correlation-heatmap" class="section level3">
<h3>correlation heatmap</h3>
<p>A common way to visualize correlation matrices is the heatmap, so i’ll use that technique first (see below). While the rankings by academic and employer reputations are highly concordant, those by international faculty and student ratios are less so; and, the faculty–student ratio and faculty citation rankings have the weakest concordance of all, but are nevertheless positively correlated.</p>
<pre class="r"><code>c &lt;- cor(select(qswurus20, starts_with(&quot;rk_&quot;)), method = &quot;kendall&quot;)
corrplot::corrplot(c)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/Kendall%20rank%20correlations-1.png" width="672" /></p>
<p>This visualization is useful, but it’s very busy: To compare any pair of rankings, i have to find the cell in the grid corresponding to that pair and refer back to the color scale to assess its meaning. I can’t rely on the nearby cells for context, because they may be stronger or weaker than average and skew my interpretation. For example, the visibly weak associations between the faculty–student ratio and other rankings (the third row or column) happen to be arranged so that the slightly stronger among them, with the two reputational variables, are sandwiched between the <em>even stronger</em> associations between the two reputational rankings and between them and the faculty citations ranking; whereas its weaker associations are sandwiched between more typical, but still comparatively stronger, associations. A different ordering of the variables might “obscure” this pattern and “reveal” others.</p>
<p>The plot is also strictly pairwise: Every correlation between two rankings occupies its own cell—two, in fact, making almost half of the plot duplicative. This means that a subset analysis of, say, three rankings requires focusing on three cells at the corners of a right triangle while ignoring all the surrounding cells. This is not an easy visual task. It would be straightforward to create a new plot for any subset, but then the larger context of the remaining rankings would be lost.</p>
</div>
<div id="correlation-biplot" class="section level3">
<h3>correlation biplot</h3>
<p>MDS of variables offers a natural alternative visualization: the biplot. As with MDS of cases, the point isn’t to overlay the case scores and variable loadings from a singular value decomposition, but to use the scores or loadings alone to endow the cases or variables with a Euclidean geometry they didn’t yet have. To that end, i’ll plot the variables as vectors with tails at the origin and heads at their fabricated coordinates <span class="math inline">\(Y=V\Lambda^{1/2}\)</span>:</p>
<pre class="r"><code>c.eigen &lt;- eigen(c)
c.coord &lt;- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
plot(c.coord, pch = NA, asp = 1, xlab = &quot;&quot;, ylab = &quot;&quot;)
arrows(0, 0, c.coord[, 1], c.coord[, 2])
text(c.coord, labels = rownames(c))</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/multidimensional%20scaling%20of%20variables-1.png" width="672" /></p>
<p>A more elegant ggplot2-style graphic can be rendered with <a href="https://github.com/corybrunson/ordr">ordr</a>, with a unit circle included for reference:</p>
<pre class="r"><code>library(ordr)
eigen_ord(c) %&gt;%
  as_tbl_ord() %&gt;%
  augment() %&gt;%
  mutate_u(metric = stringr::str_remove(.name, &quot;rk_&quot;)) %&gt;%
  confer_inertia(1) %&gt;%
  negate_to_nonneg_orthant(&quot;u&quot;) -&gt;
  c_eigen
c_eigen %&gt;%
  ggbiplot() +
  theme_minimal() +
  geom_unit_circle() +
  geom_u_vector() +
  geom_u_text_radiate(aes(label = metric)) +
  scale_x_continuous(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .2)) +
  ggtitle(&quot;MDS of Kendall correlations between university rankings&quot;)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>With respect to the pairwise correlations, the biplot is significantly less precise: Though the vectors all have unit length in <span class="math inline">\(\mathbb{R}^r\)</span> (<span class="math inline">\(r\leq m=6\)</span>), their projections onto the first two principal coordinates are much shorter, indicating that much of the geometric configuration requires additional dimensions to represent. Indeed, these coordinates capture only <span class="math inline">\(48.2\%+14.3\%=62.5\%\)</span> of the inertia in the full representation. This means that the angles between the vectors must be interpreted with caution: For example, it looks like the academic and employer reputation rankings are extremely correlated, but the apparent alignment of the vectors could be an artifact of the projection, when in fact they “rise” and “fall” in opposite directions along the remaining dimensions. The correlation heatmap leaves no such ambiguity.</p>
<p>However, the biplot far surpasses the heatmap at parsimony: Each variable is represented by a single vector, and the angle cosines between the variable vectors roughly approximate their correlations. For instance, the rankings based on international student and faculty ratios have correlation around <span class="math inline">\(\cos(\frac{\pi}{4})=\frac{1}{\sqrt{2}}\)</span>, corresponding to either explaining half the “variance” in the other—not technically meaningful in the ranking context but a useful conceptual anchor. Meanwhile, the faculty–student ratio ranking is nearly independent of the faculty citation ranking, contrary to my intuition that these rankings would reflect a <em>reverse</em> association between research- and teaching-oriented institutions. The convenience of recognizing correlations as cosines may be worth the significant risk of error, especially since that error (the residual <span class="math inline">\(37.5\%\)</span> of inertia) can be exactly quantified.</p>
<p>Moreover, the principal coordinates of the variable vectors indicate their loadings onto the first and second principal moments of inertia—the two dimensions that capture the most variation in the data. For example, the first principal coordinate is most aligned with the two reputational rankings, suggesting that a general prestige ranking is the strongest overall component of the several specific rankings. In contrast, the faculty–student ratio and faculty citation rankings load most strongly onto the second principal coordinate, suggesting that the divide between research- and teaching-focused institutions may yet be important to understanding how universities compare along these different metrics. These observations, provisional though they are, would be difficult to discern from the heatmap. More importantly, unlike the secondary patterns visible in the heatmap, these are in no sense artifacts of the layout but arise directly from the (correlational) data.</p>
<p>This last point means that observations made from a biplot can be validated from the MDS coordinates. In particular, we can examine the variables’ loadings onto the third principal coordinate, and we can check whether the reputational rankings are aligned or misaligned along it.</p>
<pre class="r"><code>c_eigen %&gt;%
  tidy(.matrix = &quot;u&quot;) %&gt;%
  select(-.name, -.matrix)</code></pre>
<pre><code>## # A tibble: 6 x 7
##     EV1     EV2     EV3     EV4     EV5      EV6 metric       
##   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        
## 1 0.834 -0.0907 -0.412   0.0430  0.0206 -0.351   academic     
## 2 0.795 -0.0964 -0.477  -0.0416  0.181   0.311   employer     
## 3 0.517  0.771   0.0480  0.331  -0.158   0.0372  ratio        
## 4 0.731 -0.352   0.239  -0.0278 -0.528   0.0685  citations    
## 5 0.631 -0.233   0.521   0.392   0.352  -0.00783 intl_faculty 
## 6 0.603  0.262   0.324  -0.665   0.140  -0.0312  intl_students</code></pre>
<pre class="r"><code>c_eigen %&gt;%
  tidy(.matrix = &quot;coord&quot;)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   .name .values .inertia .prop_var
##   &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 EV1     2.89     2.89     0.482 
## 2 EV2     0.858    0.858    0.143 
## 3 EV3     0.833    0.833    0.139 
## 4 EV4     0.709    0.709    0.118 
## 5 EV5     0.480    0.480    0.0799
## 6 EV6     0.227    0.227    0.0379</code></pre>
<p>Based on the third principal coordinates, the reputational rankings are aligned, as we knew already from the correlation matrix and heatmap. What’s a bit more interesting is that this component seems to separate these two rankings from those having to do with faculty citation rates and the international compositions of the faculty and student body. Based on the decomposition of inertia, this third principal coordinate is nearly as important as the second! It therefore makes sense to plot the two together:</p>
<pre class="r"><code>c_eigen %&gt;%
  ggbiplot(aes(x = 2, y = 3)) +
  theme_minimal() +
  geom_unit_circle() +
  geom_u_vector() +
  geom_u_text_radiate(aes(label = metric)) +
  scale_x_continuous(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .4)) +
  ggtitle(&quot;MDS of Kendall correlations between university rankings&quot;,
          &quot;Second and third principal coordinates&quot;)</code></pre>
<p><img src="/post/2019-08-16-rank-correlations_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The primary antitheses of the reputational rankings, after removing the first principal coordinate, are the two rankings based on international composition—and this axis is largely independent of the axis apparently distinguishing research- from teaching-oriented institutions. From my own limited knowledge, i’d hazard a guess that this reflects two tiers of international representation among students and faculty, one expressed by the most prestigious institutions that recruit highly qualified applicants from all over the world, and the other expressed by institutions that are not especially prestigious but are located in communities or regions with high percentages of international residents.</p>
<p>This is of course no more than idle speculation on my part! But a visualization scheme that encourages hypothesis generation is worth having on hand.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>That is to say, if the variables don’t have meaningful zero values and/or commensurate scales, then they should be centered to zero mean and/or scaled to unit variance.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This leaves us with only 38 universities, so my inferences must be taken with extreme caution!<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
</content:encoded>
    </item>
    
  </channel>
</rss>
